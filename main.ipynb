{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D0zuYQzwiO8Z"
   },
   "source": [
    "# M2608.001300 Machine Learning<br> Assignment #5 Final Projects (Pytorch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cap2TxT : An End-to-End Hybrid Neural Network for Captcha Image Text Sequence Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">\n",
    "Changwoon Choi.<br>\n",
    "SNU ECE.<br>\n",
    "2014-17733<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CJfG1l3RC_c3"
   },
   "source": [
    "For code simplication of this main.ipynb file, I added some python files for simple calculation or transformation at current directory.<br> (those codes are also submitted with this main.ipynb file with maintaining the directory hierarchy.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "M9yv4oGGDbmJ",
    "outputId": "76b24c10-0f36-407e-9bb7-2c9ce3b5edff"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch.nn import CTCLoss\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import dataset\n",
    "import utils\n",
    "import params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "acvGcUAaEkxe"
   },
   "source": [
    "Load datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "UcPk4u8qGZHB",
    "outputId": "5b4bfcbc-c731-4704-ae17-649e9f6d16ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fee3da6e3b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabet='0123456789abcdefghijklmnopqrstuvwxyz'\n",
    "NUMBER = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "ALPHABET = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "NONE = ['NONE'] # label for empty space\n",
    "ALL_CHAR_SET = NUMBER + ALPHABET + NONE\n",
    "ALL_CHAR_SET_LEN = len(ALL_CHAR_SET)\n",
    "MAX_CAPTCHA = 7\n",
    "\n",
    "def encode(a):\n",
    "    onehot = [0]*ALL_CHAR_SET_LEN\n",
    "    idx = ALL_CHAR_SET.index(a)\n",
    "    onehot[idx] += 1\n",
    "    return onehot\n",
    "\n",
    "class Mydataset(Dataset):\n",
    "    def __init__(self, img_path, label_path, is_train=True, transform=None):\n",
    "        self.path = img_path\n",
    "        self.label_path = label_path\n",
    "        if is_train: \n",
    "            self.img = os.listdir(self.path)[:10000]\n",
    "            self.labels = open(self.label_path, 'r').read().split('\\n')[:-1][:10000]\n",
    "        else: \n",
    "            self.img = os.listdir(self.path)[:1000]\n",
    "            self.labels = open(self.label_path, 'r').read().split('\\n')[:-1][:1000]\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.max_length = MAX_CAPTCHA\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img[idx]\n",
    "        img = Image.open(f'{self.path}/{self.img[idx]}')\n",
    "        img = img.convert('L')\n",
    "        label = self.labels[idx]\n",
    "        label_oh = []\n",
    "        \n",
    "        for i in range(self.max_length):\n",
    "            if i < len(label):\n",
    "                label_oh += encode(label[i])\n",
    "            else:\n",
    "                #label_oh += [0]*ALL_CHAR_SET_LEN\n",
    "                label_oh += encode('NONE')\n",
    "            \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, np.array(label_oh), label\n",
    "    def __len__(self):\n",
    "        return len(self.img)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize([160, 60]),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "EFtlQyubGJZb",
    "outputId": "4697c0a5-d24a-4e05-f226-14021b625f5e"
   },
   "outputs": [],
   "source": [
    "gPath = './'\n",
    "train_ds = Mydataset(gPath+'Data/train/', gPath+'Data/train.txt',transform=transform)\n",
    "test_ds = Mydataset(gPath+'Data/test/', gPath+'Data/test.txt',False, transform)\n",
    "train_dl = DataLoader(train_ds, batch_size=64, num_workers=4)\n",
    "test_dl = DataLoader(test_ds, batch_size=1, num_workers=4)\n",
    "my_train_loader=torch.utils.data.DataLoader(dataset.lmdbDataset('Data/train_prepared'), batch_size=32,shuffle=True, sampler=None, num_workers=4, collate_fn=dataset.alignCollate(imgH=64,imgW=160, keep_ratio=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare dataset (the python files below should be executed only once.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python misc.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python misc_2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python misc_3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_dataset.py:14: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\r\n",
      "  imageBuf = np.fromstring(imageBin, dtype=np.uint8)\r\n"
     ]
    }
   ],
   "source": [
    "!python create_dataset.py --out Data/train_prepared --folder Data/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ka5SgX6VIWcG"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tJaHW3wSENjY"
   },
   "source": [
    "Problem 1: Design LSTM model for captcha image recognition. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "rHEe3XmBFQHq",
    "outputId": "5b4bfcbc-c731-4704-ae17-649e9f6d16ac"
   },
   "outputs": [],
   "source": [
    "class ONE_LAYER_LSTM(nn.Module):\n",
    "    def __init__(self, dim_in, dim_hidden, dim_out):\n",
    "        super(ONE_LAYER_LSTM,self).__init__()\n",
    "        self.lstm = nn.LSTM(dim_in, dim_hidden, bidirectional=True)\n",
    "        self.fc_out = nn.Linear(2*dim_hidden, dim_out)\n",
    "    def forward(self, features):\n",
    "        tmp, _ = self.lstm(features)\n",
    "        T,b,h = tmp.size()\n",
    "        tmp = tmp.view(T*b,h)\n",
    "        output = self.fc_out(tmp)\n",
    "        output = output.view(T,b,-1)\n",
    "        return output\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, cnn_dim, hidden_size, classnum, num_layers=2):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstmnet = nn.Sequential(ONE_LAYER_LSTM(cnn_dim, hidden_size, hidden_size),\n",
    "                                    ONE_LAYER_LSTM(hidden_size,hidden_size, classnum))\n",
    "        \n",
    "    def forward(self, features):\n",
    "        output = self.lstmnet(features)\n",
    "        output = F.log_softmax(output,dim=2)\n",
    "        return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TM2vL2PTFeEt"
   },
   "source": [
    "Problem 2: \n",
    "\n",
    "*   1.Connect CNN model to the designed LSTM model.\n",
    "*   2.Replace ResNet to your own CNN model from Assignment3.\n",
    "\n",
    "\n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trial1\n",
    "'''my betternet from AS3'''\n",
    "\n",
    "class Inception(nn.Module):\n",
    "    def __init__(self, in_planes, n1x1, n3x3red, n3x3, n5x5red, n5x5, pool_planes):\n",
    "        super(Inception, self).__init__()\n",
    "        # 1x1 conv branch\n",
    "        self.b1 = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, n1x1, kernel_size=1),\n",
    "            nn.BatchNorm2d(n1x1),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.b2 = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, n3x3red, kernel_size=1),\n",
    "            nn.BatchNorm2d(n3x3red),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(n3x3red, n3x3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n3x3),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.b3 = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, n5x5red, kernel_size=1),\n",
    "            nn.BatchNorm2d(n5x5red),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(n5x5red, n5x5, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(n5x5),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(n5x5, n5x5, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(n5x5),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.b4 = nn.Sequential(\n",
    "            nn.MaxPool2d(3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_planes, pool_planes, kernel_size=1),\n",
    "            nn.BatchNorm2d(pool_planes),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        y1 = self.b1(x)\n",
    "        y2 = self.b2(x)\n",
    "        y3 = self.b3(x)\n",
    "        y4 = self.b4(x)\n",
    "        return torch.cat([y1,y2,y3,y4], 1)\n",
    "    \n",
    "class BetterNet_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BetterNet_1, self).__init__()        \n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1,out_channels=64,kernel_size=7,stride=1,padding=3),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(3,2,1)\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64,out_channels=64,kernel_size=1,stride=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=64,out_channels=192,kernel_size=3,stride=1,padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(3,2,1)\n",
    "        )\n",
    "        self.incep_block1 = nn.Sequential(\n",
    "            Inception(192,64,96,128,16,32,32),\n",
    "            Inception(256,128,128,192,32,96,64),\n",
    "            nn.MaxPool2d(3,2,1)\n",
    "        )        \n",
    "        self.incep_block2 = nn.Sequential(\n",
    "            Inception(480,192,96,208,16,48,64),\n",
    "            Inception(512,160,112,224,24,64,64),\n",
    "            Inception(512,128,128,256,24,64,64),\n",
    "            Inception(512,112,144,288,32,64,64),\n",
    "            Inception(528,256,160,320,32,128,128),\n",
    "            nn.MaxPool2d(3,2,1)\n",
    "        )        \n",
    "        self.incep_block3 = nn.Sequential(\n",
    "            Inception(832,256,160,320,32,128,128),\n",
    "            Inception(832,384,192,384,48,128,128),\n",
    "            nn.AvgPool2d(kernel_size=4,stride=1,padding=0)\n",
    "        )        \n",
    "        self.fc_block = nn.Sequential(\n",
    "            nn.Linear(1024,512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512,ALL_CHAR_SET_LEN*MAX_CAPTCHA)\n",
    "           # nn.ReLU(True),\n",
    "           # nn.Linear(64,10)\n",
    "        )        \n",
    "    def forward(self, x):\n",
    "        x=self.block1(x)\n",
    "        x=self.block2(x)\n",
    "        x=self.incep_block1(x)\n",
    "        x=self.incep_block2(x)\n",
    "        x=self.incep_block3(x)\n",
    "        #print('cnn 5:',end='')\n",
    "        #print(x.shape)\n",
    "        #x = x.view(-1,1024)\n",
    "        #x=self.fc_block(x)\n",
    "        out=x\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trial 2\n",
    "'''my betternet from AS3'''\n",
    "class BetterNet_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BetterNet_2, self).__init__()\n",
    "        \n",
    "        self.cnn1=nn.Sequential()\n",
    "        self.cnn1.add_module('conv1', nn.Conv2d(1,64,3,1,1))\n",
    "        self.cnn1.add_module('relu1', nn.ReLU(True))\n",
    "        self.cnn1.add_module('pooling1', nn.MaxPool2d(2,2))\n",
    "        \n",
    "        self.cnn2=nn.Sequential()\n",
    "        self.cnn2.add_module('conv2', nn.Conv2d(64,128,3,1,1))\n",
    "        self.cnn2.add_module('relu2', nn.ReLU(True))\n",
    "        self.cnn2.add_module('pooling2', nn.MaxPool2d(2,2))\n",
    "        \n",
    "        self.cnn3=nn.Sequential()\n",
    "        self.cnn3.add_module('conv3', nn.Conv2d(128,256,3,1,1))\n",
    "        self.cnn3.add_module('batchnorm', nn.BatchNorm2d(256))\n",
    "        self.cnn3.add_module('relu3', nn.ReLU(True))\n",
    "               \n",
    "        self.cnn4=nn.Sequential()\n",
    "        self.cnn4.add_module('conv4', nn.Conv2d(256,256,3,1,1))\n",
    "        self.cnn4.add_module('relu4', nn.ReLU(True))\n",
    "        self.cnn4.add_module('pooling4', nn.MaxPool2d((2,2),(2,1),(0,1)))\n",
    "        \n",
    "        self.cnn5=nn.Sequential()\n",
    "        self.cnn5.add_module('conv5', nn.Conv2d(256,512,3,1,1))\n",
    "        self.cnn5.add_module('batchnorm', nn.BatchNorm2d(512))\n",
    "        self.cnn5.add_module('relu5', nn.ReLU(True))\n",
    "\n",
    "        self.cnn6=nn.Sequential()\n",
    "        self.cnn6.add_module('conv6', nn.Conv2d(512,512,3,1,1))\n",
    "        self.cnn6.add_module('relu6', nn.ReLU(True))\n",
    "        self.cnn6.add_module('pooling6', nn.MaxPool2d((2,2),(2,1),(0,1)))\n",
    "        \n",
    "        self.cnn7=nn.Sequential()\n",
    "        self.cnn7.add_module('conv7', nn.Conv2d(512,512,2,1,0))\n",
    "        self.cnn7.add_module('batchnorm', nn.BatchNorm2d(512))\n",
    "        self.cnn7.add_module('relu7', nn.ReLU(True))\n",
    "        self.cnn7.add_module('pooling7', nn.AvgPool2d(kernel_size=3,stride=1))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out=self.cnn1(x)\n",
    "        out=self.cnn2(out)\n",
    "        out=self.cnn3(out)\n",
    "        out=self.cnn4(out)\n",
    "        out=self.cnn5(out)\n",
    "        out=self.cnn6(out)\n",
    "        out=self.cnn7(out)\n",
    "        return out\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_normalize(net):\n",
    "    a=net.__class__.__name__\n",
    "    if a.find('Conv') != -1:\n",
    "        net.weight.data.normal_(0,0.02)\n",
    "    elif a.find('BatchNorm') !=-1:\n",
    "        net.weight.data.normal_(1,0.02)\n",
    "        net.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OwgpQ1aiFq2a"
   },
   "outputs": [],
   "source": [
    "# my CNN-RNN fusioned model, Cap2TxT Network\n",
    "# trial 1,2\n",
    "vocab_size = len(alphabet)+1\n",
    "\n",
    "class Cap2TxT(nn.Module):\n",
    "    def __init__(self, hidden_size, class_num, num_layers=2):\n",
    "        super(Cap2TxT, self).__init__()\n",
    "        #self.cnn = BetterNet_2()\n",
    "        self.cnn = BetterNet_1()\n",
    "        #self.rnn = LSTM(512, hidden_size, class_num,num_layers)\n",
    "        self.rnn = LSTM(1024, hidden_size, class_num, num_layers)\n",
    "    def forward(self, x):\n",
    "        \n",
    "        features = self.cnn(x)\n",
    "        print(features.shape)\n",
    "        b,c,h,w = features.size()\n",
    "        features = features.squeeze(2)\n",
    "        features = features.permute(2,0,1)\n",
    "        output = self.rnn(features)\n",
    "        return output\n",
    "\n",
    "\n",
    "hidden_size=256\n",
    "batch_size = 32\n",
    "lr = 0.0001\n",
    "\n",
    "image = torch.FloatTensor(batch_size, 3, 64, 64)\n",
    "text = torch.LongTensor(batch_size * 5)\n",
    "length = torch.LongTensor(batch_size)\n",
    "\n",
    "net = Cap2TxT(hidden_size,vocab_size,2)\n",
    "net.apply(weight_normalize)\n",
    "\n",
    "\n",
    "converter = utils.strLabelConverter(alphabet)\n",
    "loss_func = CTCLoss()\n",
    "loss_avg=utils.averager()\n",
    "\n",
    "loss_func = loss_func.cuda()\n",
    "image=image.cuda()\n",
    "text=text.cuda()\n",
    "net=net.cuda()\n",
    "\n",
    "image=Variable(image)\n",
    "text=Variable(text)\n",
    "length=Variable(length)\n",
    "\n",
    "cap2txt_optim = optim.RMSprop(net.parameters(),lr)\n",
    "#cap2txt_optim = optim.Adam(net.parameters(),lr,betas=(0.5,0.999))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F0uCexwRHsNz"
   },
   "source": [
    "Problem3: Find hyper-parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ibfVzKZeH1yC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0, Loss : 0.122119\n",
      "saved\n",
      "epoch : 1, Loss : 0.114869\n",
      "epoch : 2, Loss : 0.097594\n",
      "epoch : 3, Loss : 0.080764\n",
      "saved\n",
      "epoch : 4, Loss : 0.063179\n",
      "epoch : 5, Loss : 0.047502\n",
      "epoch : 6, Loss : 0.036180\n",
      "saved\n",
      "epoch : 7, Loss : 0.029120\n",
      "epoch : 8, Loss : 0.022502\n",
      "epoch : 9, Loss : 0.018161\n",
      "saved\n",
      "epoch : 10, Loss : 0.015251\n",
      "epoch : 11, Loss : 0.012913\n",
      "epoch : 12, Loss : 0.010618\n",
      "saved\n",
      "epoch : 13, Loss : 0.009241\n",
      "epoch : 14, Loss : 0.007873\n",
      "epoch : 15, Loss : 0.007073\n",
      "saved\n",
      "epoch : 16, Loss : 0.006063\n",
      "epoch : 17, Loss : 0.005284\n",
      "epoch : 18, Loss : 0.005042\n",
      "saved\n",
      "epoch : 19, Loss : 0.006001\n",
      "epoch : 20, Loss : 0.003667\n",
      "epoch : 21, Loss : 0.004812\n",
      "saved\n",
      "epoch : 22, Loss : 0.004109\n",
      "epoch : 23, Loss : 0.003277\n",
      "epoch : 24, Loss : 0.002493\n",
      "saved\n",
      "epoch : 25, Loss : 0.002988\n",
      "epoch : 26, Loss : 0.002113\n",
      "epoch : 27, Loss : 0.002185\n",
      "saved\n",
      "epoch : 28, Loss : 0.002145\n",
      "epoch : 29, Loss : 0.001959\n",
      "epoch : 30, Loss : 0.001460\n",
      "saved\n",
      "epoch : 31, Loss : 0.001873\n",
      "epoch : 32, Loss : 0.001267\n",
      "epoch : 33, Loss : 0.001840\n",
      "saved\n",
      "epoch : 34, Loss : 0.001210\n",
      "epoch : 35, Loss : 0.001237\n",
      "epoch : 36, Loss : 0.001227\n",
      "saved\n",
      "epoch : 37, Loss : 0.001209\n",
      "epoch : 38, Loss : 0.001951\n",
      "epoch : 39, Loss : 0.001198\n",
      "saved\n",
      "epoch : 40, Loss : 0.001224\n",
      "epoch : 41, Loss : 0.001121\n",
      "epoch : 42, Loss : 0.000937\n",
      "saved\n",
      "epoch : 43, Loss : 0.001030\n",
      "epoch : 44, Loss : 0.000963\n",
      "epoch : 45, Loss : 0.000937\n",
      "saved\n",
      "epoch : 46, Loss : 0.000851\n",
      "epoch : 47, Loss : 0.000892\n",
      "epoch : 48, Loss : 0.001009\n",
      "saved\n",
      "epoch : 49, Loss : 0.000605\n",
      "epoch : 50, Loss : 0.000720\n",
      "epoch : 51, Loss : 0.000804\n",
      "saved\n",
      "epoch : 52, Loss : 0.000705\n",
      "epoch : 53, Loss : 0.000582\n",
      "epoch : 54, Loss : 0.000968\n",
      "saved\n",
      "epoch : 55, Loss : 0.000708\n",
      "epoch : 56, Loss : 0.000460\n",
      "epoch : 57, Loss : 0.000702\n",
      "saved\n",
      "epoch : 58, Loss : 0.000559\n",
      "epoch : 59, Loss : 0.000790\n",
      "epoch : 60, Loss : 0.000488\n",
      "saved\n",
      "epoch : 61, Loss : 0.000591\n",
      "epoch : 62, Loss : 0.000427\n",
      "epoch : 63, Loss : 0.000700\n",
      "saved\n",
      "epoch : 64, Loss : 0.000586\n",
      "epoch : 65, Loss : 0.000511\n",
      "epoch : 66, Loss : 0.000597\n",
      "saved\n",
      "epoch : 67, Loss : 0.000408\n",
      "epoch : 68, Loss : 0.000604\n",
      "epoch : 69, Loss : 0.000387\n",
      "saved\n",
      "epoch : 70, Loss : 0.000646\n",
      "epoch : 71, Loss : 0.000288\n",
      "epoch : 72, Loss : 0.000513\n",
      "saved\n",
      "epoch : 73, Loss : 0.000540\n",
      "epoch : 74, Loss : 0.000351\n",
      "epoch : 75, Loss : 0.000472\n",
      "saved\n",
      "epoch : 76, Loss : 0.000366\n",
      "epoch : 77, Loss : 0.000437\n",
      "epoch : 78, Loss : 0.000225\n",
      "saved\n",
      "epoch : 79, Loss : 0.000461\n",
      "epoch : 80, Loss : 0.000365\n",
      "epoch : 81, Loss : 0.000724\n",
      "saved\n",
      "epoch : 82, Loss : 0.000215\n",
      "epoch : 83, Loss : 0.000304\n",
      "epoch : 84, Loss : 0.000433\n",
      "saved\n",
      "epoch : 85, Loss : 0.000232\n",
      "epoch : 86, Loss : 0.000425\n",
      "epoch : 87, Loss : 0.000364\n",
      "saved\n",
      "epoch : 88, Loss : 0.000310\n",
      "epoch : 89, Loss : 0.000294\n",
      "epoch : 90, Loss : 0.000261\n",
      "saved\n",
      "epoch : 91, Loss : 0.000372\n",
      "epoch : 92, Loss : 0.000369\n",
      "epoch : 93, Loss : 0.000484\n",
      "saved\n",
      "epoch : 94, Loss : 0.000230\n",
      "epoch : 95, Loss : 0.000387\n",
      "epoch : 96, Loss : 0.000344\n",
      "saved\n",
      "epoch : 97, Loss : 0.000297\n",
      "epoch : 98, Loss : 0.000328\n",
      "epoch : 99, Loss : 0.000233\n",
      "saved\n",
      "epoch : 100, Loss : 0.000193\n",
      "epoch : 101, Loss : 0.000325\n",
      "epoch : 102, Loss : 0.000212\n",
      "saved\n",
      "epoch : 103, Loss : 0.000386\n",
      "epoch : 104, Loss : 0.000353\n",
      "epoch : 105, Loss : 0.000260\n",
      "saved\n",
      "epoch : 106, Loss : 0.000410\n",
      "epoch : 107, Loss : 0.000224\n",
      "epoch : 108, Loss : 0.000171\n",
      "saved\n",
      "epoch : 109, Loss : 0.000159\n",
      "epoch : 110, Loss : 0.000283\n",
      "epoch : 111, Loss : 0.000188\n",
      "saved\n",
      "epoch : 112, Loss : 0.000310\n",
      "epoch : 113, Loss : 0.000279\n",
      "epoch : 114, Loss : 0.000157\n",
      "saved\n",
      "epoch : 115, Loss : 0.000337\n",
      "epoch : 116, Loss : 0.000213\n",
      "epoch : 117, Loss : 0.000228\n",
      "saved\n",
      "epoch : 118, Loss : 0.000202\n",
      "epoch : 119, Loss : 0.000349\n",
      "epoch : 120, Loss : 0.000037\n",
      "saved\n",
      "epoch : 121, Loss : 0.000484\n",
      "epoch : 122, Loss : 0.000276\n",
      "epoch : 123, Loss : 0.000194\n",
      "saved\n",
      "epoch : 124, Loss : 0.000227\n",
      "epoch : 125, Loss : 0.000301\n",
      "epoch : 126, Loss : 0.000209\n",
      "saved\n",
      "epoch : 127, Loss : 0.000220\n",
      "epoch : 128, Loss : 0.000351\n",
      "epoch : 129, Loss : 0.000202\n",
      "saved\n",
      "epoch : 130, Loss : 0.000110\n",
      "epoch : 131, Loss : 0.000356\n",
      "epoch : 132, Loss : 0.000171\n",
      "saved\n",
      "epoch : 133, Loss : 0.000146\n",
      "epoch : 134, Loss : 0.000214\n",
      "epoch : 135, Loss : 0.000160\n",
      "saved\n",
      "epoch : 136, Loss : 0.000323\n",
      "epoch : 137, Loss : 0.000179\n",
      "epoch : 138, Loss : 0.000179\n",
      "saved\n",
      "epoch : 139, Loss : 0.000256\n",
      "epoch : 140, Loss : 0.000145\n",
      "epoch : 141, Loss : 0.000199\n",
      "saved\n",
      "epoch : 142, Loss : 0.000167\n",
      "epoch : 143, Loss : 0.000162\n",
      "epoch : 144, Loss : 0.000221\n",
      "saved\n",
      "epoch : 145, Loss : 0.000113\n",
      "epoch : 146, Loss : 0.000238\n",
      "epoch : 147, Loss : 0.000168\n",
      "saved\n",
      "epoch : 148, Loss : 0.000230\n",
      "epoch : 149, Loss : 0.000193\n",
      "epoch : 150, Loss : 0.000109\n",
      "saved\n",
      "epoch : 151, Loss : 0.000164\n",
      "epoch : 152, Loss : 0.000192\n",
      "epoch : 153, Loss : 0.000210\n",
      "saved\n",
      "epoch : 154, Loss : 0.000051\n",
      "epoch : 155, Loss : 0.000249\n",
      "epoch : 156, Loss : 0.000108\n",
      "saved\n",
      "epoch : 157, Loss : 0.000218\n",
      "epoch : 158, Loss : 0.000199\n",
      "epoch : 159, Loss : 0.000232\n",
      "saved\n",
      "epoch : 160, Loss : 0.000143\n",
      "epoch : 161, Loss : 0.000298\n",
      "epoch : 162, Loss : 0.000084\n",
      "saved\n",
      "epoch : 163, Loss : 0.000184\n",
      "epoch : 164, Loss : 0.000176\n",
      "epoch : 165, Loss : 0.000091\n",
      "saved\n",
      "epoch : 166, Loss : 0.000191\n",
      "epoch : 167, Loss : 0.000073\n",
      "epoch : 168, Loss : 0.000241\n",
      "saved\n",
      "epoch : 169, Loss : 0.000102\n",
      "epoch : 170, Loss : 0.000166\n",
      "epoch : 171, Loss : 0.000158\n",
      "saved\n",
      "epoch : 172, Loss : 0.000104\n",
      "epoch : 173, Loss : 0.000209\n",
      "epoch : 174, Loss : 0.000093\n",
      "saved\n",
      "epoch : 175, Loss : 0.000182\n",
      "epoch : 176, Loss : 0.000179\n",
      "epoch : 177, Loss : 0.000185\n",
      "saved\n",
      "epoch : 178, Loss : 0.000086\n",
      "epoch : 179, Loss : 0.000169\n",
      "epoch : 180, Loss : 0.000153\n",
      "saved\n",
      "epoch : 181, Loss : 0.000232\n",
      "epoch : 182, Loss : 0.000109\n",
      "epoch : 183, Loss : 0.000123\n",
      "saved\n",
      "epoch : 184, Loss : 0.000156\n",
      "epoch : 185, Loss : 0.000127\n",
      "epoch : 186, Loss : 0.000146\n",
      "saved\n",
      "epoch : 187, Loss : 0.000214\n",
      "epoch : 188, Loss : 0.000222\n",
      "epoch : 189, Loss : 0.000117\n",
      "saved\n",
      "epoch : 190, Loss : 0.000116\n",
      "epoch : 191, Loss : 0.000068\n",
      "epoch : 192, Loss : 0.000212\n",
      "saved\n",
      "epoch : 193, Loss : 0.000085\n",
      "epoch : 194, Loss : 0.000118\n",
      "epoch : 195, Loss : 0.000183\n",
      "saved\n",
      "epoch : 196, Loss : 0.000068\n",
      "epoch : 197, Loss : 0.000109\n",
      "epoch : 198, Loss : 0.000140\n",
      "saved\n",
      "epoch : 199, Loss : 0.000161\n",
      "epoch : 200, Loss : 0.000125\n",
      "epoch : 201, Loss : 0.000090\n",
      "saved\n",
      "epoch : 202, Loss : 0.000200\n",
      "epoch : 203, Loss : 0.000028\n",
      "epoch : 204, Loss : 0.000115\n",
      "saved\n",
      "epoch : 205, Loss : 0.000146\n",
      "epoch : 206, Loss : 0.000104\n",
      "epoch : 207, Loss : 0.000146\n",
      "saved\n",
      "epoch : 208, Loss : 0.000010\n",
      "epoch : 209, Loss : 0.000181\n",
      "epoch : 210, Loss : 0.000123\n",
      "saved\n",
      "epoch : 211, Loss : 0.000148\n",
      "epoch : 212, Loss : 0.000164\n",
      "epoch : 213, Loss : 0.000033\n",
      "saved\n",
      "epoch : 214, Loss : 0.000201\n",
      "epoch : 215, Loss : 0.000079\n",
      "epoch : 216, Loss : 0.000168\n",
      "saved\n",
      "epoch : 217, Loss : 0.000092\n",
      "epoch : 218, Loss : 0.000181\n",
      "epoch : 219, Loss : 0.000111\n",
      "saved\n",
      "epoch : 220, Loss : 0.000087\n",
      "epoch : 221, Loss : 0.000076\n",
      "epoch : 222, Loss : 0.000115\n",
      "saved\n",
      "epoch : 223, Loss : 0.000099\n",
      "epoch : 224, Loss : 0.000198\n",
      "epoch : 225, Loss : 0.000068\n",
      "saved\n",
      "epoch : 226, Loss : 0.000201\n",
      "epoch : 227, Loss : 0.000225\n",
      "epoch : 228, Loss : 0.000257\n",
      "saved\n",
      "epoch : 229, Loss : 0.000103\n",
      "epoch : 230, Loss : 0.000160\n",
      "epoch : 231, Loss : 0.000076\n",
      "saved\n",
      "epoch : 232, Loss : 0.000178\n",
      "epoch : 233, Loss : 0.000056\n",
      "epoch : 234, Loss : 0.000188\n",
      "saved\n",
      "epoch : 235, Loss : 0.000160\n",
      "epoch : 236, Loss : 0.000056\n",
      "epoch : 237, Loss : 0.000162\n",
      "saved\n",
      "epoch : 238, Loss : 0.000076\n",
      "epoch : 239, Loss : 0.000105\n",
      "epoch : 240, Loss : 0.000189\n",
      "saved\n",
      "epoch : 241, Loss : 0.000070\n",
      "epoch : 242, Loss : 0.000126\n",
      "epoch : 243, Loss : 0.000110\n",
      "saved\n",
      "epoch : 244, Loss : 0.000150\n",
      "epoch : 245, Loss : 0.000065\n",
      "epoch : 246, Loss : 0.000090\n",
      "saved\n",
      "epoch : 247, Loss : 0.000089\n",
      "epoch : 248, Loss : 0.000060\n",
      "epoch : 249, Loss : 0.000127\n",
      "saved\n",
      "Finished Training\n",
      "Saved Model\n"
     ]
    }
   ],
   "source": [
    "\"\"\"TRAINING\"\"\"\n",
    "\n",
    "SAVE_PATH = './ckpt/'\n",
    "display_interval = 1\n",
    "save_interval=3\n",
    "max_epoch = 250\n",
    "\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    train_it = iter(my_train_loader)\n",
    "    it=0\n",
    "    while it<len(my_train_loader):\n",
    "        for param in net.parameters():\n",
    "            param.requires_grad=True\n",
    "        net.train()\n",
    "        chunk = train_it.next()\n",
    "        img, label = chunk\n",
    "        utils.loadData(image,img)\n",
    "        txt, lth = converter.encode(label)\n",
    "        utils.loadData(text,txt)\n",
    "        utils.loadData(length,lth)\n",
    "        \n",
    "        cap2txt_optim.zero_grad()\n",
    "        predict = net(image)\n",
    "        predict_size = Variable(torch.LongTensor([predict.size(0)] * img.size(0)))\n",
    "        \n",
    "        loss = loss_func(predict,text,predict_size,length)/img.size(0)\n",
    "        loss.backward()\n",
    "        cap2txt_optim.step()\n",
    "        \n",
    "        loss_avg.add(loss)\n",
    "        it+=1\n",
    "        \n",
    "    if epoch%display_interval==0:\n",
    "        print('epoch : %d, Loss : %f' %(epoch,loss_avg.val()))\n",
    "        loss_avg.reset()\n",
    "    if epoch%save_interval==0:\n",
    "        print('saved')\n",
    "        #torch.save(net.state_dict(),SAVE_PATH=\"Cap2TxT_\"+str(epoch)+'pth')\n",
    "        torch.save(net.state_dict(),SAVE_PATH+'Cap2TxT_GoogLe_'+str(epoch)+'.pth')\n",
    "\n",
    "print('Finished Training')\n",
    "torch.save(net.state_dict(),SAVE_PATH+'Cap2TxT'+'_final.pth')\n",
    "print('Saved Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1024, 1, 7])\n",
      "here\n",
      "tensor([[-3.9577e+00, -3.5050e+00, -3.7317e+00, -3.5668e+00, -3.5474e+00,\n",
      "         -3.4850e+00, -3.5377e+00, -3.6249e+00, -3.6213e+00, -3.6172e+00,\n",
      "         -3.5492e+00, -3.6494e+00, -3.5739e+00, -3.6187e+00, -3.5649e+00,\n",
      "         -3.5134e+00, -3.6227e+00, -3.6083e+00, -3.6113e+00, -3.7008e+00,\n",
      "         -3.6817e+00, -3.4374e+00, -3.6909e+00, -3.8928e+00, -3.4895e+00,\n",
      "         -3.7098e+00, -3.6091e+00, -3.6494e+00, -3.6046e+00, -3.5331e+00,\n",
      "         -3.6777e+00, -3.4763e+00, -3.5117e+00, -3.8493e+00, -3.5324e+00,\n",
      "         -3.5408e+00, -3.7342e+00],\n",
      "        [-3.6584e+00, -3.4096e+00, -3.8467e+00, -3.6683e+00, -3.5553e+00,\n",
      "         -3.3649e+00, -3.5084e+00, -3.6627e+00, -3.7511e+00, -3.6241e+00,\n",
      "         -3.5335e+00, -3.6264e+00, -3.5230e+00, -3.7498e+00, -3.5111e+00,\n",
      "         -3.4376e+00, -3.6989e+00, -3.5725e+00, -3.5086e+00, -3.7872e+00,\n",
      "         -3.8275e+00, -3.4195e+00, -3.7938e+00, -3.8582e+00, -3.4235e+00,\n",
      "         -3.7077e+00, -3.5876e+00, -3.6091e+00, -3.6995e+00, -3.5221e+00,\n",
      "         -3.7845e+00, -3.4213e+00, -3.4863e+00, -3.8205e+00, -3.6249e+00,\n",
      "         -3.5639e+00, -3.8193e+00],\n",
      "        [-2.3683e+00, -3.3841e+00, -4.0442e+00, -3.9082e+00, -3.6732e+00,\n",
      "         -3.2394e+00, -3.5098e+00, -3.7708e+00, -4.0177e+00, -3.7020e+00,\n",
      "         -3.5972e+00, -3.6972e+00, -3.4700e+00, -4.0140e+00, -3.5328e+00,\n",
      "         -3.4242e+00, -3.8526e+00, -3.6400e+00, -3.4459e+00, -3.8990e+00,\n",
      "         -4.0304e+00, -3.5031e+00, -3.9136e+00, -3.8191e+00, -3.4270e+00,\n",
      "         -3.7854e+00, -3.6434e+00, -3.6152e+00, -3.8667e+00, -3.6035e+00,\n",
      "         -3.9475e+00, -3.4593e+00, -3.5964e+00, -3.7977e+00, -3.8565e+00,\n",
      "         -3.6547e+00, -4.0135e+00],\n",
      "        [-1.3216e-01, -5.4692e+00, -6.0501e+00, -6.0835e+00, -5.6711e+00,\n",
      "         -5.1171e+00, -5.3665e+00, -5.6757e+00, -6.1664e+00, -5.7028e+00,\n",
      "         -5.5739e+00, -5.7505e+00, -5.3739e+00, -6.0146e+00, -5.5163e+00,\n",
      "         -5.5317e+00, -5.8237e+00, -5.7341e+00, -5.2793e+00, -5.5449e+00,\n",
      "         -5.8927e+00, -5.5498e+00, -5.6113e+00, -5.7945e+00, -5.6462e+00,\n",
      "         -5.8391e+00, -5.7591e+00, -5.5546e+00, -5.6926e+00, -5.7833e+00,\n",
      "         -5.7281e+00, -5.7106e+00, -5.7808e+00, -5.8655e+00, -6.0125e+00,\n",
      "         -5.5179e+00, -5.9771e+00],\n",
      "        [-1.2337e-02, -7.9461e+00, -8.2094e+00, -8.4130e+00, -7.9941e+00,\n",
      "         -7.5146e+00, -7.6196e+00, -7.9835e+00, -8.4417e+00, -8.0725e+00,\n",
      "         -7.8109e+00, -8.1312e+00, -7.7837e+00, -8.1863e+00, -7.9576e+00,\n",
      "         -7.9800e+00, -7.9920e+00, -8.0782e+00, -7.5200e+00, -7.5683e+00,\n",
      "         -8.0021e+00, -7.9392e+00, -7.7073e+00, -8.2931e+00, -8.1931e+00,\n",
      "         -8.2821e+00, -8.2444e+00, -7.8709e+00, -7.8470e+00, -8.1886e+00,\n",
      "         -7.8577e+00, -8.2504e+00, -8.2611e+00, -8.3129e+00, -8.2944e+00,\n",
      "         -7.6924e+00, -8.1609e+00],\n",
      "        [-6.9113e-03, -8.6388e+00, -8.6833e+00, -8.9716e+00, -8.6001e+00,\n",
      "         -8.2037e+00, -8.1933e+00, -8.5623e+00, -8.9124e+00, -8.6584e+00,\n",
      "         -8.3344e+00, -8.7198e+00, -8.4684e+00, -8.6555e+00, -8.6199e+00,\n",
      "         -8.5972e+00, -8.4749e+00, -8.6719e+00, -8.1059e+00, -8.0382e+00,\n",
      "         -8.4476e+00, -8.5827e+00, -8.2347e+00, -8.9819e+00, -8.8611e+00,\n",
      "         -8.9232e+00, -8.8400e+00, -8.4783e+00, -8.3708e+00, -8.7968e+00,\n",
      "         -8.3820e+00, -8.8862e+00, -8.9221e+00, -8.9265e+00, -8.7842e+00,\n",
      "         -8.2387e+00, -8.6848e+00],\n",
      "        [-1.4803e-02, -7.9108e+00, -7.8973e+00, -8.1420e+00, -7.9976e+00,\n",
      "         -7.5564e+00, -7.5146e+00, -7.7776e+00, -8.0242e+00, -7.8139e+00,\n",
      "         -7.5264e+00, -7.9102e+00, -7.7888e+00, -7.7848e+00, -7.9583e+00,\n",
      "         -7.8025e+00, -7.7018e+00, -7.8980e+00, -7.4174e+00, -7.2561e+00,\n",
      "         -7.5997e+00, -7.9104e+00, -7.5436e+00, -8.1636e+00, -8.1295e+00,\n",
      "         -8.2006e+00, -7.9893e+00, -7.7022e+00, -7.6448e+00, -8.0169e+00,\n",
      "         -7.6584e+00, -8.1103e+00, -8.1894e+00, -8.0393e+00, -7.9007e+00,\n",
      "         -7.5346e+00, -7.9352e+00]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([34, 21,  6,  0,  0], device='cuda:0')\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor([3, 3, 3, 3, 3, 3, 5, 5, 4, 3, 3, 3, 5, 2, 5, 4, 3, 3, 5, 3, 3, 4, 2, 5,\n",
      "        4, 4, 3, 2, 3, 3, 3, 4])\n",
      "torch.Size([32, 1024, 1, 7])\n",
      "here\n",
      "tensor([[-3.8289, -3.7563, -3.5437, -3.3466, -3.4238, -3.6933, -3.4984, -3.6793,\n",
      "         -3.3752, -3.8250, -3.7005, -3.8225, -3.6299, -3.4233, -3.6030, -3.7011,\n",
      "         -3.4210, -3.7354, -3.7030, -3.5172, -3.3932, -3.4620, -3.5235, -4.3430,\n",
      "         -3.6277, -3.8090, -3.8595, -3.8570, -3.4689, -3.4434, -3.3937, -3.5823,\n",
      "         -3.6555, -4.2122, -3.4539, -3.5153, -3.5637],\n",
      "        [-3.5181, -3.6530, -3.6947, -3.4423, -3.4503, -3.5836, -3.4745, -3.6861,\n",
      "         -3.4899, -3.7785, -3.6573, -3.7343, -3.5657, -3.5549, -3.5286, -3.5907,\n",
      "         -3.5381, -3.6553, -3.6009, -3.6314, -3.5448, -3.4427, -3.6453, -4.2536,\n",
      "         -3.5479, -3.7996, -3.8054, -3.7764, -3.5857, -3.4166, -3.5013, -3.5051,\n",
      "         -3.6147, -4.1178, -3.5247, -3.5333, -3.6357],\n",
      "        [-2.5039, -3.4931, -3.9745, -3.7239, -3.6192, -3.4024, -3.5483, -3.7557,\n",
      "         -3.7944, -3.7014, -3.6516, -3.6456, -3.4914, -3.8643, -3.5285, -3.4657,\n",
      "         -3.7907, -3.5838, -3.5027, -3.8485, -3.8579, -3.5186, -3.8797, -3.9089,\n",
      "         -3.4759, -3.8148, -3.7159, -3.6491, -3.8465, -3.5054, -3.7913, -3.4436,\n",
      "         -3.6387, -3.8243, -3.7275, -3.6566, -3.8677],\n",
      "        [-0.7416, -3.8906, -4.7956, -4.6022, -4.3820, -3.7315, -4.1757, -4.3159,\n",
      "         -4.6911, -4.1119, -4.1540, -4.1204, -3.9538, -4.6680, -4.1525, -3.8878,\n",
      "         -4.5337, -4.0933, -3.9554, -4.4645, -4.6166, -4.1535, -4.5538, -3.9407,\n",
      "         -4.0421, -4.3465, -4.1580, -4.0153, -4.5817, -4.2296, -4.5406, -4.0222,\n",
      "         -4.2947, -3.9870, -4.5106, -4.3056, -4.6471],\n",
      "        [-0.0851, -5.7245, -6.7454, -6.6450, -6.3820, -5.4723, -6.0644, -6.1555,\n",
      "         -6.7816, -5.8834, -5.9448, -5.9748, -5.8081, -6.5636, -6.1294, -5.7447,\n",
      "         -6.4408, -5.9760, -5.7058, -6.1826, -6.4881, -6.0674, -6.3592, -5.5323,\n",
      "         -6.0511, -6.2460, -6.0501, -5.7785, -6.4290, -6.2346, -6.3790, -6.0536,\n",
      "         -6.3263, -5.6831, -6.4944, -6.1531, -6.5641],\n",
      "        [-0.0239, -7.0350, -7.8941, -7.8871, -7.6704, -6.7472, -7.2488, -7.3816,\n",
      "         -8.0263, -7.0977, -7.1216, -7.2565, -7.0908, -7.6919, -7.4742, -7.0228,\n",
      "         -7.5890, -7.2392, -6.8825, -7.2287, -7.5927, -7.3668, -7.4912, -6.8171,\n",
      "         -7.4186, -7.5534, -7.3538, -7.0342, -7.5661, -7.5154, -7.5230, -7.4228,\n",
      "         -7.6823, -6.9635, -7.7088, -7.2992, -7.7173],\n",
      "        [-0.0352, -6.7509, -7.3831, -7.3942, -7.3799, -6.5026, -6.8314, -6.9850,\n",
      "         -7.4935, -6.6833, -6.6904, -6.9116, -6.7814, -7.1260, -7.1828, -6.6831,\n",
      "         -7.0799, -6.8858, -6.5481, -6.6965, -7.0022, -7.0726, -7.0718, -6.5595,\n",
      "         -7.1181, -7.2241, -6.9480, -6.7027, -7.0981, -7.1142, -7.0812, -7.1027,\n",
      "         -7.3491, -6.6292, -7.1810, -6.8602, -7.2385]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([30,  5, 12,  1,  0], device='cuda:0')\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor([4, 3, 4, 4, 4, 5, 3, 5, 3, 5, 5, 3, 4, 5, 3, 5, 5, 3, 4, 3, 3, 3, 5, 4,\n",
      "        5, 2, 3, 2, 5, 3, 2, 5])\n",
      "torch.Size([32, 1024, 1, 7])\n",
      "here\n",
      "tensor([[-4.0367, -3.6897, -3.5535, -3.4061, -3.4333, -3.6493, -3.4828, -3.6985,\n",
      "         -3.4337, -3.7710, -3.6865, -3.8124, -3.6060, -3.4310, -3.6130, -3.7378,\n",
      "         -3.3744, -3.7569, -3.6962, -3.5460, -3.4372, -3.3828, -3.5348, -4.3043,\n",
      "         -3.5899, -3.7446, -3.7583, -3.8377, -3.4230, -3.4918, -3.4549, -3.6144,\n",
      "         -3.6403, -4.1977, -3.4159, -3.4928, -3.6197],\n",
      "        [-3.8374, -3.7132, -3.5629, -3.3954, -3.3624, -3.6507, -3.4052, -3.7581,\n",
      "         -3.4442, -3.8469, -3.7197, -3.8641, -3.6000, -3.4303, -3.5751, -3.7844,\n",
      "         -3.3185, -3.7839, -3.6427, -3.5314, -3.4357, -3.3575, -3.5477, -4.5235,\n",
      "         -3.6005, -3.7651, -3.8150, -3.9077, -3.4013, -3.4580, -3.4193, -3.6209,\n",
      "         -3.6820, -4.3789, -3.4373, -3.4583, -3.6205],\n",
      "        [-2.9295, -3.8198, -3.5417, -3.4194, -3.3169, -3.6917, -3.3036, -3.8693,\n",
      "         -3.4697, -3.9845, -3.8225, -4.0026, -3.6308, -3.4518, -3.5550, -3.9165,\n",
      "         -3.2668, -3.8583, -3.5871, -3.4813, -3.3914, -3.4324, -3.5228, -4.8513,\n",
      "         -3.6868, -3.8519, -3.9485, -4.0408, -3.3752, -3.4935, -3.3555, -3.7082,\n",
      "         -3.8221, -4.6608, -3.5120, -3.4355, -3.6484],\n",
      "        [-0.4181, -5.0009, -4.4496, -4.5021, -4.3443, -4.7979, -4.1680, -4.9070,\n",
      "         -4.5505, -5.1679, -4.8921, -5.1539, -4.6744, -4.4688, -4.5718, -5.1572,\n",
      "         -4.2087, -4.9326, -4.5080, -4.1858, -4.2022, -4.6530, -4.2274, -6.1350,\n",
      "         -4.9301, -4.9799, -5.1159, -5.1062, -4.2468, -4.6991, -4.2436, -4.9624,\n",
      "         -5.0875, -5.9499, -4.6027, -4.3648, -4.6419],\n",
      "        [-0.0242, -7.7169, -7.0462, -7.3445, -7.0900, -7.4723, -6.8520, -7.5015,\n",
      "         -7.3294, -7.8585, -7.3949, -7.8000, -7.3861, -7.1682, -7.3376, -7.8905,\n",
      "         -6.9137, -7.6117, -7.0930, -6.6320, -6.8106, -7.3974, -6.6706, -8.6649,\n",
      "         -7.8094, -7.7146, -7.8477, -7.6623, -6.8367, -7.5694, -6.8794, -7.7948,\n",
      "         -7.8592, -8.5091, -7.3070, -6.9456, -7.3240],\n",
      "        [-0.0098, -8.5841, -7.9271, -8.3535, -8.0825, -8.3233, -7.8330, -8.3133,\n",
      "         -8.2247, -8.6524, -8.1364, -8.5884, -8.3099, -8.0687, -8.3092, -8.6894,\n",
      "         -7.8735, -8.4958, -7.9333, -7.5153, -7.7298, -8.3041, -7.5849, -9.3748,\n",
      "         -8.7487, -8.6694, -8.6882, -8.4584, -7.7599, -8.4938, -7.7992, -8.6935,\n",
      "         -8.7737, -9.1602, -8.1901, -7.8182, -8.2709],\n",
      "        [-0.0180, -7.8688, -7.4030, -7.7707, -7.6813, -7.6488, -7.3538, -7.6134,\n",
      "         -7.5802, -7.8448, -7.3995, -7.8072, -7.6934, -7.4438, -7.7981, -7.8820,\n",
      "         -7.3852, -7.8189, -7.3142, -6.9790, -7.1805, -7.7283, -7.1155, -8.4274,\n",
      "         -8.0842, -8.0868, -7.9109, -7.6947, -7.2666, -7.8119, -7.2930, -8.0233,\n",
      "         -8.1173, -8.1634, -7.5449, -7.2725, -7.7253]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([18, 12,  6,  0,  0], device='cuda:0')\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor([3, 3, 2, 5, 4, 5, 2, 4, 3, 3, 4, 5, 5, 3, 5, 3, 5, 5, 3, 2, 4, 5, 4, 4,\n",
      "        4, 3, 4, 4, 4, 3, 5, 4])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1024, 1, 7])\n",
      "here\n",
      "tensor([[-3.7243, -3.9630, -3.3583, -3.1902, -3.3420, -3.8593, -3.4688, -3.8180,\n",
      "         -3.2381, -4.0558, -3.8986, -4.0768, -3.7099, -3.2723, -3.6973, -3.9278,\n",
      "         -3.2108, -3.8913, -3.7609, -3.3825, -3.1905, -3.4474, -3.3607, -4.7275,\n",
      "         -3.7695, -3.9088, -4.1175, -4.0744, -3.3390, -3.3747, -3.2048, -3.7586,\n",
      "         -3.8399, -4.6208, -3.4401, -3.4788, -3.4448],\n",
      "        [-3.3735, -3.9734, -3.4371, -3.1856, -3.3212, -3.8456, -3.4100, -3.8485,\n",
      "         -3.2625, -4.1201, -3.8964, -4.0883, -3.6821, -3.3169, -3.6561, -3.8765,\n",
      "         -3.2380, -3.8772, -3.7043, -3.4192, -3.2185, -3.4353, -3.3953, -4.8837,\n",
      "         -3.7690, -3.9729, -4.1982, -4.0855, -3.3750, -3.3083, -3.1762, -3.8020,\n",
      "         -3.8791, -4.7445, -3.5068, -3.4658, -3.4384],\n",
      "        [-2.1103, -3.8161, -3.7248, -3.4823, -3.4629, -3.6910, -3.4255, -3.8760,\n",
      "         -3.5252, -4.1047, -3.8315, -4.0106, -3.6033, -3.6385, -3.6250, -3.7779,\n",
      "         -3.4929, -3.8276, -3.5888, -3.6255, -3.5088, -3.5051, -3.5557, -4.7382,\n",
      "         -3.7420, -4.0762, -4.1745, -3.9716, -3.5929, -3.3886, -3.4232, -3.8229,\n",
      "         -3.9224, -4.5895, -3.6866, -3.5590, -3.6598],\n",
      "        [-0.1755, -5.1149, -5.8080, -5.6769, -5.4500, -4.9530, -5.2107, -5.4594,\n",
      "         -5.6944, -5.4831, -5.2435, -5.4378, -5.0941, -5.7330, -5.3487, -5.2285,\n",
      "         -5.5412, -5.3536, -5.0650, -5.4096, -5.5930, -5.2358, -5.3630, -5.4915,\n",
      "         -5.4484, -5.7077, -5.5526, -5.1809, -5.5120, -5.3562, -5.4204, -5.5165,\n",
      "         -5.6604, -5.4847, -5.6720, -5.3437, -5.6992],\n",
      "        [-0.0196, -7.1235, -8.0711, -8.0756, -7.8325, -6.8920, -7.4229, -7.5538,\n",
      "         -8.1431, -7.3832, -7.2261, -7.4686, -7.2281, -7.9433, -7.6428, -7.2318,\n",
      "         -7.8109, -7.4575, -7.0864, -7.4597, -7.8536, -7.4109, -7.5723, -7.1780,\n",
      "         -7.6810, -7.8708, -7.5986, -7.1270, -7.7303, -7.6576, -7.6449, -7.7093,\n",
      "         -7.9023, -7.2241, -7.9816, -7.4898, -7.9631],\n",
      "        [-0.0104, -7.7851, -8.6842, -8.7501, -8.5570, -7.5418, -8.0405, -8.2020,\n",
      "         -8.8450, -7.9258, -7.8161, -8.1001, -7.9027, -8.5156, -8.3656, -7.8357,\n",
      "         -8.4062, -8.0901, -7.6919, -7.9996, -8.4248, -8.1131, -8.2314, -7.7615,\n",
      "         -8.3583, -8.5265, -8.2131, -7.7808, -8.3358, -8.3299, -8.2878, -8.3534,\n",
      "         -8.5969, -7.7938, -8.6122, -8.0810, -8.5582],\n",
      "        [-0.0198, -7.2599, -7.9451, -8.0035, -7.9961, -7.0518, -7.3760, -7.5712,\n",
      "         -8.0763, -7.2479, -7.1779, -7.4846, -7.3408, -7.7046, -7.7914, -7.2326,\n",
      "         -7.6610, -7.4691, -7.1106, -7.2477, -7.5959, -7.5895, -7.5964, -7.2242,\n",
      "         -7.7675, -7.8996, -7.5215, -7.2277, -7.6192, -7.6847, -7.6178, -7.7344,\n",
      "         -7.9761, -7.1992, -7.8232, -7.3920, -7.8210]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([36, 13, 26,  0,  0], device='cuda:0')\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor([3, 5, 4, 4, 3, 5, 4, 3, 5, 2, 2, 2, 4, 3, 5, 2, 5, 5, 3, 5, 4, 5, 5, 5,\n",
      "        3, 5, 3, 3, 5, 5, 5, 5])\n",
      "torch.Size([32, 1024, 1, 7])\n",
      "here\n",
      "tensor([[-4.1020, -3.8158, -3.4551, -3.3295, -3.4059, -3.7792, -3.4832, -3.7212,\n",
      "         -3.3267, -3.8489, -3.8326, -3.9074, -3.6254, -3.3048, -3.6666, -3.8617,\n",
      "         -3.2669, -3.8292, -3.7401, -3.4795, -3.2782, -3.4171, -3.4460, -4.4738,\n",
      "         -3.6763, -3.7632, -3.8617, -3.9574, -3.3705, -3.4656, -3.3969, -3.6617,\n",
      "         -3.7168, -4.3503, -3.3688, -3.4776, -3.5059],\n",
      "        [-3.9496, -3.9716, -3.3901, -3.2228, -3.3144, -3.9062, -3.4276, -3.8171,\n",
      "         -3.2569, -4.0305, -3.9709, -4.0455, -3.6818, -3.2221, -3.6781, -4.0039,\n",
      "         -3.1429, -3.9102, -3.7591, -3.4050, -3.1483, -3.4289, -3.3778, -4.8645,\n",
      "         -3.7707, -3.8462, -4.0352, -4.1436, -3.3101, -3.4111, -3.2606, -3.7416,\n",
      "         -3.8482, -4.7000, -3.3570, -3.4473, -3.4204],\n",
      "        [-3.3507, -4.1597, -3.3313, -3.1597, -3.2697, -4.0541, -3.3673, -3.9350,\n",
      "         -3.2174, -4.2273, -4.1278, -4.2122, -3.7628, -3.1625, -3.7134, -4.1641,\n",
      "         -3.0577, -4.0197, -3.7758, -3.3240, -3.0080, -3.5196, -3.3153, -5.3011,\n",
      "         -3.9129, -3.9817, -4.2157, -4.3410, -3.2700, -3.4181, -3.1314, -3.8787,\n",
      "         -4.0472, -5.0813, -3.3799, -3.4353, -3.3523],\n",
      "        [-2.0098, -4.4110, -3.3923, -3.2794, -3.3795, -4.2757, -3.3707, -4.1112,\n",
      "         -3.3209, -4.4626, -4.3350, -4.4173, -3.9304, -3.2313, -3.8582, -4.3860,\n",
      "         -3.1251, -4.2020, -3.8509, -3.2915, -2.9575, -3.7631, -3.3178, -5.7504,\n",
      "         -4.1700, -4.2218, -4.4222, -4.5361, -3.3260, -3.6111, -3.1377, -4.1342,\n",
      "         -4.3576, -5.4811, -3.4794, -3.5054, -3.4008],\n",
      "        [-0.6158, -5.0422, -4.0215, -4.0174, -4.0924, -4.9073, -3.9299, -4.7135,\n",
      "         -4.0432, -5.0870, -4.9038, -5.0195, -4.5882, -3.8867, -4.5520, -5.0413,\n",
      "         -3.8049, -4.8240, -4.4052, -3.7842, -3.5159, -4.4996, -3.8390, -6.3937,\n",
      "         -4.8951, -4.9190, -5.0182, -5.0681, -3.9312, -4.3891, -3.7533, -4.8477,\n",
      "         -5.0865, -6.1171, -4.1195, -4.0984, -4.0319],\n",
      "        [-0.1692, -5.9983, -5.1142, -5.2259, -5.2832, -5.8715, -5.0440, -5.6690,\n",
      "         -5.2209, -6.0647, -5.7775, -5.9706, -5.6376, -5.0248, -5.7015, -6.0299,\n",
      "         -4.9924, -5.8314, -5.4096, -4.7989, -4.6563, -5.6105, -4.8806, -7.1812,\n",
      "         -5.9830, -5.9991, -5.9683, -5.9504, -5.0290, -5.5519, -4.8713, -5.9174,\n",
      "         -6.1382, -6.8986, -5.2289, -5.1660, -5.1872],\n",
      "        [-0.1376, -5.9675, -5.3802, -5.5342, -5.6561, -5.8651, -5.3585, -5.6875,\n",
      "         -5.4629, -6.0212, -5.6994, -5.9257, -5.7691, -5.3074, -5.9342, -5.9810,\n",
      "         -5.3435, -5.8881, -5.5264, -5.0361, -5.0136, -5.7890, -5.1704, -6.7923,\n",
      "         -6.0840, -6.1360, -5.9369, -5.8645, -5.3194, -5.7290, -5.1870, -6.0160,\n",
      "         -6.2014, -6.4891, -5.4231, -5.3989, -5.5249]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([ 4, 30, 31,  3,  0], device='cuda:0')\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor([4, 4, 4, 3, 4, 5, 4, 4, 3, 5, 2, 4, 4, 2, 5, 4, 4, 5, 3, 3, 3, 3, 5, 5,\n",
      "        5, 3, 3, 3, 4, 4, 3, 3])\n",
      "torch.Size([32, 1024, 1, 7])\n",
      "here\n",
      "tensor([[-3.5228e+00, -3.8877e+00, -3.3632e+00, -3.3180e+00, -3.3448e+00,\n",
      "         -3.8013e+00, -3.3800e+00, -3.8445e+00, -3.3391e+00, -3.9566e+00,\n",
      "         -3.9181e+00, -4.0503e+00, -3.6631e+00, -3.2622e+00, -3.6573e+00,\n",
      "         -3.9830e+00, -3.1969e+00, -3.9143e+00, -3.6838e+00, -3.3960e+00,\n",
      "         -3.1876e+00, -3.4061e+00, -3.4439e+00, -4.7199e+00, -3.7656e+00,\n",
      "         -3.8419e+00, -3.9321e+00, -4.1089e+00, -3.2983e+00, -3.5323e+00,\n",
      "         -3.3083e+00, -3.7182e+00, -3.8165e+00, -4.6013e+00, -3.4710e+00,\n",
      "         -3.4496e+00, -3.5586e+00],\n",
      "        [-2.5719e+00, -4.3755e+00, -3.1936e+00, -3.1188e+00, -3.2648e+00,\n",
      "         -4.2577e+00, -3.3517e+00, -4.1448e+00, -3.1970e+00, -4.4004e+00,\n",
      "         -4.3369e+00, -4.4518e+00, -3.9278e+00, -3.1314e+00, -3.8027e+00,\n",
      "         -4.3929e+00, -3.0271e+00, -4.2142e+00, -3.8229e+00, -3.2171e+00,\n",
      "         -2.8696e+00, -3.6672e+00, -3.2202e+00, -5.6159e+00, -4.1428e+00,\n",
      "         -4.1450e+00, -4.4596e+00, -4.5833e+00, -3.2474e+00, -3.5999e+00,\n",
      "         -3.0280e+00, -4.0867e+00, -4.2577e+00, -5.4330e+00, -3.5652e+00,\n",
      "         -3.4773e+00, -3.4116e+00],\n",
      "        [-2.3385e-01, -6.1207e+00, -4.6195e+00, -4.6811e+00, -4.8603e+00,\n",
      "         -5.9753e+00, -4.6938e+00, -5.6982e+00, -4.7386e+00, -6.1097e+00,\n",
      "         -5.8874e+00, -6.0884e+00, -5.5241e+00, -4.6068e+00, -5.3671e+00,\n",
      "         -6.1474e+00, -4.5090e+00, -5.8754e+00, -5.2806e+00, -4.3783e+00,\n",
      "         -4.1156e+00, -5.4291e+00, -4.4015e+00, -7.5785e+00, -5.9566e+00,\n",
      "         -5.8460e+00, -6.1436e+00, -6.1429e+00, -4.6843e+00, -5.3892e+00,\n",
      "         -4.4256e+00, -5.9237e+00, -6.0877e+00, -7.3886e+00, -5.0967e+00,\n",
      "         -4.9109e+00, -4.8173e+00],\n",
      "        [-1.5965e-02, -8.5125e+00, -7.2402e+00, -7.5219e+00, -7.5325e+00,\n",
      "         -8.3251e+00, -7.2180e+00, -8.1147e+00, -7.4906e+00, -8.5525e+00,\n",
      "         -8.1407e+00, -8.5041e+00, -7.8812e+00, -7.3589e+00, -7.9143e+00,\n",
      "         -8.6241e+00, -7.1893e+00, -8.2932e+00, -7.7213e+00, -6.8364e+00,\n",
      "         -6.8321e+00, -7.9336e+00, -6.9256e+00, -9.7648e+00, -8.4963e+00,\n",
      "         -8.3511e+00, -8.5539e+00, -8.4449e+00, -7.2501e+00, -8.1253e+00,\n",
      "         -7.0932e+00, -8.4552e+00, -8.5637e+00, -9.5956e+00, -7.6711e+00,\n",
      "         -7.4178e+00, -7.5207e+00],\n",
      "        [-6.4716e-03, -9.2598e+00, -8.1563e+00, -8.5922e+00, -8.4633e+00,\n",
      "         -9.0344e+00, -8.1456e+00, -8.8828e+00, -8.4545e+00, -9.2991e+00,\n",
      "         -8.8241e+00, -9.2745e+00, -8.6827e+00, -8.3538e+00, -8.7665e+00,\n",
      "         -9.3777e+00, -8.1722e+00, -9.0835e+00, -8.4991e+00, -7.7475e+00,\n",
      "         -7.8670e+00, -8.7669e+00, -7.8949e+00, -1.0356e+01, -9.3489e+00,\n",
      "         -9.2197e+00, -9.3293e+00, -9.1759e+00, -8.1722e+00, -9.0416e+00,\n",
      "         -8.0592e+00, -9.2517e+00, -9.3640e+00, -1.0148e+01, -8.5668e+00,\n",
      "         -8.2722e+00, -8.5344e+00],\n",
      "        [-5.6505e-03, -9.2669e+00, -8.3441e+00, -8.8454e+00, -8.6393e+00,\n",
      "         -9.0271e+00, -8.3476e+00, -8.9093e+00, -8.6268e+00, -9.2727e+00,\n",
      "         -8.7970e+00, -9.2612e+00, -8.7600e+00, -8.5479e+00, -8.8751e+00,\n",
      "         -9.3444e+00, -8.3957e+00, -9.1307e+00, -8.5473e+00, -7.9614e+00,\n",
      "         -8.1354e+00, -8.8619e+00, -8.1186e+00, -1.0194e+01, -9.4184e+00,\n",
      "         -9.3325e+00, -9.3307e+00, -9.1398e+00, -8.3288e+00, -9.1238e+00,\n",
      "         -8.2646e+00, -9.3133e+00, -9.4248e+00, -9.9495e+00, -8.7022e+00,\n",
      "         -8.3868e+00, -8.7523e+00],\n",
      "        [-1.2779e-02, -8.2682e+00, -7.6642e+00, -8.0843e+00, -8.0156e+00,\n",
      "         -8.0659e+00, -7.6608e+00, -7.9818e+00, -7.8328e+00, -8.2168e+00,\n",
      "         -7.8126e+00, -8.2272e+00, -7.9204e+00, -7.7557e+00, -8.1243e+00,\n",
      "         -8.2834e+00, -7.7176e+00, -8.2026e+00, -7.6903e+00, -7.2663e+00,\n",
      "         -7.4613e+00, -8.0566e+00, -7.4591e+00, -8.9298e+00, -8.4881e+00,\n",
      "         -8.4992e+00, -8.3008e+00, -8.0928e+00, -7.6072e+00, -8.1856e+00,\n",
      "         -7.5762e+00, -8.4155e+00, -8.5097e+00, -8.6455e+00, -7.8699e+00,\n",
      "         -7.6197e+00, -8.0189e+00]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([25,  8,  0,  0,  0], device='cuda:0')\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor([2, 5, 5, 5, 5, 3, 2, 2, 3, 5, 4, 3, 5, 4, 4, 3, 4, 5, 4, 3, 5, 3, 4, 5,\n",
      "        3, 3, 2, 3, 3, 5, 3, 5])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1024, 1, 7])\n",
      "here\n",
      "tensor([[-4.1804, -3.7671, -3.5010, -3.3427, -3.4473, -3.7484, -3.5146, -3.6856,\n",
      "         -3.3412, -3.7912, -3.7390, -3.8320, -3.6395, -3.3504, -3.6410, -3.7246,\n",
      "         -3.3777, -3.7176, -3.7650, -3.5170, -3.3546, -3.4245, -3.4954, -4.3160,\n",
      "         -3.6608, -3.7626, -3.8458, -3.8583, -3.4511, -3.4320, -3.4244, -3.6313,\n",
      "         -3.6423, -4.2229, -3.3725, -3.5149, -3.5436],\n",
      "        [-4.1013, -3.7087, -3.5990, -3.3821, -3.4329, -3.6912, -3.4680, -3.6851,\n",
      "         -3.4103, -3.7811, -3.6970, -3.7796, -3.5902, -3.4222, -3.5636, -3.6419,\n",
      "         -3.4465, -3.6403, -3.7057, -3.5895, -3.4477, -3.3869, -3.5779, -4.3389,\n",
      "         -3.6142, -3.7639, -3.8230, -3.8160, -3.5146, -3.3871, -3.4879, -3.5847,\n",
      "         -3.6059, -4.2288, -3.3978, -3.5085, -3.5774],\n",
      "        [-3.5863, -3.4984, -3.8107, -3.5797, -3.5021, -3.4794, -3.4536, -3.6853,\n",
      "         -3.6412, -3.6744, -3.6107, -3.6545, -3.4630, -3.6617, -3.4704, -3.4701,\n",
      "         -3.6592, -3.4925, -3.5665, -3.7774, -3.7190, -3.3751, -3.7714, -4.1062,\n",
      "         -3.5024, -3.7667, -3.7010, -3.6457, -3.7087, -3.4081, -3.7282, -3.4864,\n",
      "         -3.5376, -3.9801, -3.5299, -3.5633, -3.7467],\n",
      "        [-1.9350, -3.3764, -4.2573, -4.0511, -3.8215, -3.2921, -3.6198, -3.8468,\n",
      "         -4.1356, -3.6371, -3.6723, -3.6684, -3.3601, -4.1575, -3.5838, -3.3657,\n",
      "         -4.0649, -3.5001, -3.5507, -4.0883, -4.1784, -3.5677, -4.1145, -3.7286,\n",
      "         -3.5427, -3.9065, -3.6618, -3.5103, -4.1160, -3.6508, -4.1569, -3.5733,\n",
      "         -3.6911, -3.6121, -3.9125, -3.8099, -4.1824],\n",
      "        [-0.1765, -4.9780, -6.0741, -5.9704, -5.6692, -4.7615, -5.3285, -5.4745,\n",
      "         -6.0888, -5.1837, -5.2834, -5.3451, -4.8922, -5.9569, -5.3420, -5.0174,\n",
      "         -5.8388, -5.2367, -5.0459, -5.6091, -5.8887, -5.2973, -5.7218, -5.0726,\n",
      "         -5.3812, -5.6551, -5.3259, -5.0091, -5.7720, -5.5206, -5.7646, -5.4644,\n",
      "         -5.5610, -5.0435, -5.8035, -5.4824, -5.9899],\n",
      "        [-0.0259, -6.9133, -7.7707, -7.7979, -7.5689, -6.6320, -7.1328, -7.3089,\n",
      "         -7.9317, -7.0144, -7.0479, -7.2336, -6.7861, -7.6544, -7.3041, -6.9501,\n",
      "         -7.5676, -7.1487, -6.7791, -7.1849, -7.5358, -7.1920, -7.3944, -6.9828,\n",
      "         -7.3994, -7.6081, -7.2780, -6.8584, -7.4336, -7.4266, -7.4248, -7.4814,\n",
      "         -7.5579, -6.9657, -7.6420, -7.2117, -7.7098],\n",
      "        [-0.0283, -6.9485, -7.5289, -7.5983, -7.5751, -6.6979, -7.0142, -7.2118,\n",
      "         -7.6686, -6.9017, -6.9007, -7.1760, -6.8414, -7.3580, -7.3369, -6.9245,\n",
      "         -7.3171, -7.1200, -6.7329, -6.9039, -7.2100, -7.2162, -7.2461, -7.0050,\n",
      "         -7.3970, -7.5575, -7.1804, -6.8556, -7.2250, -7.3293, -7.2400, -7.4366,\n",
      "         -7.5453, -6.9407, -7.4124, -7.0494, -7.4737]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([ 5, 16, 15,  1,  0], device='cuda:0')\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor([4, 5, 4, 5, 5, 3, 2, 2, 3, 3, 5, 3, 3, 4, 5, 3, 5, 4, 5, 4, 2, 5, 4, 4,\n",
      "        3, 3, 5, 3, 2, 4, 5, 3])\n",
      "torch.Size([32, 1024, 1, 7])\n",
      "here\n",
      "tensor([[-3.6316e+00, -3.3695e+00, -3.8312e+00, -3.7265e+00, -3.5652e+00,\n",
      "         -3.3199e+00, -3.4751e+00, -3.6808e+00, -3.7592e+00, -3.6186e+00,\n",
      "         -3.5032e+00, -3.6762e+00, -3.4705e+00, -3.7625e+00, -3.5080e+00,\n",
      "         -3.4903e+00, -3.7081e+00, -3.4898e+00, -3.5410e+00, -3.8093e+00,\n",
      "         -3.8445e+00, -3.3284e+00, -3.8299e+00, -3.7936e+00, -3.4432e+00,\n",
      "         -3.7106e+00, -3.6311e+00, -3.5723e+00, -3.6848e+00, -3.5250e+00,\n",
      "         -3.8430e+00, -3.5121e+00, -3.4630e+00, -3.8165e+00, -3.6081e+00,\n",
      "         -3.6018e+00, -3.9000e+00],\n",
      "        [-2.3315e+00, -3.3411e+00, -4.0161e+00, -3.9502e+00, -3.6242e+00,\n",
      "         -3.2139e+00, -3.4429e+00, -3.8368e+00, -4.0044e+00, -3.7130e+00,\n",
      "         -3.5653e+00, -3.7831e+00, -3.3305e+00, -4.0105e+00, -3.5217e+00,\n",
      "         -3.5648e+00, -3.8430e+00, -3.5581e+00, -3.4980e+00, -3.9295e+00,\n",
      "         -4.0141e+00, -3.3594e+00, -3.9147e+00, -3.8967e+00, -3.4560e+00,\n",
      "         -3.8345e+00, -3.7557e+00, -3.5852e+00, -3.8199e+00, -3.5527e+00,\n",
      "         -3.9772e+00, -3.6270e+00, -3.5861e+00, -3.8972e+00, -3.8131e+00,\n",
      "         -3.6978e+00, -4.1115e+00],\n",
      "        [-7.1557e-02, -6.0228e+00, -6.5342e+00, -6.6788e+00, -6.2048e+00,\n",
      "         -5.7223e+00, -5.8969e+00, -6.3228e+00, -6.6939e+00, -6.2717e+00,\n",
      "         -6.0966e+00, -6.4754e+00, -5.7338e+00, -6.5482e+00, -6.0645e+00,\n",
      "         -6.3245e+00, -6.4150e+00, -6.2942e+00, -5.8766e+00, -6.1032e+00,\n",
      "         -6.3723e+00, -5.9770e+00, -6.1737e+00, -6.5701e+00, -6.3224e+00,\n",
      "         -6.5688e+00, -6.5128e+00, -6.0738e+00, -6.1715e+00, -6.3141e+00,\n",
      "         -6.2550e+00, -6.5621e+00, -6.3715e+00, -6.6259e+00, -6.5802e+00,\n",
      "         -6.1527e+00, -6.5970e+00],\n",
      "        [-7.5612e-03, -8.3859e+00, -8.5729e+00, -8.9077e+00, -8.4618e+00,\n",
      "         -8.0352e+00, -8.0579e+00, -8.5562e+00, -8.8550e+00, -8.5465e+00,\n",
      "         -8.2515e+00, -8.7558e+00, -8.0596e+00, -8.6380e+00, -8.4137e+00,\n",
      "         -8.6430e+00, -8.4937e+00, -8.5877e+00, -7.9937e+00, -8.0397e+00,\n",
      "         -8.3806e+00, -8.2611e+00, -8.2139e+00, -9.0022e+00, -8.7617e+00,\n",
      "         -8.9216e+00, -8.8914e+00, -8.3252e+00, -8.2526e+00, -8.6261e+00,\n",
      "         -8.2982e+00, -8.9699e+00, -8.7531e+00, -8.9783e+00, -8.7836e+00,\n",
      "         -8.2387e+00, -8.6864e+00],\n",
      "        [-4.0455e-03, -9.1121e+00, -9.0860e+00, -9.5284e+00, -9.0868e+00,\n",
      "         -8.7481e+00, -8.6661e+00, -9.1920e+00, -9.3894e+00, -9.1959e+00,\n",
      "         -8.8520e+00, -9.3989e+00, -8.8019e+00, -9.1947e+00, -9.1050e+00,\n",
      "         -9.3098e+00, -9.0269e+00, -9.2498e+00, -8.6066e+00, -8.5653e+00,\n",
      "         -8.8967e+00, -8.9372e+00, -8.7681e+00, -9.7668e+00, -9.4662e+00,\n",
      "         -9.5907e+00, -9.5490e+00, -9.0072e+00, -8.8258e+00, -9.2807e+00,\n",
      "         -8.8656e+00, -9.6288e+00, -9.4631e+00, -9.6520e+00, -9.3530e+00,\n",
      "         -8.8149e+00, -9.2602e+00],\n",
      "        [-4.2343e-03, -9.1131e+00, -8.9843e+00, -9.4629e+00, -9.0684e+00,\n",
      "         -8.7760e+00, -8.6551e+00, -9.1236e+00, -9.2681e+00, -9.1300e+00,\n",
      "         -8.7965e+00, -9.3198e+00, -8.8493e+00, -9.1019e+00, -9.1105e+00,\n",
      "         -9.2517e+00, -8.9535e+00, -9.2047e+00, -8.5785e+00, -8.4991e+00,\n",
      "         -8.8028e+00, -8.9552e+00, -8.7022e+00, -9.7405e+00, -9.4556e+00,\n",
      "         -9.5506e+00, -9.4611e+00, -8.9811e+00, -8.7642e+00, -9.2388e+00,\n",
      "         -8.8145e+00, -9.5615e+00, -9.4630e+00, -9.5615e+00, -9.2382e+00,\n",
      "         -8.7626e+00, -9.2013e+00],\n",
      "        [-1.1438e-02, -8.0955e+00, -8.0156e+00, -8.4061e+00, -8.2558e+00,\n",
      "         -7.8430e+00, -7.7761e+00, -8.0852e+00, -8.1959e+00, -8.0334e+00,\n",
      "         -7.7707e+00, -8.2328e+00, -7.9355e+00, -8.0480e+00, -8.2195e+00,\n",
      "         -8.1608e+00, -7.9941e+00, -8.1808e+00, -7.6469e+00, -7.5238e+00,\n",
      "         -7.7905e+00, -8.0800e+00, -7.7941e+00, -8.5797e+00, -8.4477e+00,\n",
      "         -8.5616e+00, -8.3192e+00, -7.9415e+00, -7.8362e+00, -8.2081e+00,\n",
      "         -7.8820e+00, -8.5009e+00, -8.4815e+00, -8.3581e+00, -8.1489e+00,\n",
      "         -7.8161e+00, -8.2255e+00]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([31,  7,  0,  0,  0], device='cuda:0')\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor([2, 4, 5, 2, 2, 4, 3, 5, 5, 3, 4, 3, 5, 5, 4, 3, 2, 4, 4, 2, 4, 3, 4, 4,\n",
      "        5, 4, 5, 2, 5, 5, 5, 4])\n",
      "torch.Size([32, 1024, 1, 7])\n",
      "here\n",
      "tensor([[-4.0442e+00, -3.3753e+00, -3.8543e+00, -3.7250e+00, -3.6353e+00,\n",
      "         -3.3763e+00, -3.5822e+00, -3.6289e+00, -3.7772e+00, -3.4923e+00,\n",
      "         -3.4908e+00, -3.5419e+00, -3.5437e+00, -3.7383e+00, -3.5820e+00,\n",
      "         -3.4618e+00, -3.7405e+00, -3.5260e+00, -3.5969e+00, -3.8374e+00,\n",
      "         -3.8147e+00, -3.3989e+00, -3.8661e+00, -3.5943e+00, -3.4609e+00,\n",
      "         -3.6340e+00, -3.5019e+00, -3.5140e+00, -3.7071e+00, -3.6106e+00,\n",
      "         -3.8616e+00, -3.4436e+00, -3.4506e+00, -3.6064e+00, -3.5675e+00,\n",
      "         -3.5898e+00, -3.8918e+00],\n",
      "        [-3.9765e+00, -3.4019e+00, -3.8492e+00, -3.7118e+00, -3.5711e+00,\n",
      "         -3.3419e+00, -3.4749e+00, -3.6555e+00, -3.7641e+00, -3.5671e+00,\n",
      "         -3.5136e+00, -3.5929e+00, -3.5041e+00, -3.7210e+00, -3.5412e+00,\n",
      "         -3.4951e+00, -3.6791e+00, -3.5664e+00, -3.5475e+00, -3.8328e+00,\n",
      "         -3.8026e+00, -3.2964e+00, -3.8941e+00, -3.8337e+00, -3.4499e+00,\n",
      "         -3.6424e+00, -3.5652e+00, -3.5601e+00, -3.6856e+00, -3.5388e+00,\n",
      "         -3.8582e+00, -3.4232e+00, -3.4241e+00, -3.7889e+00, -3.5495e+00,\n",
      "         -3.5748e+00, -3.9210e+00],\n",
      "        [-3.2868e+00, -3.4965e+00, -3.8133e+00, -3.6762e+00, -3.4965e+00,\n",
      "         -3.3435e+00, -3.3267e+00, -3.7141e+00, -3.7147e+00, -3.6985e+00,\n",
      "         -3.5897e+00, -3.7364e+00, -3.4488e+00, -3.7026e+00, -3.5011e+00,\n",
      "         -3.6071e+00, -3.6069e+00, -3.6578e+00, -3.5134e+00, -3.7798e+00,\n",
      "         -3.7474e+00, -3.2178e+00, -3.8816e+00, -4.1988e+00, -3.4885e+00,\n",
      "         -3.7177e+00, -3.7060e+00, -3.6613e+00, -3.6614e+00, -3.4677e+00,\n",
      "         -3.7661e+00, -3.4780e+00, -3.4633e+00, -4.0799e+00, -3.5828e+00,\n",
      "         -3.5610e+00, -3.9391e+00],\n",
      "        [-6.4288e-01, -4.3609e+00, -4.4033e+00, -4.3754e+00, -4.1425e+00,\n",
      "         -4.0821e+00, -3.8065e+00, -4.4340e+00, -4.3446e+00, -4.5288e+00,\n",
      "         -4.3788e+00, -4.6514e+00, -4.0233e+00, -4.3303e+00, -4.1495e+00,\n",
      "         -4.5100e+00, -4.2244e+00, -4.4769e+00, -4.1650e+00, -4.1607e+00,\n",
      "         -4.2291e+00, -3.9772e+00, -4.3307e+00, -5.2982e+00, -4.3821e+00,\n",
      "         -4.5725e+00, -4.5735e+00, -4.4231e+00, -4.2057e+00, -4.2595e+00,\n",
      "         -4.1972e+00, -4.4599e+00, -4.3479e+00, -5.1406e+00, -4.3620e+00,\n",
      "         -4.1987e+00, -4.5887e+00],\n",
      "        [-2.7039e-02, -7.3728e+00, -7.1378e+00, -7.3368e+00, -7.0502e+00,\n",
      "         -7.0693e+00, -6.6417e+00, -7.2924e+00, -7.2554e+00, -7.4916e+00,\n",
      "         -7.1888e+00, -7.6295e+00, -6.9570e+00, -7.1412e+00, -7.1279e+00,\n",
      "         -7.5298e+00, -7.0467e+00, -7.4223e+00, -6.9575e+00, -6.6832e+00,\n",
      "         -6.9278e+00, -7.0049e+00, -6.8632e+00, -8.2681e+00, -7.5728e+00,\n",
      "         -7.5888e+00, -7.6309e+00, -7.2741e+00, -6.8996e+00, -7.3678e+00,\n",
      "         -6.9028e+00, -7.6564e+00, -7.4666e+00, -8.1285e+00, -7.3021e+00,\n",
      "         -6.9663e+00, -7.3390e+00],\n",
      "        [-8.9507e-03, -8.5124e+00, -8.1394e+00, -8.4935e+00, -8.2247e+00,\n",
      "         -8.2429e+00, -7.8115e+00, -8.3659e+00, -8.3221e+00, -8.5599e+00,\n",
      "         -8.1898e+00, -8.6896e+00, -8.1635e+00, -8.2007e+00, -8.3487e+00,\n",
      "         -8.6195e+00, -8.1138e+00, -8.5539e+00, -8.0018e+00, -7.6946e+00,\n",
      "         -7.9498e+00, -8.1934e+00, -7.8789e+00, -9.3376e+00, -8.7802e+00,\n",
      "         -8.7812e+00, -8.7480e+00, -8.3555e+00, -7.9630e+00, -8.5223e+00,\n",
      "         -7.9577e+00, -8.8199e+00, -8.6894e+00, -9.1154e+00, -8.3725e+00,\n",
      "         -8.0129e+00, -8.4102e+00],\n",
      "        [-1.6015e-02, -7.8719e+00, -7.5879e+00, -7.9178e+00, -7.8437e+00,\n",
      "         -7.6671e+00, -7.3860e+00, -7.7347e+00, -7.7048e+00, -7.8262e+00,\n",
      "         -7.5150e+00, -7.9711e+00, -7.6610e+00, -7.5939e+00, -7.8885e+00,\n",
      "         -7.9110e+00, -7.5855e+00, -7.9320e+00, -7.4152e+00, -7.1439e+00,\n",
      "         -7.3697e+00, -7.7269e+00, -7.3784e+00, -8.4944e+00, -8.1762e+00,\n",
      "         -8.2333e+00, -8.0308e+00, -7.6918e+00, -7.4536e+00, -7.8934e+00,\n",
      "         -7.4405e+00, -8.1824e+00, -8.1430e+00, -8.2165e+00, -7.7302e+00,\n",
      "         -7.4739e+00, -7.8572e+00]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([18, 11, 24,  0,  0], device='cuda:0')\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor([3, 4, 2, 3, 3, 4, 2, 2, 3, 4, 4, 4, 4, 3, 3, 3, 2, 5, 5, 5, 5, 5, 3, 2,\n",
      "        3, 2, 5, 4, 3, 3, 4, 4])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1024, 1, 7])\n",
      "here\n",
      "tensor([[-3.8395, -3.9088, -3.3477, -3.2539, -3.3789, -3.8626, -3.4766, -3.7781,\n",
      "         -3.2222, -3.9727, -3.8837, -4.0056, -3.6658, -3.2769, -3.6857, -3.9337,\n",
      "         -3.2352, -3.8955, -3.7722, -3.3968, -3.2085, -3.3764, -3.4437, -4.7017,\n",
      "         -3.7360, -3.8404, -3.9701, -4.0407, -3.3672, -3.3953, -3.2705, -3.7368,\n",
      "         -3.8009, -4.5766, -3.3801, -3.5142, -3.4915],\n",
      "        [-3.6336, -3.9223, -3.4012, -3.2526, -3.3333, -3.8791, -3.4064, -3.8143,\n",
      "         -3.2239, -4.0160, -3.8997, -4.0123, -3.6280, -3.2971, -3.6440, -3.9518,\n",
      "         -3.2385, -3.9110, -3.7479, -3.4216, -3.2118, -3.3304, -3.4851, -4.9149,\n",
      "         -3.7401, -3.8669, -3.9944, -4.0753, -3.3808, -3.3655, -3.2571, -3.7705,\n",
      "         -3.8260, -4.7310, -3.3740, -3.4951, -3.4998],\n",
      "        [-2.7946, -3.9256, -3.5015, -3.3423, -3.3487, -3.8469, -3.3213, -3.8694,\n",
      "         -3.2848, -4.0457, -3.9269, -4.0393, -3.5664, -3.3718, -3.6205, -3.9958,\n",
      "         -3.3133, -3.9455, -3.7066, -3.4703, -3.2529, -3.3327, -3.5610, -5.1191,\n",
      "         -3.7835, -3.9381, -4.0098, -4.0721, -3.4370, -3.4248, -3.2996, -3.8397,\n",
      "         -3.8768, -4.8525, -3.4198, -3.4944, -3.5801],\n",
      "        [-0.9013, -4.4191, -3.9591, -3.8467, -3.8363, -4.2848, -3.6552, -4.3477,\n",
      "         -3.7873, -4.5330, -4.4039, -4.5604, -3.9460, -3.8195, -4.0789, -4.5287,\n",
      "         -3.7897, -4.4166, -4.0781, -3.7883, -3.6131, -3.8752, -3.9117, -5.7432,\n",
      "         -4.3604, -4.4919, -4.5052, -4.4616, -3.8354, -4.0063, -3.6772, -4.4406,\n",
      "         -4.4542, -5.4097, -3.9190, -3.9022, -4.0467],\n",
      "        [-0.1047, -6.2934, -5.6436, -5.7006, -5.7015, -6.1427, -5.4197, -6.1079,\n",
      "         -5.6462, -6.3848, -6.1357, -6.4299, -5.7765, -5.5566, -5.9629, -6.4173,\n",
      "         -5.5739, -6.2589, -5.8054, -5.3476, -5.2841, -5.8070, -5.4799, -7.5776,\n",
      "         -6.3520, -6.3862, -6.3840, -6.2147, -5.5303, -5.9640, -5.3689, -6.3924,\n",
      "         -6.3993, -7.2366, -5.7616, -5.6311, -5.7923],\n",
      "        [-0.0317, -7.4686, -6.7174, -6.9518, -6.9425, -7.3200, -6.6425, -7.2026,\n",
      "         -6.8252, -7.5281, -7.1722, -7.5681, -7.0042, -6.7149, -7.2132, -7.5556,\n",
      "         -6.7522, -7.4425, -6.9273, -6.4021, -6.4205, -7.0649, -6.5457, -8.6004,\n",
      "         -7.6041, -7.6055, -7.5472, -7.3222, -6.6730, -7.1929, -6.5069, -7.6001,\n",
      "         -7.6225, -8.2477, -6.9286, -6.7557, -6.9676],\n",
      "        [-0.0443, -6.9959, -6.4467, -6.6992, -6.7945, -6.8827, -6.4519, -6.7656,\n",
      "         -6.5219, -7.0089, -6.6640, -7.0594, -6.7032, -6.4359, -6.9601, -7.0380,\n",
      "         -6.5172, -7.0160, -6.5523, -6.1171, -6.1870, -6.7943, -6.3100, -7.8194,\n",
      "         -7.2092, -7.2639, -7.0574, -6.8185, -6.4271, -6.8220, -6.2900, -7.1937,\n",
      "         -7.2458, -7.4500, -6.5795, -6.4727, -6.7246]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([16, 10, 15,  2,  0], device='cuda:0')\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor([4, 4, 4, 4, 5, 4, 3, 3, 5, 5, 2, 4, 4, 3, 2, 5, 5, 3, 4, 4, 5, 4, 3, 5,\n",
      "        4, 3, 3, 3, 2, 2, 3, 5])\n",
      "torch.Size([32, 1024, 1, 7])\n",
      "here\n",
      "tensor([[-3.7116, -3.3267, -4.0086, -3.9084, -3.7999, -3.3203, -3.7514, -3.6747,\n",
      "         -3.9584, -3.3647, -3.4946, -3.4506, -3.5647, -3.8602, -3.7015, -3.3801,\n",
      "         -3.8887, -3.5041, -3.5886, -3.9693, -3.9691, -3.5339, -4.0468, -3.0377,\n",
      "         -3.4686, -3.5698, -3.4058, -3.3975, -3.8852, -3.7925, -3.9917, -3.4792,\n",
      "         -3.4445, -3.1452, -3.7216, -3.7333, -4.0063],\n",
      "        [-3.9228, -3.5540, -3.6597, -3.6373, -3.5830, -3.4745, -3.5272, -3.6740,\n",
      "         -3.5935, -3.5905, -3.6414, -3.6327, -3.5080, -3.5217, -3.6500, -3.6037,\n",
      "         -3.5042, -3.7071, -3.5702, -3.7118, -3.5793, -3.3602, -3.7923, -3.8840,\n",
      "         -3.5288, -3.6038, -3.5863, -3.6702, -3.5822, -3.5596, -3.7303, -3.4929,\n",
      "         -3.5134, -3.8171, -3.4949, -3.5855, -3.8010],\n",
      "        [-3.5496, -3.9549, -3.3305, -3.2794, -3.3451, -3.8484, -3.3917, -3.8537,\n",
      "         -3.2146, -4.0279, -3.9525, -4.0047, -3.6389, -3.2374, -3.6802, -4.0304,\n",
      "         -3.1560, -3.9760, -3.7035, -3.4297, -3.1900, -3.3460, -3.4922, -4.8872,\n",
      "         -3.7695, -3.8096, -3.9985, -4.1207, -3.3450, -3.4050, -3.3241, -3.7345,\n",
      "         -3.8251, -4.6887, -3.4161, -3.5282, -3.5346],\n",
      "        [-2.4659, -4.3404, -3.2432, -3.1425, -3.3129, -4.2277, -3.3516, -4.0908,\n",
      "         -3.0413, -4.4349, -4.2664, -4.3816, -3.8523, -3.1863, -3.8130, -4.3801,\n",
      "         -3.0825, -4.2432, -3.8716, -3.2973, -2.9771, -3.5495, -3.3478, -5.6550,\n",
      "         -4.0912, -4.1032, -4.4076, -4.4713, -3.3023, -3.4656, -3.1025, -4.1186,\n",
      "         -4.2083, -5.3831, -3.4961, -3.5812, -3.3916],\n",
      "        [-0.6491, -5.1111, -3.8871, -3.8367, -3.9999, -5.0064, -3.8904, -4.7571,\n",
      "         -3.6691, -5.1975, -4.9225, -5.1440, -4.5795, -3.8517, -4.5368, -5.1540,\n",
      "         -3.7843, -4.9614, -4.5074, -3.7666, -3.4690, -4.3442, -3.8070, -6.5774,\n",
      "         -4.9226, -4.9078, -5.1629, -5.0785, -3.9046, -4.2804, -3.6462, -5.0165,\n",
      "         -5.0696, -6.2792, -4.1630, -4.1842, -3.9462],\n",
      "        [-0.0825, -6.7127, -5.7525, -5.8577, -5.9222, -6.5911, -5.6779, -6.3563,\n",
      "         -5.6557, -6.8094, -6.4002, -6.7607, -6.2186, -5.7658, -6.3453, -6.8003,\n",
      "         -5.7442, -6.6001, -6.1595, -5.4890, -5.3721, -6.1409, -5.5288, -7.9161,\n",
      "         -6.6850, -6.6736, -6.7588, -6.5129, -5.6918, -6.2006, -5.5116, -6.7986,\n",
      "         -6.7876, -7.6584, -5.9941, -5.9220, -5.8545],\n",
      "        [-0.0557, -6.8106, -6.2355, -6.4274, -6.5371, -6.6891, -6.1966, -6.5280,\n",
      "         -6.1954, -6.8739, -6.4581, -6.8630, -6.5039, -6.2516, -6.7483, -6.8748,\n",
      "         -6.3009, -6.7863, -6.3848, -5.9207, -5.9500, -6.5451, -6.0335, -7.5912,\n",
      "         -6.9723, -7.0042, -6.8669, -6.5654, -6.1767, -6.5694, -6.0613, -7.0229,\n",
      "         -7.0249, -7.3207, -6.3749, -6.3087, -6.4343]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([ 6, 24,  4, 26,  0], device='cuda:0')\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor([4, 5, 4, 3, 4, 3, 4, 2, 2, 4, 5, 4, 3, 3, 4, 5, 4, 5, 3, 4, 5, 5, 3, 2,\n",
      "        2, 5, 4, 2, 3, 4, 4, 2])\n",
      "torch.Size([32, 1024, 1, 7])\n",
      "here\n",
      "tensor([[-3.8578e+00, -3.4178e+00, -3.8375e+00, -3.6961e+00, -3.5840e+00,\n",
      "         -3.4328e+00, -3.5822e+00, -3.6393e+00, -3.7308e+00, -3.5568e+00,\n",
      "         -3.4777e+00, -3.5123e+00, -3.4986e+00, -3.7170e+00, -3.5222e+00,\n",
      "         -3.4735e+00, -3.7363e+00, -3.5249e+00, -3.5783e+00, -3.8030e+00,\n",
      "         -3.7824e+00, -3.3462e+00, -3.8231e+00, -3.7874e+00, -3.4729e+00,\n",
      "         -3.7117e+00, -3.5648e+00, -3.5171e+00, -3.6947e+00, -3.5626e+00,\n",
      "         -3.8594e+00, -3.4583e+00, -3.4488e+00, -3.7853e+00, -3.5480e+00,\n",
      "         -3.5864e+00, -3.8608e+00],\n",
      "        [-3.5209e+00, -3.3505e+00, -3.9944e+00, -3.8204e+00, -3.5908e+00,\n",
      "         -3.3334e+00, -3.5719e+00, -3.6963e+00, -3.8718e+00, -3.5732e+00,\n",
      "         -3.4531e+00, -3.4488e+00, -3.4088e+00, -3.8610e+00, -3.4669e+00,\n",
      "         -3.4283e+00, -3.8396e+00, -3.4827e+00, -3.4951e+00, -3.9143e+00,\n",
      "         -3.9371e+00, -3.2836e+00, -3.9591e+00, -3.7732e+00, -3.4135e+00,\n",
      "         -3.7286e+00, -3.5713e+00, -3.4294e+00, -3.8115e+00, -3.5400e+00,\n",
      "         -4.0037e+00, -3.4356e+00, -3.3978e+00, -3.7671e+00, -3.6294e+00,\n",
      "         -3.6410e+00, -3.9745e+00],\n",
      "        [-1.9560e+00, -3.4029e+00, -4.3409e+00, -4.1615e+00, -3.7885e+00,\n",
      "         -3.2730e+00, -3.6827e+00, -3.8716e+00, -4.2046e+00, -3.7003e+00,\n",
      "         -3.5390e+00, -3.5213e+00, -3.3025e+00, -4.2182e+00, -3.5604e+00,\n",
      "         -3.4974e+00, -4.1221e+00, -3.5990e+00, -3.5056e+00, -4.1102e+00,\n",
      "         -4.2325e+00, -3.3739e+00, -4.1744e+00, -3.7262e+00, -3.4799e+00,\n",
      "         -3.8954e+00, -3.6906e+00, -3.3553e+00, -4.0771e+00, -3.6886e+00,\n",
      "         -4.2394e+00, -3.5998e+00, -3.5081e+00, -3.7385e+00, -3.9233e+00,\n",
      "         -3.8411e+00, -4.2844e+00],\n",
      "        [-6.7004e-02, -6.0907e+00, -6.9340e+00, -6.8648e+00, -6.4175e+00,\n",
      "         -5.7678e+00, -6.1825e+00, -6.3457e+00, -6.8891e+00, -6.3176e+00,\n",
      "         -6.0897e+00, -6.1750e+00, -5.7250e+00, -6.7868e+00, -6.1652e+00,\n",
      "         -6.2358e+00, -6.7098e+00, -6.3418e+00, -5.9024e+00, -6.3191e+00,\n",
      "         -6.7230e+00, -5.9679e+00, -6.4844e+00, -6.2608e+00, -6.2855e+00,\n",
      "         -6.6284e+00, -6.4497e+00, -5.7957e+00, -6.4736e+00, -6.4237e+00,\n",
      "         -6.4978e+00, -6.5559e+00, -6.2789e+00, -6.3679e+00, -6.7208e+00,\n",
      "         -6.3249e+00, -6.7908e+00],\n",
      "        [-8.2192e-03, -8.2629e+00, -8.8246e+00, -8.8855e+00, -8.5196e+00,\n",
      "         -7.9111e+00, -8.1779e+00, -8.3997e+00, -8.8932e+00, -8.4225e+00,\n",
      "         -8.0855e+00, -8.3333e+00, -7.8752e+00, -8.7317e+00, -8.3615e+00,\n",
      "         -8.4069e+00, -8.6307e+00, -8.4772e+00, -7.8996e+00, -8.1128e+00,\n",
      "         -8.5982e+00, -8.1017e+00, -8.3735e+00, -8.4710e+00, -8.5244e+00,\n",
      "         -8.8340e+00, -8.6768e+00, -7.8652e+00, -8.4008e+00, -8.5171e+00,\n",
      "         -8.3556e+00, -8.8168e+00, -8.5143e+00, -8.5387e+00, -8.7946e+00,\n",
      "         -8.2596e+00, -8.6904e+00],\n",
      "        [-5.1041e-03, -8.8147e+00, -9.1577e+00, -9.3131e+00, -9.0103e+00,\n",
      "         -8.4839e+00, -8.6008e+00, -8.8737e+00, -9.2374e+00, -8.8976e+00,\n",
      "         -8.5344e+00, -8.8802e+00, -8.4798e+00, -9.1036e+00, -8.9207e+00,\n",
      "         -8.9161e+00, -8.9793e+00, -8.9735e+00, -8.3923e+00, -8.4558e+00,\n",
      "         -8.9187e+00, -8.6748e+00, -8.7735e+00, -9.0759e+00, -9.0875e+00,\n",
      "         -9.3332e+00, -9.1625e+00, -8.4192e+00, -8.7898e+00, -9.0037e+00,\n",
      "         -8.7466e+00, -9.3191e+00, -9.0937e+00, -9.0679e+00, -9.1870e+00,\n",
      "         -8.6555e+00, -9.0585e+00],\n",
      "        [-1.1508e-02, -8.0544e+00, -8.2466e+00, -8.4065e+00, -8.3299e+00,\n",
      "         -7.7877e+00, -7.8049e+00, -8.0477e+00, -8.2711e+00, -8.0102e+00,\n",
      "         -7.7165e+00, -8.1160e+00, -7.8256e+00, -8.1465e+00, -8.2022e+00,\n",
      "         -8.0798e+00, -8.0784e+00, -8.1643e+00, -7.6634e+00, -7.5584e+00,\n",
      "         -7.9558e+00, -8.0250e+00, -7.9670e+00, -8.2985e+00, -8.3375e+00,\n",
      "         -8.5205e+00, -8.2523e+00, -7.7092e+00, -7.9509e+00, -8.1945e+00,\n",
      "         -7.9256e+00, -8.4663e+00, -8.3718e+00, -8.2008e+00, -8.2320e+00,\n",
      "         -7.8342e+00, -8.2003e+00]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([14, 18,  7,  0,  0], device='cuda:0')\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor([3, 3, 4, 2, 4, 5, 5, 3, 4, 3, 3, 3, 3, 2, 3, 5, 3, 4, 3, 3, 3, 5, 5, 3,\n",
      "        4, 4, 3, 3, 2, 3, 5, 4])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1024, 1, 7])\n",
      "here\n",
      "tensor([[-3.7560e+00, -3.2524e+00, -4.0739e+00, -3.9809e+00, -3.7741e+00,\n",
      "         -3.2921e+00, -3.7506e+00, -3.6826e+00, -4.0382e+00, -3.3870e+00,\n",
      "         -3.4219e+00, -3.3739e+00, -3.5405e+00, -3.9184e+00, -3.6260e+00,\n",
      "         -3.2996e+00, -3.9712e+00, -3.4433e+00, -3.5662e+00, -4.0133e+00,\n",
      "         -4.0574e+00, -3.4874e+00, -4.1071e+00, -3.1051e+00, -3.4312e+00,\n",
      "         -3.6201e+00, -3.4067e+00, -3.3295e+00, -3.9258e+00, -3.8212e+00,\n",
      "         -4.0948e+00, -3.4540e+00, -3.3725e+00, -3.2675e+00, -3.7245e+00,\n",
      "         -3.6869e+00, -4.0823e+00],\n",
      "        [-3.7597e+00, -3.3447e+00, -3.9152e+00, -3.8669e+00, -3.6207e+00,\n",
      "         -3.3253e+00, -3.5733e+00, -3.6693e+00, -3.8827e+00, -3.5360e+00,\n",
      "         -3.4894e+00, -3.4393e+00, -3.4535e+00, -3.7628e+00, -3.5861e+00,\n",
      "         -3.3993e+00, -3.7560e+00, -3.5679e+00, -3.4913e+00, -3.8981e+00,\n",
      "         -3.8958e+00, -3.3279e+00, -4.0189e+00, -3.6257e+00, -3.4323e+00,\n",
      "         -3.6259e+00, -3.4895e+00, -3.4272e+00, -3.7743e+00, -3.6575e+00,\n",
      "         -3.9985e+00, -3.4481e+00, -3.3647e+00, -3.6691e+00, -3.6241e+00,\n",
      "         -3.6043e+00, -4.0120e+00],\n",
      "        [-2.6935e+00, -3.6430e+00, -3.6998e+00, -3.6626e+00, -3.3966e+00,\n",
      "         -3.5666e+00, -3.3875e+00, -3.7478e+00, -3.6177e+00, -3.8841e+00,\n",
      "         -3.7068e+00, -3.6856e+00, -3.4543e+00, -3.5832e+00, -3.5956e+00,\n",
      "         -3.7581e+00, -3.4812e+00, -3.8174e+00, -3.5334e+00, -3.6440e+00,\n",
      "         -3.6375e+00, -3.2513e+00, -3.8300e+00, -4.4707e+00, -3.5874e+00,\n",
      "         -3.7913e+00, -3.7676e+00, -3.6816e+00, -3.6085e+00, -3.5382e+00,\n",
      "         -3.7069e+00, -3.6613e+00, -3.5727e+00, -4.3668e+00, -3.6177e+00,\n",
      "         -3.5226e+00, -3.8589e+00],\n",
      "        [-1.6104e-01, -5.7720e+00, -5.3518e+00, -5.4380e+00, -5.1040e+00,\n",
      "         -5.6413e+00, -5.0253e+00, -5.5695e+00, -5.3413e+00, -5.9832e+00,\n",
      "         -5.5971e+00, -5.7710e+00, -5.3145e+00, -5.3261e+00, -5.4517e+00,\n",
      "         -5.9216e+00, -5.1924e+00, -5.8131e+00, -5.3706e+00, -4.9913e+00,\n",
      "         -5.1789e+00, -5.2340e+00, -5.2225e+00, -6.7556e+00, -5.7704e+00,\n",
      "         -5.8084e+00, -5.8866e+00, -5.5643e+00, -5.2052e+00, -5.5635e+00,\n",
      "         -5.2116e+00, -5.9038e+00, -5.7129e+00, -6.6750e+00, -5.5443e+00,\n",
      "         -5.2339e+00, -5.5007e+00],\n",
      "        [-1.1685e-02, -8.3554e+00, -7.8463e+00, -8.1197e+00, -7.7322e+00,\n",
      "         -8.1948e+00, -7.5938e+00, -8.0849e+00, -7.9448e+00, -8.5281e+00,\n",
      "         -7.9898e+00, -8.3361e+00, -7.8999e+00, -7.9238e+00, -8.1043e+00,\n",
      "         -8.5050e+00, -7.7625e+00, -8.3724e+00, -7.8452e+00, -7.3598e+00,\n",
      "         -7.6873e+00, -7.8761e+00, -7.5933e+00, -9.2495e+00, -8.5041e+00,\n",
      "         -8.4844e+00, -8.5490e+00, -8.0052e+00, -7.6979e+00, -8.2685e+00,\n",
      "         -7.6804e+00, -8.6227e+00, -8.4113e+00, -9.1311e+00, -8.1472e+00,\n",
      "         -7.7014e+00, -8.0630e+00],\n",
      "        [-6.2027e-03, -8.9371e+00, -8.4893e+00, -8.8493e+00, -8.4745e+00,\n",
      "         -8.7447e+00, -8.2983e+00, -8.6688e+00, -8.6024e+00, -9.0412e+00,\n",
      "         -8.4880e+00, -8.8843e+00, -8.5523e+00, -8.5951e+00, -8.7954e+00,\n",
      "         -9.0305e+00, -8.4516e+00, -8.9604e+00, -8.3881e+00, -8.0100e+00,\n",
      "         -8.3460e+00, -8.5626e+00, -8.2606e+00, -9.6870e+00, -9.1358e+00,\n",
      "         -9.1787e+00, -9.1296e+00, -8.5455e+00, -8.3623e+00, -8.8845e+00,\n",
      "         -8.3330e+00, -9.2295e+00, -9.0793e+00, -9.5011e+00, -8.7819e+00,\n",
      "         -8.3265e+00, -8.7459e+00],\n",
      "        [-1.4307e-02, -8.0067e+00, -7.7302e+00, -8.0335e+00, -7.8931e+00,\n",
      "         -7.8244e+00, -7.5815e+00, -7.7980e+00, -7.7793e+00, -7.9997e+00,\n",
      "         -7.5653e+00, -7.9463e+00, -7.7598e+00, -7.7621e+00, -8.0505e+00,\n",
      "         -8.0252e+00, -7.7117e+00, -8.0464e+00, -7.5235e+00, -7.2439e+00,\n",
      "         -7.5413e+00, -7.8441e+00, -7.5522e+00, -8.5151e+00, -8.2426e+00,\n",
      "         -8.3598e+00, -8.1364e+00, -7.6339e+00, -7.6165e+00, -7.9704e+00,\n",
      "         -7.5907e+00, -8.3029e+00, -8.2431e+00, -8.2942e+00, -7.8966e+00,\n",
      "         -7.5549e+00, -7.9671e+00]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([14, 27, 16,  0,  0], device='cuda:0')\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor([3, 5, 5, 3, 4, 2, 2, 5, 3, 3, 4, 3, 3, 5, 2, 2, 2, 3, 4, 5, 4, 4, 3, 2,\n",
      "        4, 2, 4, 5, 3, 5, 4, 5])\n",
      "torch.Size([32, 1024, 1, 7])\n",
      "here\n",
      "tensor([[-4.0438, -3.8660, -3.4176, -3.2149, -3.3263, -3.8728, -3.5309, -3.7566,\n",
      "         -3.2267, -3.9354, -3.8524, -3.9438, -3.6531, -3.3176, -3.7229, -3.8679,\n",
      "         -3.1975, -3.8848, -3.7709, -3.4177, -3.2642, -3.4000, -3.4216, -4.6279,\n",
      "         -3.7277, -3.8240, -3.9429, -3.9935, -3.3750, -3.4084, -3.3179, -3.7363,\n",
      "         -3.7444, -4.5018, -3.3437, -3.5231, -3.4984],\n",
      "        [-4.0343, -3.8217, -3.5178, -3.2455, -3.2856, -3.8575, -3.4917, -3.7448,\n",
      "         -3.2737, -3.9363, -3.8133, -3.8744, -3.6020, -3.3651, -3.6628, -3.8037,\n",
      "         -3.2194, -3.8669, -3.7260, -3.4794, -3.3299, -3.3429, -3.5031, -4.7177,\n",
      "         -3.6766, -3.8130, -3.9062, -3.9507, -3.4036, -3.3817, -3.3730, -3.7123,\n",
      "         -3.6936, -4.5834, -3.3278, -3.5033, -3.5265],\n",
      "        [-3.6643, -3.7064, -3.6577, -3.3732, -3.2767, -3.7571, -3.4421, -3.7090,\n",
      "         -3.3893, -3.8972, -3.7443, -3.7815, -3.5012, -3.4665, -3.5873, -3.7037,\n",
      "         -3.3072, -3.8337, -3.6164, -3.5833, -3.4711, -3.2845, -3.6466, -4.7111,\n",
      "         -3.5991, -3.8136, -3.8265, -3.8164, -3.4896, -3.4032, -3.5097, -3.6659,\n",
      "         -3.6043, -4.5792, -3.3792, -3.4816, -3.6246],\n",
      "        [-1.9518, -3.8067, -3.8617, -3.6014, -3.3741, -3.8157, -3.4678, -3.8093,\n",
      "         -3.5676, -4.0803, -3.8740, -3.8932, -3.4592, -3.6893, -3.6698, -3.8067,\n",
      "         -3.4572, -3.9578, -3.6130, -3.6643, -3.6617, -3.3963, -3.7978, -4.9137,\n",
      "         -3.7138, -4.0258, -3.9724, -3.7684, -3.6557, -3.5653, -3.6490, -3.8916,\n",
      "         -3.7357, -4.7694, -3.5882, -3.5444, -3.8336],\n",
      "        [-0.0994, -6.0693, -6.0430, -5.9860, -5.6143, -5.9838, -5.6052, -5.9187,\n",
      "         -5.8896, -6.3200, -5.9782, -6.1465, -5.6051, -5.9387, -5.9050, -6.1049,\n",
      "         -5.7175, -6.1713, -5.7010, -5.5539, -5.8264, -5.7059, -5.7301, -6.9698,\n",
      "         -6.1554, -6.3168, -6.2786, -5.7706, -5.7324, -6.0106, -5.6971, -6.3833,\n",
      "         -6.1143, -6.9108, -5.9704, -5.6347, -6.0466],\n",
      "        [-0.0147, -7.9912, -7.8061, -7.9715, -7.6146, -7.8290, -7.5154, -7.7510,\n",
      "         -7.7987, -8.1417, -7.6976, -8.0273, -7.6020, -7.7936, -7.8878, -8.0036,\n",
      "         -7.6061, -8.0582, -7.5143, -7.2459, -7.6308, -7.6878, -7.4822, -8.7063,\n",
      "         -8.1998, -8.2717, -8.2035, -7.6038, -7.5534, -7.9927, -7.5012, -8.3480,\n",
      "         -8.1275, -8.5939, -7.8791, -7.4382, -7.9314],\n",
      "        [-0.0196, -7.6714, -7.5082, -7.6996, -7.5788, -7.5065, -7.3149, -7.4398,\n",
      "         -7.4907, -7.6842, -7.3001, -7.6699, -7.4218, -7.4629, -7.7115, -7.6364,\n",
      "         -7.3668, -7.7336, -7.2238, -6.9439, -7.2942, -7.5307, -7.2687, -8.1666,\n",
      "         -7.9163, -8.0179, -7.8039, -7.2929, -7.3274, -7.6704, -7.2660, -7.9939,\n",
      "         -7.9069, -7.9762, -7.5446, -7.1935, -7.6685]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([36, 12, 11, 18,  0], device='cuda:0')\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor([4, 4, 4, 2, 3, 5, 5, 3, 2, 5, 3, 4, 3, 5, 5, 3, 3, 3, 5, 4, 4, 4, 3, 3,\n",
      "        5, 5, 5, 3, 4, 5, 2, 5])\n",
      "torch.Size([32, 1024, 1, 7])\n",
      "here\n",
      "tensor([[-3.8736, -3.3385, -3.9300, -3.7935, -3.6663, -3.4234, -3.6792, -3.6360,\n",
      "         -3.8579, -3.4506, -3.4353, -3.4269, -3.5234, -3.7784, -3.5367, -3.3477,\n",
      "         -3.8508, -3.4938, -3.5433, -3.8712, -3.8896, -3.4295, -3.9116, -3.5562,\n",
      "         -3.4731, -3.6738, -3.4503, -3.4217, -3.7802, -3.6544, -3.9842, -3.4226,\n",
      "         -3.4404, -3.5961, -3.6064, -3.5807, -3.9324],\n",
      "        [-3.6990, -3.2673, -4.1015, -3.9331, -3.7014, -3.3399, -3.7086, -3.6767,\n",
      "         -4.0135, -3.4384, -3.4123, -3.3549, -3.4736, -3.8950, -3.5086, -3.2449,\n",
      "         -3.9549, -3.4717, -3.4763, -3.9824, -4.0523, -3.3881, -4.0873, -3.4851,\n",
      "         -3.4308, -3.6604, -3.4062, -3.3148, -3.9095, -3.6747, -4.1611, -3.3794,\n",
      "         -3.3915, -3.5361, -3.6785, -3.6230, -4.0776],\n",
      "        [-2.9860, -3.2546, -4.3021, -4.0992, -3.7930, -3.2715, -3.7690, -3.7287,\n",
      "         -4.1863, -3.4909, -3.4384, -3.3519, -3.4049, -4.0554, -3.5208, -3.1448,\n",
      "         -4.0795, -3.5110, -3.4242, -4.0844, -4.2406, -3.3579, -4.2783, -3.4131,\n",
      "         -3.4211, -3.6947, -3.4165, -3.1955, -4.0828, -3.7074, -4.3309, -3.3901,\n",
      "         -3.3822, -3.4858, -3.7987, -3.7060, -4.2783],\n",
      "        [-1.0543, -3.6365, -4.8034, -4.5932, -4.2371, -3.5598, -4.1616, -4.0634,\n",
      "         -4.6659, -3.9220, -3.8306, -3.7873, -3.6209, -4.5201, -3.9243, -3.4463,\n",
      "         -4.4918, -3.9583, -3.6837, -4.3420, -4.6842, -3.6899, -4.6563, -3.7114,\n",
      "         -3.8547, -4.1324, -3.8323, -3.3794, -4.4922, -4.1349, -4.6712, -3.9000,\n",
      "         -3.8196, -3.8367, -4.3015, -4.0911, -4.7927],\n",
      "        [-0.0856, -5.7946, -6.7656, -6.6182, -6.2777, -5.6280, -6.1544, -6.0557,\n",
      "         -6.7036, -6.0690, -5.8779, -5.9654, -5.6459, -6.4698, -6.0513, -5.6365,\n",
      "         -6.4454, -6.1067, -5.6320, -6.0388, -6.5956, -5.7679, -6.4400, -5.8573,\n",
      "         -6.1204, -6.3408, -6.0701, -5.3896, -6.3413, -6.2608, -6.4333, -6.2652,\n",
      "         -6.0889, -6.0301, -6.4202, -6.0238, -6.7243],\n",
      "        [-0.0193, -7.3589, -8.0293, -7.9752, -7.7201, -7.1672, -7.5020, -7.4949,\n",
      "         -8.0332, -7.5524, -7.2809, -7.5193, -7.1930, -7.7773, -7.6083, -7.2302,\n",
      "         -7.7294, -7.6087, -7.0714, -7.2252, -7.8265, -7.3044, -7.6953, -7.4657,\n",
      "         -7.7167, -7.8957, -7.6436, -6.9300, -7.6238, -7.7305, -7.6499, -7.8830,\n",
      "         -7.7113, -7.5847, -7.8258, -7.3549, -7.9807],\n",
      "        [-0.0272, -7.1017, -7.5388, -7.5209, -7.5081, -6.9336, -7.1104, -7.1571,\n",
      "         -7.5229, -7.1400, -6.9117, -7.2343, -6.9941, -7.2824, -7.3805, -6.9729,\n",
      "         -7.2548, -7.2843, -6.8099, -6.7585, -7.2572, -7.1295, -7.2958, -7.1937,\n",
      "         -7.4431, -7.5824, -7.2760, -6.7360, -7.2115, -7.3571, -7.2114, -7.5436,\n",
      "         -7.5004, -7.2008, -7.3592, -6.9771, -7.4974]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([32, 18, 14,  0,  0], device='cuda:0')\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor([3, 3, 4, 4, 4, 4, 5, 4, 5, 3, 5, 5, 4, 4, 2, 3, 4, 3, 4, 5, 2, 5, 5, 4,\n",
      "        2, 4, 3, 2, 3, 3, 3, 4])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1024, 1, 7])\n",
      "here\n",
      "tensor([[-3.4962e+00, -4.1627e+00, -3.1961e+00, -3.0482e+00, -3.1678e+00,\n",
      "         -4.1783e+00, -3.5029e+00, -4.0147e+00, -3.0240e+00, -4.2781e+00,\n",
      "         -4.2321e+00, -4.2899e+00, -3.8246e+00, -3.1584e+00, -3.8797e+00,\n",
      "         -4.2598e+00, -2.9380e+00, -4.1799e+00, -3.8128e+00, -3.2450e+00,\n",
      "         -3.0604e+00, -3.4773e+00, -3.2990e+00, -5.2549e+00, -3.9885e+00,\n",
      "         -4.0185e+00, -4.2910e+00, -4.3772e+00, -3.1881e+00, -3.4626e+00,\n",
      "         -3.0639e+00, -3.9786e+00, -4.0520e+00, -5.0801e+00, -3.4760e+00,\n",
      "         -3.5313e+00, -3.3088e+00],\n",
      "        [-3.0953e+00, -4.3677e+00, -3.1862e+00, -2.9591e+00, -3.0823e+00,\n",
      "         -4.4193e+00, -3.4784e+00, -4.1712e+00, -2.9440e+00, -4.4954e+00,\n",
      "         -4.4399e+00, -4.4653e+00, -3.9501e+00, -3.1490e+00, -3.9670e+00,\n",
      "         -4.4261e+00, -2.8273e+00, -4.3375e+00, -3.8881e+00, -3.1908e+00,\n",
      "         -2.9687e+00, -3.5636e+00, -3.2667e+00, -5.7090e+00, -4.1783e+00,\n",
      "         -4.1558e+00, -4.5153e+00, -4.5822e+00, -3.1524e+00, -3.5054e+00,\n",
      "         -2.9560e+00, -4.2079e+00, -4.2438e+00, -5.5173e+00, -3.5127e+00,\n",
      "         -3.5351e+00, -3.2113e+00],\n",
      "        [-1.6946e+00, -4.8033e+00, -3.3060e+00, -3.0658e+00, -3.1795e+00,\n",
      "         -4.8892e+00, -3.5714e+00, -4.4837e+00, -3.0093e+00, -4.9022e+00,\n",
      "         -4.8115e+00, -4.8421e+00, -4.3286e+00, -3.2860e+00, -4.2274e+00,\n",
      "         -4.8080e+00, -2.9022e+00, -4.7198e+00, -4.1260e+00, -3.1597e+00,\n",
      "         -2.9433e+00, -3.9029e+00, -3.3098e+00, -6.4533e+00, -4.6102e+00,\n",
      "         -4.5053e+00, -4.9460e+00, -4.8980e+00, -3.2719e+00, -3.8261e+00,\n",
      "         -2.9760e+00, -4.7098e+00, -4.7008e+00, -6.2107e+00, -3.6893e+00,\n",
      "         -3.6487e+00, -3.2347e+00],\n",
      "        [-8.7225e-02, -7.1467e+00, -5.6459e+00, -5.5435e+00, -5.5721e+00,\n",
      "         -7.1537e+00, -5.7734e+00, -6.6429e+00, -5.4123e+00, -7.1097e+00,\n",
      "         -6.8724e+00, -7.0693e+00, -6.6574e+00, -5.5568e+00, -6.5004e+00,\n",
      "         -7.0780e+00, -5.3495e+00, -7.0287e+00, -6.3004e+00, -5.1580e+00,\n",
      "         -5.1562e+00, -6.2412e+00, -5.4002e+00, -8.6900e+00, -6.9859e+00,\n",
      "         -6.8166e+00, -7.1111e+00, -6.8880e+00, -5.5620e+00, -6.3955e+00,\n",
      "         -5.2445e+00, -7.1579e+00, -7.0834e+00, -8.4948e+00, -5.9588e+00,\n",
      "         -5.8086e+00, -5.5281e+00],\n",
      "        [-1.1583e-02, -8.9321e+00, -7.6461e+00, -7.7664e+00, -7.6606e+00,\n",
      "         -8.8533e+00, -7.7536e+00, -8.3897e+00, -7.5338e+00, -8.9066e+00,\n",
      "         -8.4866e+00, -8.8437e+00, -8.4637e+00, -7.6234e+00, -8.4345e+00,\n",
      "         -8.8528e+00, -7.4916e+00, -8.8568e+00, -8.1220e+00, -7.0787e+00,\n",
      "         -7.2766e+00, -8.1111e+00, -7.3476e+00, -1.0196e+01, -8.8831e+00,\n",
      "         -8.6937e+00, -8.8480e+00, -8.5720e+00, -7.5557e+00, -8.4457e+00,\n",
      "         -7.3052e+00, -8.9827e+00, -8.9033e+00, -1.0025e+01, -7.9368e+00,\n",
      "         -7.7055e+00, -7.6946e+00],\n",
      "        [-6.6547e-03, -9.3028e+00, -8.2104e+00, -8.4757e+00, -8.2726e+00,\n",
      "         -9.1794e+00, -8.3355e+00, -8.7637e+00, -8.1652e+00, -9.2891e+00,\n",
      "         -8.7907e+00, -9.2317e+00, -8.9060e+00, -8.2573e+00, -8.9259e+00,\n",
      "         -9.2109e+00, -8.1495e+00, -9.2598e+00, -8.5549e+00, -7.6884e+00,\n",
      "         -7.9686e+00, -8.6067e+00, -7.9597e+00, -1.0342e+01, -9.3595e+00,\n",
      "         -9.1719e+00, -9.2210e+00, -8.9388e+00, -8.1403e+00, -8.9725e+00,\n",
      "         -7.9473e+00, -9.3773e+00, -9.3235e+00, -1.0154e+01, -8.4861e+00,\n",
      "         -8.2156e+00, -8.4006e+00],\n",
      "        [-1.2556e-02, -8.4338e+00, -7.6561e+00, -7.9207e+00, -7.8482e+00,\n",
      "         -8.3147e+00, -7.7762e+00, -7.9836e+00, -7.5955e+00, -8.4042e+00,\n",
      "         -7.9272e+00, -8.3846e+00, -8.1688e+00, -7.6798e+00, -8.2781e+00,\n",
      "         -8.3341e+00, -7.6629e+00, -8.4213e+00, -7.8366e+00, -7.1840e+00,\n",
      "         -7.4556e+00, -8.0014e+00, -7.4719e+00, -9.1988e+00, -8.5849e+00,\n",
      "         -8.4973e+00, -8.3581e+00, -8.0774e+00, -7.6036e+00, -8.2102e+00,\n",
      "         -7.4608e+00, -8.5707e+00, -8.5799e+00, -8.9567e+00, -7.8356e+00,\n",
      "         -7.6087e+00, -7.9125e+00]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([22,  7,  3,  0,  0], device='cuda:0')\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor([3, 3, 3, 5, 4, 4, 3, 3, 5, 5, 4, 5, 5, 5, 5, 5, 3, 5, 3, 3, 5, 5, 5, 4,\n",
      "        5, 3, 5, 5, 2, 2, 5, 4])\n",
      "torch.Size([32, 1024, 1, 7])\n",
      "here\n",
      "tensor([[-3.5988e+00, -3.2588e+00, -4.1619e+00, -3.9972e+00, -3.7438e+00,\n",
      "         -3.3559e+00, -3.8227e+00, -3.7020e+00, -4.0862e+00, -3.3404e+00,\n",
      "         -3.3996e+00, -3.3688e+00, -3.5557e+00, -3.9370e+00, -3.5351e+00,\n",
      "         -3.2481e+00, -4.0688e+00, -3.3853e+00, -3.5303e+00, -3.9964e+00,\n",
      "         -4.1422e+00, -3.4524e+00, -4.1316e+00, -3.1389e+00, -3.4058e+00,\n",
      "         -3.6430e+00, -3.3807e+00, -3.2865e+00, -3.9778e+00, -3.7969e+00,\n",
      "         -4.2195e+00, -3.4701e+00, -3.3685e+00, -3.3175e+00, -3.7646e+00,\n",
      "         -3.6869e+00, -4.0853e+00],\n",
      "        [-3.0973e+00, -3.2379e+00, -4.4294e+00, -4.2200e+00, -3.8515e+00,\n",
      "         -3.2862e+00, -3.9171e+00, -3.7881e+00, -4.3362e+00, -3.3279e+00,\n",
      "         -3.4362e+00, -3.3381e+00, -3.5067e+00, -4.1318e+00, -3.5733e+00,\n",
      "         -3.1268e+00, -4.2539e+00, -3.4187e+00, -3.4872e+00, -4.1591e+00,\n",
      "         -4.4117e+00, -3.4379e+00, -4.3671e+00, -2.9205e+00, -3.3606e+00,\n",
      "         -3.6271e+00, -3.3682e+00, -3.1536e+00, -4.1988e+00, -3.8630e+00,\n",
      "         -4.4697e+00, -3.4967e+00, -3.3342e+00, -3.1583e+00, -3.9334e+00,\n",
      "         -3.8284e+00, -4.3088e+00],\n",
      "        [-7.9053e-01, -3.8604e+00, -5.1037e+00, -4.8793e+00, -4.4856e+00,\n",
      "         -3.8079e+00, -4.4825e+00, -4.3155e+00, -5.0537e+00, -3.9272e+00,\n",
      "         -4.0394e+00, -3.9763e+00, -3.8411e+00, -4.7880e+00, -4.2013e+00,\n",
      "         -3.6488e+00, -4.8288e+00, -4.1468e+00, -3.8688e+00, -4.5654e+00,\n",
      "         -5.0638e+00, -3.9349e+00, -4.8828e+00, -3.3508e+00, -3.9571e+00,\n",
      "         -4.2261e+00, -3.9872e+00, -3.4869e+00, -4.7504e+00, -4.4718e+00,\n",
      "         -4.9609e+00, -4.2770e+00, -3.9715e+00, -3.6811e+00, -4.6818e+00,\n",
      "         -4.3755e+00, -4.9670e+00],\n",
      "        [-2.9774e-02, -6.9862e+00, -7.6442e+00, -7.4486e+00, -7.1945e+00,\n",
      "         -6.8317e+00, -7.1399e+00, -7.1353e+00, -7.7158e+00, -7.0665e+00,\n",
      "         -6.9545e+00, -7.0707e+00, -6.6587e+00, -7.3522e+00, -7.1193e+00,\n",
      "         -6.8504e+00, -7.3957e+00, -7.2394e+00, -6.5647e+00, -6.8747e+00,\n",
      "         -7.6075e+00, -6.8022e+00, -7.2914e+00, -6.8718e+00, -7.1422e+00,\n",
      "         -7.3451e+00, -7.2062e+00, -6.4506e+00, -7.2244e+00, -7.3219e+00,\n",
      "         -7.2976e+00, -7.5424e+00, -7.1499e+00, -7.1121e+00, -7.5537e+00,\n",
      "         -6.9715e+00, -7.5669e+00],\n",
      "        [-7.7128e-03, -8.4934e+00, -8.7197e+00, -8.6304e+00, -8.4341e+00,\n",
      "         -8.3022e+00, -8.3389e+00, -8.4704e+00, -8.8170e+00, -8.5576e+00,\n",
      "         -8.2489e+00, -8.5521e+00, -8.1303e+00, -8.4749e+00, -8.5259e+00,\n",
      "         -8.3836e+00, -8.4930e+00, -8.6503e+00, -7.9166e+00, -7.9287e+00,\n",
      "         -8.6589e+00, -8.1848e+00, -8.4153e+00, -8.6855e+00, -8.6383e+00,\n",
      "         -8.8075e+00, -8.6954e+00, -7.9149e+00, -8.3662e+00, -8.6511e+00,\n",
      "         -8.3869e+00, -9.0075e+00, -8.6541e+00, -8.7684e+00, -8.7884e+00,\n",
      "         -8.1608e+00, -8.7142e+00],\n",
      "        [-6.1426e-03, -8.8123e+00, -8.8022e+00, -8.8281e+00, -8.6509e+00,\n",
      "         -8.6060e+00, -8.5166e+00, -8.6836e+00, -8.8820e+00, -8.8190e+00,\n",
      "         -8.4563e+00, -8.8547e+00, -8.4885e+00, -8.6177e+00, -8.8014e+00,\n",
      "         -8.6888e+00, -8.6038e+00, -8.9041e+00, -8.1829e+00, -8.0683e+00,\n",
      "         -8.7315e+00, -8.4978e+00, -8.5497e+00, -9.1273e+00, -8.9539e+00,\n",
      "         -9.0766e+00, -8.9510e+00, -8.2663e+00, -8.5029e+00, -8.8849e+00,\n",
      "         -8.5226e+00, -9.2321e+00, -8.9838e+00, -9.0855e+00, -8.9149e+00,\n",
      "         -8.3390e+00, -8.8528e+00],\n",
      "        [-1.4004e-02, -7.9687e+00, -7.9530e+00, -7.9912e+00, -8.0201e+00,\n",
      "         -7.8028e+00, -7.7273e+00, -7.8344e+00, -7.9575e+00, -7.9033e+00,\n",
      "         -7.5927e+00, -8.0227e+00, -7.7641e+00, -7.7551e+00, -8.0597e+00,\n",
      "         -7.8373e+00, -7.7749e+00, -8.0482e+00, -7.4299e+00, -7.2666e+00,\n",
      "         -7.8124e+00, -7.8208e+00, -7.7571e+00, -8.2338e+00, -8.1601e+00,\n",
      "         -8.2838e+00, -8.0346e+00, -7.5166e+00, -7.6975e+00, -8.0227e+00,\n",
      "         -7.7142e+00, -8.3314e+00, -8.2297e+00, -8.1019e+00, -7.9801e+00,\n",
      "         -7.5427e+00, -8.0095e+00]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([34, 11,  0,  0,  0], device='cuda:0')\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor([2, 3, 4, 4, 3, 5, 4, 3, 4, 4, 3, 2, 4, 2, 2, 3, 3, 2, 5, 5, 4, 3, 4, 3,\n",
      "        5, 5, 5, 3, 4, 3, 3, 2])\n",
      "torch.Size([32, 1024, 1, 7])\n",
      "here\n",
      "tensor([[-3.9409, -3.6968, -3.5978, -3.4048, -3.4244, -3.7267, -3.5474, -3.6651,\n",
      "         -3.4256, -3.7463, -3.6767, -3.7156, -3.5402, -3.4598, -3.6277, -3.6629,\n",
      "         -3.4303, -3.7291, -3.6572, -3.5706, -3.5055, -3.3871, -3.5376, -4.3492,\n",
      "         -3.5925, -3.7163, -3.7359, -3.7622, -3.4901, -3.4715, -3.5513, -3.6049,\n",
      "         -3.6056, -4.2326, -3.3774, -3.5008, -3.6080],\n",
      "        [-3.7461, -3.5605, -3.7915, -3.5465, -3.4553, -3.6236, -3.5575, -3.6699,\n",
      "         -3.5862, -3.6630, -3.6153, -3.5763, -3.4441, -3.5990, -3.5522, -3.5181,\n",
      "         -3.5683, -3.6452, -3.5241, -3.7017, -3.7060, -3.3353, -3.7204, -4.2454,\n",
      "         -3.4954, -3.6604, -3.6217, -3.6075, -3.6300, -3.4942, -3.7476, -3.5300,\n",
      "         -3.5160, -4.1518, -3.4203, -3.5125, -3.7469],\n",
      "        [-3.0495, -3.4837, -3.9916, -3.7141, -3.5329, -3.5239, -3.6021, -3.7247,\n",
      "         -3.7755, -3.6323, -3.6343, -3.5117, -3.3508, -3.7562, -3.5413, -3.4039,\n",
      "         -3.7404, -3.6182, -3.3885, -3.8194, -3.9363, -3.3276, -3.9278, -4.0985,\n",
      "         -3.4330, -3.6366, -3.5504, -3.4399, -3.8032, -3.5497, -3.9497, -3.5182,\n",
      "         -3.4615, -4.0413, -3.5543, -3.5688, -3.9303],\n",
      "        [-1.1387, -3.8625, -4.3713, -4.0748, -3.8618, -3.8305, -3.9134, -4.0706,\n",
      "         -4.1598, -4.0292, -4.0372, -3.8957, -3.5685, -4.1002, -3.9114, -3.7175,\n",
      "         -4.0965, -3.9976, -3.5655, -4.0136, -4.3347, -3.6627, -4.2299, -4.4350,\n",
      "         -3.8026, -3.9893, -3.9028, -3.6202, -4.1198, -3.9309, -4.2231, -4.0056,\n",
      "         -3.8483, -4.3915, -4.0159, -3.8675, -4.3258],\n",
      "        [-0.0531, -6.5936, -6.7933, -6.6325, -6.3742, -6.4705, -6.3995, -6.5776,\n",
      "         -6.7122, -6.7248, -6.5552, -6.6451, -6.1925, -6.5446, -6.5562, -6.4802,\n",
      "         -6.5800, -6.6734, -6.0710, -6.1628, -6.7638, -6.3039, -6.4851, -7.0814,\n",
      "         -6.6624, -6.7006, -6.6884, -6.1490, -6.4461, -6.6678, -6.5012, -6.9388,\n",
      "         -6.6491, -7.0879, -6.7000, -6.2857, -6.7761],\n",
      "        [-0.0113, -8.2446, -8.1330, -8.1648, -7.9250, -8.0807, -7.8894, -8.0786,\n",
      "         -8.1522, -8.2867, -7.9785, -8.2822, -7.8849, -7.9707, -8.1917, -8.1247,\n",
      "         -7.9815, -8.2740, -7.6270, -7.4756, -8.1017, -7.9335, -7.8575, -8.7159,\n",
      "         -8.3731, -8.3649, -8.3393, -7.7588, -7.8480, -8.2662, -7.8721, -8.5955,\n",
      "         -8.3720, -8.6374, -8.2068, -7.7155, -8.2031],\n",
      "        [-0.0174, -7.8159, -7.6440, -7.7257, -7.7032, -7.6698, -7.4966, -7.6159,\n",
      "         -7.6320, -7.7615, -7.4575, -7.8356, -7.5752, -7.4810, -7.8611, -7.6732,\n",
      "         -7.5271, -7.8467, -7.2589, -7.0249, -7.5360, -7.6372, -7.4530, -8.1919,\n",
      "         -7.9804, -8.0280, -7.8433, -7.3838, -7.4395, -7.8029, -7.4331, -8.1190,\n",
      "         -8.0340, -8.0142, -7.6994, -7.3053, -7.7567]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([10, 17, 12, 29,  0], device='cuda:0')\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor([4, 3, 5, 3, 3, 5, 5, 3, 4, 3, 2, 5, 3, 3, 5, 4, 5, 4, 5, 5, 4, 5, 3, 3,\n",
      "        3, 3, 3, 3, 3, 4, 3, 4])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1024, 1, 7])\n",
      "here\n",
      "tensor([[-4.0626, -3.8815, -3.4343, -3.2148, -3.3342, -3.9076, -3.5477, -3.7440,\n",
      "         -3.2402, -3.9086, -3.8398, -3.9315, -3.6138, -3.3288, -3.7507, -3.8757,\n",
      "         -3.2104, -3.8643, -3.7685, -3.4366, -3.3073, -3.4481, -3.3534, -4.6355,\n",
      "         -3.7330, -3.8061, -3.9789, -3.9955, -3.3702, -3.3989, -3.3128, -3.7309,\n",
      "         -3.7479, -4.4866, -3.3248, -3.4984, -3.4401],\n",
      "        [-4.0321, -3.8659, -3.5346, -3.2136, -3.3081, -3.9166, -3.5271, -3.7398,\n",
      "         -3.2793, -3.9128, -3.8079, -3.8733, -3.5638, -3.3664, -3.6992, -3.8116,\n",
      "         -3.2326, -3.8299, -3.7255, -3.4866, -3.3609, -3.4058, -3.4211, -4.7389,\n",
      "         -3.6999, -3.7927, -3.9640, -3.9648, -3.4067, -3.3571, -3.3543, -3.7085,\n",
      "         -3.7255, -4.5679, -3.3057, -3.4717, -3.4445],\n",
      "        [-3.6721, -3.7499, -3.6998, -3.3138, -3.3405, -3.8234, -3.5224, -3.7093,\n",
      "         -3.3980, -3.8366, -3.7231, -3.7461, -3.4711, -3.4720, -3.6096, -3.6575,\n",
      "         -3.3655, -3.7453, -3.6015, -3.5975, -3.5132, -3.3553, -3.5865, -4.6596,\n",
      "         -3.6089, -3.7681, -3.8545, -3.8101, -3.5332, -3.3693, -3.5096, -3.6340,\n",
      "         -3.6525, -4.4879, -3.3514, -3.4538, -3.5443],\n",
      "        [-2.6419, -3.5906, -3.9851, -3.5645, -3.4913, -3.6934, -3.6186, -3.7385,\n",
      "         -3.6583, -3.7371, -3.6960, -3.6143, -3.3684, -3.7180, -3.5827, -3.4483,\n",
      "         -3.6518, -3.6460, -3.4436, -3.7895, -3.8257, -3.3656, -3.8695, -4.3205,\n",
      "         -3.5179, -3.7753, -3.7135, -3.5582, -3.7957, -3.4807, -3.8009, -3.6141,\n",
      "         -3.6087, -4.2040, -3.5297, -3.5124, -3.8006],\n",
      "        [-0.9194, -3.8465, -4.7117, -4.3036, -4.1351, -3.9493, -4.1730, -4.1626,\n",
      "         -4.4101, -4.0022, -4.0783, -3.9327, -3.7014, -4.3643, -4.0850, -3.6433,\n",
      "         -4.3577, -3.9737, -3.7180, -4.2928, -4.5420, -3.8276, -4.5169, -4.0910,\n",
      "         -3.9309, -4.1840, -3.9721, -3.6462, -4.4334, -4.1030, -4.4494, -4.1110,\n",
      "         -4.0620, -4.1098, -4.2194, -3.9934, -4.4975],\n",
      "        [-0.1485, -5.2873, -6.2699, -5.9581, -5.8106, -5.3120, -5.7087, -5.6021,\n",
      "         -6.0762, -5.3700, -5.4714, -5.4162, -5.2155, -5.8712, -5.6977, -5.0733,\n",
      "         -5.9531, -5.4558, -5.1168, -5.6355, -6.0674, -5.3792, -6.0092, -5.1239,\n",
      "         -5.5650, -5.7150, -5.4408, -4.9692, -5.9174, -5.7275, -5.9101, -5.7610,\n",
      "         -5.6960, -5.2449, -5.8647, -5.4699, -6.0471],\n",
      "        [-0.0940, -5.7723, -6.5577, -6.3503, -6.3931, -5.7339, -6.0891, -6.0257,\n",
      "         -6.4565, -5.7448, -5.8239, -5.9189, -5.7586, -6.1559, -6.2432, -5.5816,\n",
      "         -6.3001, -5.9378, -5.5805, -5.8702, -6.2724, -5.9420, -6.3525, -5.5283,\n",
      "         -6.1241, -6.2304, -5.8925, -5.4824, -6.2442, -6.1575, -6.2278, -6.2606,\n",
      "         -6.2908, -5.5850, -6.2508, -5.8578, -6.3699]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([34, 10,  8, 27,  0], device='cuda:0')\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor([4, 2, 3, 4, 5, 4, 3, 4, 5, 5, 5, 3, 5, 4, 4, 3, 4, 4, 5, 4, 2, 3, 3, 3,\n",
      "        3, 5, 4, 2, 3, 3, 3, 3])\n",
      "torch.Size([32, 1024, 1, 7])\n",
      "here\n",
      "tensor([[-3.9302, -3.4172, -3.7892, -3.6680, -3.6230, -3.5211, -3.6401, -3.6515,\n",
      "         -3.6798, -3.5135, -3.5638, -3.5513, -3.4558, -3.6349, -3.6521, -3.5204,\n",
      "         -3.6275, -3.6434, -3.5287, -3.7564, -3.7007, -3.4304, -3.7992, -3.7470,\n",
      "         -3.4934, -3.5894, -3.4880, -3.5769, -3.6536, -3.6229, -3.8104, -3.5011,\n",
      "         -3.5223, -3.7224, -3.4779, -3.5635, -3.7918],\n",
      "        [-3.8394, -3.6820, -3.5578, -3.4225, -3.3934, -3.7314, -3.5093, -3.7229,\n",
      "         -3.4250, -3.7806, -3.7834, -3.7494, -3.5044, -3.3989, -3.6826, -3.8148,\n",
      "         -3.3012, -3.8459, -3.5685, -3.5492, -3.4407, -3.3647, -3.5825, -4.5065,\n",
      "         -3.6102, -3.6626, -3.7307, -3.8694, -3.4524, -3.4846, -3.5781, -3.6006,\n",
      "         -3.6814, -4.3402, -3.3322, -3.4645, -3.6015],\n",
      "        [-3.1552, -4.0981, -3.3488, -3.1216, -3.2090, -4.1126, -3.4563, -3.9176,\n",
      "         -3.1598, -4.1936, -4.1200, -4.1067, -3.7199, -3.2118, -3.8172, -4.2054,\n",
      "         -3.0309, -4.1258, -3.7297, -3.3263, -3.1765, -3.4647, -3.3442, -5.3506,\n",
      "         -3.8868, -3.9029, -4.1614, -4.2848, -3.3147, -3.4215, -3.2672, -3.8936,\n",
      "         -4.0142, -5.0882, -3.3061, -3.4402, -3.3868],\n",
      "        [-1.7750, -4.5353, -3.3917, -3.1204, -3.2992, -4.5524, -3.5392, -4.1854,\n",
      "         -3.1449, -4.6026, -4.4833, -4.4994, -4.0568, -3.2599, -4.1076, -4.5547,\n",
      "         -3.0802, -4.4561, -3.9689, -3.2676, -3.1159, -3.7650, -3.3087, -6.0524,\n",
      "         -4.2821, -4.2649, -4.5801, -4.6081, -3.3970, -3.6181, -3.2196, -4.3520,\n",
      "         -4.4610, -5.7306, -3.4525, -3.5360, -3.3724],\n",
      "        [-0.5403, -5.2393, -4.0734, -3.8677, -4.0550, -5.2755, -4.1837, -4.8292,\n",
      "         -3.8645, -5.2633, -5.1088, -5.1932, -4.8127, -3.9348, -4.8757, -5.2125,\n",
      "         -3.8457, -5.1322, -4.6229, -3.8343, -3.7298, -4.5021, -3.9148, -6.7683,\n",
      "         -5.0417, -5.0311, -5.2442, -5.1492, -4.0814, -4.4192, -3.8789, -5.1716,\n",
      "         -5.2546, -6.4351, -4.1571, -4.1477, -4.0453],\n",
      "        [-0.1604, -6.0775, -5.1515, -5.0722, -5.2194, -6.1069, -5.2355, -5.6813,\n",
      "         -5.0299, -6.0964, -5.8746, -6.0432, -5.7479, -5.0311, -5.9012, -6.0304,\n",
      "         -5.0546, -5.9959, -5.5202, -4.8601, -4.8484, -5.5041, -4.9737, -7.3534,\n",
      "         -5.9978, -6.0277, -6.0556, -5.8653, -5.1477, -5.5047, -4.9772, -6.1429,\n",
      "         -6.2013, -7.0311, -5.2410, -5.1378, -5.2060],\n",
      "        [-0.1389, -5.9181, -5.3976, -5.4103, -5.5931, -5.9320, -5.4583, -5.6236,\n",
      "         -5.3145, -5.9338, -5.6995, -5.9096, -5.7498, -5.2942, -5.9891, -5.8464,\n",
      "         -5.4095, -5.8929, -5.5009, -5.0885, -5.1612, -5.6381, -5.2680, -6.7306,\n",
      "         -6.0031, -6.0875, -5.9022, -5.6584, -5.3923, -5.6251, -5.2737, -6.1100,\n",
      "         -6.1690, -6.4225, -5.4309, -5.3197, -5.5459]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([25, 22, 13,  4, 29], device='cuda:0')\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor([5, 3, 4, 5, 5, 3, 5, 5, 4, 3, 3, 3, 5, 5, 3, 3, 3, 4, 4, 3, 2, 4, 5, 4,\n",
      "        3, 5, 5, 5, 5, 4, 4, 5])\n",
      "torch.Size([32, 1024, 1, 7])\n",
      "here\n",
      "tensor([[-3.6741e+00, -3.2005e+00, -4.1540e+00, -4.0739e+00, -3.8622e+00,\n",
      "         -3.3135e+00, -3.8693e+00, -3.7343e+00, -4.1506e+00, -3.3045e+00,\n",
      "         -3.3767e+00, -3.3365e+00, -3.5087e+00, -4.0261e+00, -3.5685e+00,\n",
      "         -3.2757e+00, -4.1057e+00, -3.3748e+00, -3.4765e+00, -4.0737e+00,\n",
      "         -4.1929e+00, -3.5031e+00, -4.1668e+00, -3.0302e+00, -3.4646e+00,\n",
      "         -3.6108e+00, -3.3383e+00, -3.2346e+00, -4.0168e+00, -3.8985e+00,\n",
      "         -4.2437e+00, -3.4698e+00, -3.3835e+00, -3.2052e+00, -3.7329e+00,\n",
      "         -3.7078e+00, -4.1662e+00],\n",
      "        [-3.6859e+00, -3.2464e+00, -4.0639e+00, -4.0253e+00, -3.7623e+00,\n",
      "         -3.3067e+00, -3.7669e+00, -3.7157e+00, -4.0855e+00, -3.4064e+00,\n",
      "         -3.4304e+00, -3.3532e+00, -3.4171e+00, -3.9469e+00, -3.5251e+00,\n",
      "         -3.2875e+00, -3.9812e+00, -3.4507e+00, -3.3579e+00, -4.0305e+00,\n",
      "         -4.1385e+00, -3.3712e+00, -4.1189e+00, -3.4123e+00, -3.4286e+00,\n",
      "         -3.5895e+00, -3.3732e+00, -3.2584e+00, -3.9517e+00, -3.7625e+00,\n",
      "         -4.2463e+00, -3.4419e+00, -3.3328e+00, -3.5005e+00, -3.6429e+00,\n",
      "         -3.6575e+00, -4.1652e+00],\n",
      "        [-2.9087e+00, -3.3898e+00, -3.9356e+00, -3.8979e+00, -3.6074e+00,\n",
      "         -3.4066e+00, -3.6954e+00, -3.7452e+00, -3.9786e+00, -3.6305e+00,\n",
      "         -3.5895e+00, -3.4496e+00, -3.3388e+00, -3.8596e+00, -3.5168e+00,\n",
      "         -3.4277e+00, -3.8320e+00, -3.5900e+00, -3.2811e+00, -3.9074e+00,\n",
      "         -4.0813e+00, -3.2403e+00, -4.0003e+00, -3.9515e+00, -3.4374e+00,\n",
      "         -3.6544e+00, -3.5159e+00, -3.3242e+00, -3.8903e+00, -3.6039e+00,\n",
      "         -4.1442e+00, -3.5542e+00, -3.3872e+00, -3.9333e+00, -3.6070e+00,\n",
      "         -3.6142e+00, -4.1329e+00],\n",
      "        [-2.8593e-01, -4.8809e+00, -5.1268e+00, -5.1694e+00, -4.8218e+00,\n",
      "         -4.8510e+00, -4.9324e+00, -4.9862e+00, -5.2690e+00, -5.0997e+00,\n",
      "         -5.0175e+00, -4.9227e+00, -4.6096e+00, -5.0788e+00, -4.8757e+00,\n",
      "         -4.8943e+00, -5.1149e+00, -5.0555e+00, -4.5434e+00, -4.8323e+00,\n",
      "         -5.3286e+00, -4.4803e+00, -5.0230e+00, -5.4814e+00, -4.9568e+00,\n",
      "         -5.1472e+00, -4.9964e+00, -4.5785e+00, -4.9908e+00, -4.9636e+00,\n",
      "         -5.1981e+00, -5.2580e+00, -4.9036e+00, -5.5207e+00, -5.0262e+00,\n",
      "         -4.7944e+00, -5.3986e+00],\n",
      "        [-1.7796e-02, -7.6053e+00, -7.6676e+00, -7.8438e+00, -7.4639e+00,\n",
      "         -7.5457e+00, -7.5358e+00, -7.5547e+00, -7.9024e+00, -7.7692e+00,\n",
      "         -7.5597e+00, -7.6879e+00, -7.3469e+00, -7.6659e+00, -7.6463e+00,\n",
      "         -7.6400e+00, -7.7074e+00, -7.7394e+00, -7.1706e+00, -7.1740e+00,\n",
      "         -7.8403e+00, -7.1941e+00, -7.4401e+00, -8.1243e+00, -7.8282e+00,\n",
      "         -7.9811e+00, -7.8142e+00, -7.1717e+00, -7.4875e+00, -7.7191e+00,\n",
      "         -7.6499e+00, -8.1686e+00, -7.7763e+00, -8.1718e+00, -7.7471e+00,\n",
      "         -7.3112e+00, -7.9513e+00],\n",
      "        [-7.9527e-03, -8.4609e+00, -8.3993e+00, -8.6602e+00, -8.3086e+00,\n",
      "         -8.3770e+00, -8.3173e+00, -8.3364e+00, -8.6245e+00, -8.5373e+00,\n",
      "         -8.2846e+00, -8.5405e+00, -8.2678e+00, -8.4173e+00, -8.5453e+00,\n",
      "         -8.4643e+00, -8.4401e+00, -8.5553e+00, -7.9783e+00, -7.8827e+00,\n",
      "         -8.5110e+00, -8.1016e+00, -8.1955e+00, -8.9435e+00, -8.7166e+00,\n",
      "         -8.8687e+00, -8.6411e+00, -8.0270e+00, -8.2377e+00, -8.5456e+00,\n",
      "         -8.3664e+00, -9.0008e+00, -8.7143e+00, -8.8966e+00, -8.5268e+00,\n",
      "         -8.0680e+00, -8.6794e+00],\n",
      "        [-1.5145e-02, -7.8347e+00, -7.7741e+00, -7.9921e+00, -7.8714e+00,\n",
      "         -7.7376e+00, -7.6821e+00, -7.6807e+00, -7.8832e+00, -7.7814e+00,\n",
      "         -7.5866e+00, -7.9005e+00, -7.7248e+00, -7.7086e+00, -7.9944e+00,\n",
      "         -7.7735e+00, -7.7767e+00, -7.8935e+00, -7.4085e+00, -7.2295e+00,\n",
      "         -7.7436e+00, -7.6379e+00, -7.6229e+00, -8.1823e+00, -8.1015e+00,\n",
      "         -8.2568e+00, -7.9079e+00, -7.4504e+00, -7.6159e+00, -7.8776e+00,\n",
      "         -7.7100e+00, -8.2925e+00, -8.1597e+00, -8.0494e+00, -7.8179e+00,\n",
      "         -7.4521e+00, -7.9924e+00]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([18, 32, 12,  0,  0], device='cuda:0')\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor([3, 4, 3, 5, 3, 4, 5, 5, 5, 5, 2, 3, 5, 3, 4, 4, 5, 2, 3, 4, 5, 2, 5, 5,\n",
      "        5, 5, 2, 3, 2, 5, 4, 3])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1024, 1, 7])\n",
      "here\n",
      "tensor([[-3.8074e+00, -3.9631e+00, -3.2873e+00, -3.1748e+00, -3.1939e+00,\n",
      "         -3.9928e+00, -3.5354e+00, -3.8428e+00, -3.2084e+00, -4.0994e+00,\n",
      "         -3.9550e+00, -4.0636e+00, -3.6915e+00, -3.3225e+00, -3.8028e+00,\n",
      "         -4.0525e+00, -3.0854e+00, -4.0041e+00, -3.6809e+00, -3.3649e+00,\n",
      "         -3.2781e+00, -3.4375e+00, -3.2499e+00, -4.8984e+00, -3.7994e+00,\n",
      "         -3.8553e+00, -4.0745e+00, -4.1379e+00, -3.3017e+00, -3.4265e+00,\n",
      "         -3.2284e+00, -3.8072e+00, -3.8390e+00, -4.7857e+00, -3.3660e+00,\n",
      "         -3.5040e+00, -3.3922e+00],\n",
      "        [-3.4841e+00, -4.0708e+00, -3.2558e+00, -3.1064e+00, -3.0806e+00,\n",
      "         -4.1454e+00, -3.5068e+00, -3.9159e+00, -3.1982e+00, -4.2322e+00,\n",
      "         -4.0624e+00, -4.1469e+00, -3.7306e+00, -3.3263e+00, -3.8210e+00,\n",
      "         -4.1417e+00, -3.0125e+00, -4.0781e+00, -3.6436e+00, -3.3341e+00,\n",
      "         -3.2691e+00, -3.4363e+00, -3.2053e+00, -5.2385e+00, -3.9034e+00,\n",
      "         -3.9177e+00, -4.2141e+00, -4.2459e+00, -3.2729e+00, -3.4015e+00,\n",
      "         -3.1785e+00, -3.9490e+00, -3.9358e+00, -5.1265e+00, -3.3914e+00,\n",
      "         -3.4905e+00, -3.3169e+00],\n",
      "        [-1.6025e+00, -4.3319e+00, -3.3828e+00, -3.3395e+00, -3.1954e+00,\n",
      "         -4.4670e+00, -3.5956e+00, -4.1042e+00, -3.4287e+00, -4.5202e+00,\n",
      "         -4.3428e+00, -4.3541e+00, -3.9019e+00, -3.5692e+00, -3.9886e+00,\n",
      "         -4.4084e+00, -3.2344e+00, -4.3141e+00, -3.7126e+00, -3.4018e+00,\n",
      "         -3.4620e+00, -3.5882e+00, -3.3046e+00, -5.7257e+00, -4.1829e+00,\n",
      "         -4.2253e+00, -4.4873e+00, -4.3382e+00, -3.4203e+00, -3.6441e+00,\n",
      "         -3.3772e+00, -4.4181e+00, -4.2518e+00, -5.6319e+00, -3.5954e+00,\n",
      "         -3.5939e+00, -3.5078e+00],\n",
      "        [-3.5124e-02, -7.2798e+00, -6.6425e+00, -6.8694e+00, -6.4851e+00,\n",
      "         -7.3310e+00, -6.7546e+00, -7.0003e+00, -6.9035e+00, -7.4138e+00,\n",
      "         -7.1605e+00, -7.2967e+00, -6.8226e+00, -6.8836e+00, -7.0393e+00,\n",
      "         -7.3832e+00, -6.7129e+00, -7.2538e+00, -6.6178e+00, -6.4076e+00,\n",
      "         -6.8114e+00, -6.6028e+00, -6.3676e+00, -8.1644e+00, -7.3317e+00,\n",
      "         -7.3296e+00, -7.4326e+00, -6.8932e+00, -6.5460e+00, -7.0227e+00,\n",
      "         -6.6722e+00, -7.7335e+00, -7.3381e+00, -8.2545e+00, -6.8987e+00,\n",
      "         -6.6143e+00, -6.9317e+00],\n",
      "        [-6.8998e-03, -8.7541e+00, -8.3297e+00, -8.7151e+00, -8.2860e+00,\n",
      "         -8.7066e+00, -8.4565e+00, -8.5191e+00, -8.6326e+00, -8.8341e+00,\n",
      "         -8.5365e+00, -8.7805e+00, -8.4150e+00, -8.5627e+00, -8.7069e+00,\n",
      "         -8.8209e+00, -8.4983e+00, -8.7845e+00, -8.0831e+00, -8.0087e+00,\n",
      "         -8.5350e+00, -8.1984e+00, -8.0795e+00, -9.3900e+00, -8.9666e+00,\n",
      "         -9.0163e+00, -8.9448e+00, -8.3013e+00, -8.2452e+00, -8.7035e+00,\n",
      "         -8.3665e+00, -9.3011e+00, -8.9316e+00, -9.4279e+00, -8.6004e+00,\n",
      "         -8.1749e+00, -8.7230e+00],\n",
      "        [-5.1928e-03, -9.0167e+00, -8.6555e+00, -9.0843e+00, -8.6803e+00,\n",
      "         -8.8996e+00, -8.7521e+00, -8.7759e+00, -8.9248e+00, -9.0015e+00,\n",
      "         -8.7344e+00, -9.0162e+00, -8.7529e+00, -8.8397e+00, -9.0406e+00,\n",
      "         -9.0139e+00, -8.8223e+00, -9.0289e+00, -8.3426e+00, -8.2894e+00,\n",
      "         -8.8182e+00, -8.5450e+00, -8.4471e+00, -9.5187e+00, -9.2600e+00,\n",
      "         -9.3288e+00, -9.1645e+00, -8.5648e+00, -8.5655e+00, -8.9878e+00,\n",
      "         -8.6898e+00, -9.5220e+00, -9.2365e+00, -9.4713e+00, -8.9015e+00,\n",
      "         -8.4383e+00, -9.0360e+00],\n",
      "        [-1.2012e-02, -8.1609e+00, -7.8862e+00, -8.2297e+00, -8.0698e+00,\n",
      "         -8.0282e+00, -7.9211e+00, -7.9233e+00, -8.0199e+00, -8.0304e+00,\n",
      "         -7.8460e+00, -8.1365e+00, -7.9819e+00, -7.9359e+00, -8.2851e+00,\n",
      "         -8.0797e+00, -8.0063e+00, -8.1457e+00, -7.5870e+00, -7.4537e+00,\n",
      "         -7.8971e+00, -7.8697e+00, -7.7303e+00, -8.5115e+00, -8.4167e+00,\n",
      "         -8.5107e+00, -8.2094e+00, -7.7498e+00, -7.7840e+00, -8.1070e+00,\n",
      "         -7.8864e+00, -8.5973e+00, -8.4367e+00, -8.3739e+00, -8.0260e+00,\n",
      "         -7.6257e+00, -8.1861e+00]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([ 9,  3, 10,  0,  0], device='cuda:0')\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor([3, 4, 2, 3, 4, 3, 3, 4, 5, 2, 5, 3, 3, 4, 5, 5, 5, 4, 5, 5, 3, 5, 5, 3,\n",
      "        5, 3, 4, 5, 5, 5, 4, 5])\n",
      "torch.Size([32, 1024, 1, 7])\n",
      "here\n",
      "tensor([[-3.1796e+00, -3.2193e+00, -4.4592e+00, -4.3667e+00, -3.9952e+00,\n",
      "         -3.2892e+00, -4.1326e+00, -3.8541e+00, -4.4218e+00, -3.1975e+00,\n",
      "         -3.3667e+00, -3.2877e+00, -3.6221e+00, -4.2426e+00, -3.5987e+00,\n",
      "         -3.1959e+00, -4.4199e+00, -3.2880e+00, -3.5629e+00, -4.2974e+00,\n",
      "         -4.5214e+00, -3.5593e+00, -4.4234e+00, -2.5553e+00, -3.4865e+00,\n",
      "         -3.6764e+00, -3.3118e+00, -3.1235e+00, -4.2851e+00, -4.0940e+00,\n",
      "         -4.5270e+00, -3.5807e+00, -3.3605e+00, -2.9169e+00, -3.9624e+00,\n",
      "         -3.8818e+00, -4.3532e+00],\n",
      "        [-2.3229e+00, -3.2763e+00, -4.8233e+00, -4.7548e+00, -4.2631e+00,\n",
      "         -3.2849e+00, -4.3621e+00, -4.0056e+00, -4.8158e+00, -3.2270e+00,\n",
      "         -3.4908e+00, -3.3475e+00, -3.6611e+00, -4.5813e+00, -3.7583e+00,\n",
      "         -3.1510e+00, -4.7158e+00, -3.4282e+00, -3.5882e+00, -4.5963e+00,\n",
      "         -4.9279e+00, -3.5927e+00, -4.7509e+00, -2.2820e+00, -3.5267e+00,\n",
      "         -3.7390e+00, -3.3480e+00, -3.0459e+00, -4.6057e+00, -4.2932e+00,\n",
      "         -4.8885e+00, -3.7522e+00, -3.3829e+00, -2.8005e+00, -4.2245e+00,\n",
      "         -4.1064e+00, -4.7558e+00],\n",
      "        [-1.5737e-01, -5.1807e+00, -6.4414e+00, -6.4990e+00, -5.9536e+00,\n",
      "         -5.0600e+00, -6.0160e+00, -5.6307e+00, -6.6621e+00, -5.1334e+00,\n",
      "         -5.3476e+00, -5.2779e+00, -5.2220e+00, -6.3127e+00, -5.5830e+00,\n",
      "         -4.9956e+00, -6.3611e+00, -5.4853e+00, -5.0208e+00, -5.9969e+00,\n",
      "         -6.6031e+00, -5.1192e+00, -6.1921e+00, -4.3522e+00, -5.4244e+00,\n",
      "         -5.6847e+00, -5.2801e+00, -4.7377e+00, -6.1115e+00, -6.0036e+00,\n",
      "         -6.3685e+00, -5.8713e+00, -5.3833e+00, -4.9450e+00, -6.0791e+00,\n",
      "         -5.6571e+00, -6.4668e+00],\n",
      "        [-1.3054e-02, -7.7172e+00, -8.4520e+00, -8.5576e+00, -8.1582e+00,\n",
      "         -7.5528e+00, -8.1544e+00, -7.9536e+00, -8.8026e+00, -7.7227e+00,\n",
      "         -7.6942e+00, -7.8242e+00, -7.5469e+00, -8.4025e+00, -7.9968e+00,\n",
      "         -7.5806e+00, -8.4186e+00, -8.0006e+00, -7.2261e+00, -7.9099e+00,\n",
      "         -8.6017e+00, -7.4055e+00, -8.1596e+00, -7.4044e+00, -7.9593e+00,\n",
      "         -8.3193e+00, -7.9096e+00, -7.2503e+00, -8.1416e+00, -8.2067e+00,\n",
      "         -8.2786e+00, -8.4894e+00, -8.0083e+00, -7.8288e+00, -8.3305e+00,\n",
      "         -7.7509e+00, -8.5760e+00],\n",
      "        [-5.7821e-03, -8.6452e+00, -9.0465e+00, -9.2299e+00, -8.8656e+00,\n",
      "         -8.4648e+00, -8.8399e+00, -8.7235e+00, -9.4214e+00, -8.6292e+00,\n",
      "         -8.4518e+00, -8.7303e+00, -8.4405e+00, -9.0367e+00, -8.8429e+00,\n",
      "         -8.5244e+00, -9.0250e+00, -8.8497e+00, -8.0391e+00, -8.5003e+00,\n",
      "         -9.1662e+00, -8.2222e+00, -8.8111e+00, -8.5789e+00, -8.8273e+00,\n",
      "         -9.2264e+00, -8.8363e+00, -8.1617e+00, -8.7747e+00, -8.9394e+00,\n",
      "         -8.8862e+00, -9.3368e+00, -8.9157e+00, -8.8697e+00, -9.0208e+00,\n",
      "         -8.4417e+00, -9.1930e+00],\n",
      "        [-5.1908e-03, -8.8596e+00, -8.9964e+00, -9.2516e+00, -8.9106e+00,\n",
      "         -8.6649e+00, -8.8553e+00, -8.7837e+00, -9.3384e+00, -8.7860e+00,\n",
      "         -8.5599e+00, -8.9234e+00, -8.6739e+00, -9.0000e+00, -8.9806e+00,\n",
      "         -8.7397e+00, -8.9775e+00, -8.9707e+00, -8.2229e+00, -8.4641e+00,\n",
      "         -9.0750e+00, -8.4150e+00, -8.8265e+00, -8.9390e+00, -9.0028e+00,\n",
      "         -9.3522e+00, -8.9861e+00, -8.3942e+00, -8.7661e+00, -9.0291e+00,\n",
      "         -8.8685e+00, -9.4124e+00, -9.1176e+00, -9.0822e+00, -9.0105e+00,\n",
      "         -8.4767e+00, -9.1496e+00],\n",
      "        [-1.2414e-02, -8.0318e+00, -8.0780e+00, -8.2830e+00, -8.1827e+00,\n",
      "         -7.8645e+00, -7.9391e+00, -7.8883e+00, -8.2794e+00, -7.8752e+00,\n",
      "         -7.7023e+00, -8.0915e+00, -7.9147e+00, -7.9844e+00, -8.1867e+00,\n",
      "         -7.8856e+00, -8.0175e+00, -8.0804e+00, -7.4976e+00, -7.5177e+00,\n",
      "         -8.0188e+00, -7.7461e+00, -7.9751e+00, -8.1254e+00, -8.1848e+00,\n",
      "         -8.4829e+00, -8.0457e+00, -7.6369e+00, -7.8787e+00, -8.1196e+00,\n",
      "         -7.9574e+00, -8.4645e+00, -8.3513e+00, -8.1168e+00, -8.0285e+00,\n",
      "         -7.6025e+00, -8.1892e+00]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([22,  9,  0,  0,  0], device='cuda:0')\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor([2, 3, 2, 5, 2, 3, 3, 2, 3, 4, 2, 5, 5, 2, 3, 4, 4, 5, 4, 5, 4, 2, 4, 4,\n",
      "        5, 5, 4, 5, 5, 3, 5, 3])\n",
      "torch.Size([32, 1024, 1, 7])\n",
      "here\n",
      "tensor([[-3.4300e+00, -3.9612e+00, -3.2411e+00, -3.2837e+00, -3.1696e+00,\n",
      "         -4.0321e+00, -3.4545e+00, -3.8456e+00, -3.2962e+00, -4.0977e+00,\n",
      "         -3.9233e+00, -4.1001e+00, -3.7751e+00, -3.3690e+00, -3.8076e+00,\n",
      "         -4.1515e+00, -3.0330e+00, -4.0873e+00, -3.6331e+00, -3.3141e+00,\n",
      "         -3.3039e+00, -3.3376e+00, -3.2543e+00, -5.0696e+00, -3.9001e+00,\n",
      "         -3.8772e+00, -4.0763e+00, -4.2282e+00, -3.1766e+00, -3.5284e+00,\n",
      "         -3.2304e+00, -3.8950e+00, -3.8940e+00, -4.9522e+00, -3.3919e+00,\n",
      "         -3.4731e+00, -3.3970e+00],\n",
      "        [-2.0995e+00, -4.3636e+00, -3.1871e+00, -3.3112e+00, -3.1372e+00,\n",
      "         -4.4552e+00, -3.4407e+00, -4.0730e+00, -3.3388e+00, -4.4614e+00,\n",
      "         -4.2227e+00, -4.4174e+00, -4.0459e+00, -3.4358e+00, -4.0092e+00,\n",
      "         -4.5109e+00, -2.9823e+00, -4.3781e+00, -3.7306e+00, -3.2280e+00,\n",
      "         -3.2679e+00, -3.4695e+00, -3.1525e+00, -5.8255e+00, -4.2734e+00,\n",
      "         -4.1659e+00, -4.4788e+00, -4.5381e+00, -3.1093e+00, -3.6929e+00,\n",
      "         -3.1564e+00, -4.3710e+00, -4.2872e+00, -5.6960e+00, -3.4827e+00,\n",
      "         -3.4957e+00, -3.3765e+00],\n",
      "        [-6.0918e-02, -7.1927e+00, -5.8387e+00, -6.2475e+00, -5.9170e+00,\n",
      "         -7.2285e+00, -6.0799e+00, -6.6190e+00, -6.2355e+00, -7.1533e+00,\n",
      "         -6.7799e+00, -7.1524e+00, -6.7608e+00, -6.1558e+00, -6.7152e+00,\n",
      "         -7.2703e+00, -5.8379e+00, -7.0813e+00, -6.3336e+00, -5.6829e+00,\n",
      "         -5.9576e+00, -6.1045e+00, -5.6600e+00, -8.4382e+00, -7.1360e+00,\n",
      "         -6.9136e+00, -7.1694e+00, -6.9627e+00, -5.6692e+00, -6.6807e+00,\n",
      "         -5.8959e+00, -7.4062e+00, -7.1278e+00, -8.4007e+00, -6.1976e+00,\n",
      "         -6.0164e+00, -6.1718e+00],\n",
      "        [-7.8478e-03, -9.0919e+00, -7.8905e+00, -8.4759e+00, -8.0472e+00,\n",
      "         -9.0623e+00, -8.1392e+00, -8.4906e+00, -8.3433e+00, -9.0332e+00,\n",
      "         -8.5609e+00, -9.0469e+00, -8.6945e+00, -8.2483e+00, -8.7216e+00,\n",
      "         -9.1443e+00, -8.0136e+00, -9.0123e+00, -8.1977e+00, -7.6675e+00,\n",
      "         -8.0562e+00, -8.0654e+00, -7.7393e+00, -1.0115e+01, -9.1411e+00,\n",
      "         -8.9622e+00, -9.0914e+00, -8.7705e+00, -7.7411e+00, -8.8017e+00,\n",
      "         -7.9798e+00, -9.3760e+00, -9.0898e+00, -1.0047e+01, -8.2478e+00,\n",
      "         -7.9530e+00, -8.3552e+00],\n",
      "        [-4.4994e-03, -9.5764e+00, -8.4750e+00, -9.1337e+00, -8.6616e+00,\n",
      "         -9.4905e+00, -8.7295e+00, -8.9624e+00, -8.9182e+00, -9.4782e+00,\n",
      "         -8.9956e+00, -9.5095e+00, -9.1912e+00, -8.8419e+00, -9.2349e+00,\n",
      "         -9.5718e+00, -8.6580e+00, -9.5163e+00, -8.6517e+00, -8.2431e+00,\n",
      "         -8.6606e+00, -8.6195e+00, -8.3776e+00, -1.0473e+01, -9.6473e+00,\n",
      "         -9.5546e+00, -9.5737e+00, -9.2241e+00, -8.3529e+00, -9.3473e+00,\n",
      "         -8.5818e+00, -9.8544e+00, -9.6038e+00, -1.0359e+01, -8.8249e+00,\n",
      "         -8.4805e+00, -8.9848e+00],\n",
      "        [-4.5376e-03, -9.4821e+00, -8.5411e+00, -9.1800e+00, -8.7374e+00,\n",
      "         -9.3499e+00, -8.7570e+00, -8.8921e+00, -8.9185e+00, -9.3447e+00,\n",
      "         -8.9054e+00, -9.3872e+00, -9.1221e+00, -8.8604e+00, -9.2087e+00,\n",
      "         -9.4178e+00, -8.7383e+00, -9.4253e+00, -8.5871e+00, -8.2970e+00,\n",
      "         -8.6945e+00, -8.6571e+00, -8.4586e+00, -1.0222e+01, -9.5576e+00,\n",
      "         -9.5540e+00, -9.4606e+00, -9.0943e+00, -8.4163e+00, -9.2742e+00,\n",
      "         -8.6220e+00, -9.7532e+00, -9.5558e+00, -1.0071e+01, -8.8462e+00,\n",
      "         -8.4775e+00, -9.0238e+00],\n",
      "        [-1.1547e-02, -8.3950e+00, -7.7574e+00, -8.2291e+00, -8.0506e+00,\n",
      "         -8.2552e+00, -7.8971e+00, -7.9148e+00, -7.9637e+00, -8.2215e+00,\n",
      "         -7.8866e+00, -8.2911e+00, -8.1437e+00, -7.9108e+00, -8.3312e+00,\n",
      "         -8.2730e+00, -7.9297e+00, -8.3469e+00, -7.6619e+00, -7.4485e+00,\n",
      "         -7.7831e+00, -7.8711e+00, -7.6885e+00, -8.8891e+00, -8.5079e+00,\n",
      "         -8.6212e+00, -8.3286e+00, -7.9972e+00, -7.6432e+00, -8.2118e+00,\n",
      "         -7.7831e+00, -8.6820e+00, -8.5765e+00, -8.6817e+00, -7.9296e+00,\n",
      "         -7.6026e+00, -8.1475e+00]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([18, 29,  0,  0,  0], device='cuda:0')\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor([2, 3, 4, 3, 2, 3, 3, 5, 4, 4, 3, 5, 4, 3, 4, 5, 3, 2, 4, 5, 2, 5, 3, 4,\n",
      "        5, 2, 3, 5, 4, 5, 5, 3])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1024, 1, 7])\n",
      "here\n",
      "tensor([[-3.9093e+00, -3.7818e+00, -3.4482e+00, -3.3502e+00, -3.3709e+00,\n",
      "         -3.8106e+00, -3.4862e+00, -3.6792e+00, -3.3315e+00, -3.8713e+00,\n",
      "         -3.6960e+00, -3.8486e+00, -3.5878e+00, -3.4737e+00, -3.6954e+00,\n",
      "         -3.7784e+00, -3.2696e+00, -3.8349e+00, -3.6540e+00, -3.4788e+00,\n",
      "         -3.4155e+00, -3.3819e+00, -3.4253e+00, -4.6380e+00, -3.7072e+00,\n",
      "         -3.7936e+00, -3.8944e+00, -3.9368e+00, -3.3730e+00, -3.4289e+00,\n",
      "         -3.4034e+00, -3.6883e+00, -3.6815e+00, -4.5184e+00, -3.3328e+00,\n",
      "         -3.5070e+00, -3.5139e+00],\n",
      "        [-3.7028e+00, -3.6635e+00, -3.6079e+00, -3.4857e+00, -3.3783e+00,\n",
      "         -3.7266e+00, -3.4418e+00, -3.6571e+00, -3.4781e+00, -3.7970e+00,\n",
      "         -3.6029e+00, -3.7066e+00, -3.4770e+00, -3.6100e+00, -3.6112e+00,\n",
      "         -3.6406e+00, -3.3724e+00, -3.7580e+00, -3.4988e+00, -3.5921e+00,\n",
      "         -3.6003e+00, -3.2909e+00, -3.5589e+00, -4.6515e+00, -3.6379e+00,\n",
      "         -3.7485e+00, -3.7996e+00, -3.8189e+00, -3.4554e+00, -3.4170e+00,\n",
      "         -3.5728e+00, -3.6690e+00, -3.5956e+00, -4.5485e+00, -3.3720e+00,\n",
      "         -3.5030e+00, -3.6094e+00],\n",
      "        [-2.5308e+00, -3.5695e+00, -3.8483e+00, -3.7591e+00, -3.4808e+00,\n",
      "         -3.6498e+00, -3.4684e+00, -3.7027e+00, -3.7641e+00, -3.7786e+00,\n",
      "         -3.6087e+00, -3.5956e+00, -3.3494e+00, -3.8720e+00, -3.6084e+00,\n",
      "         -3.5327e+00, -3.6260e+00, -3.7332e+00, -3.2972e+00, -3.7545e+00,\n",
      "         -3.9277e+00, -3.2374e+00, -3.7646e+00, -4.5624e+00, -3.5904e+00,\n",
      "         -3.7889e+00, -3.7332e+00, -3.6219e+00, -3.6122e+00, -3.4996e+00,\n",
      "         -3.8492e+00, -3.7873e+00, -3.5871e+00, -4.5193e+00, -3.5507e+00,\n",
      "         -3.5628e+00, -3.8612e+00],\n",
      "        [-1.3476e-01, -5.5787e+00, -5.9420e+00, -6.0114e+00, -5.5688e+00,\n",
      "         -5.5552e+00, -5.4600e+00, -5.5810e+00, -6.0165e+00, -5.7075e+00,\n",
      "         -5.5760e+00, -5.5552e+00, -5.2158e+00, -5.9609e+00, -5.6605e+00,\n",
      "         -5.4577e+00, -5.8677e+00, -5.7805e+00, -5.0271e+00, -5.5300e+00,\n",
      "         -6.0901e+00, -5.1220e+00, -5.6855e+00, -6.1156e+00, -5.7094e+00,\n",
      "         -5.8818e+00, -5.7147e+00, -5.2917e+00, -5.4815e+00, -5.7054e+00,\n",
      "         -5.8061e+00, -6.1393e+00, -5.6970e+00, -6.2970e+00, -5.7943e+00,\n",
      "         -5.4412e+00, -6.0299e+00],\n",
      "        [-1.1580e-02, -8.0327e+00, -8.2758e+00, -8.5068e+00, -8.0551e+00,\n",
      "         -7.8890e+00, -7.8833e+00, -7.9166e+00, -8.4889e+00, -8.0616e+00,\n",
      "         -7.8538e+00, -8.0151e+00, -7.6927e+00, -8.3169e+00, -8.1760e+00,\n",
      "         -7.8942e+00, -8.2869e+00, -8.2325e+00, -7.3455e+00, -7.7072e+00,\n",
      "         -8.4274e+00, -7.5479e+00, -8.0182e+00, -8.3160e+00, -8.2533e+00,\n",
      "         -8.4345e+00, -8.2255e+00, -7.6042e+00, -7.8170e+00, -8.2168e+00,\n",
      "         -8.0884e+00, -8.7118e+00, -8.2191e+00, -8.5170e+00, -8.2811e+00,\n",
      "         -7.7255e+00, -8.4303e+00],\n",
      "        [-6.1798e-03, -8.7242e+00, -8.8162e+00, -9.1233e+00, -8.7236e+00,\n",
      "         -8.5262e+00, -8.5048e+00, -8.5280e+00, -9.0500e+00, -8.6518e+00,\n",
      "         -8.4307e+00, -8.7014e+00, -8.4342e+00, -8.8600e+00, -8.8703e+00,\n",
      "         -8.5544e+00, -8.8307e+00, -8.8617e+00, -8.0111e+00, -8.2268e+00,\n",
      "         -8.9198e+00, -8.2798e+00, -8.6266e+00, -8.9385e+00, -8.9402e+00,\n",
      "         -9.1143e+00, -8.8714e+00, -8.2797e+00, -8.4183e+00, -8.8670e+00,\n",
      "         -8.6461e+00, -9.3249e+00, -8.9497e+00, -9.0417e+00, -8.8689e+00,\n",
      "         -8.2985e+00, -8.9784e+00],\n",
      "        [-1.2254e-02, -8.0795e+00, -8.0860e+00, -8.3290e+00, -8.1978e+00,\n",
      "         -7.8998e+00, -7.8471e+00, -7.8446e+00, -8.2232e+00, -7.9022e+00,\n",
      "         -7.7437e+00, -8.0495e+00, -7.8731e+00, -8.0392e+00, -8.2571e+00,\n",
      "         -7.8891e+00, -8.0597e+00, -8.1384e+00, -7.4733e+00, -7.4809e+00,\n",
      "         -8.0491e+00, -7.7996e+00, -7.9714e+00, -8.2189e+00, -8.2852e+00,\n",
      "         -8.4534e+00, -8.1078e+00, -7.6666e+00, -7.7727e+00, -8.1555e+00,\n",
      "         -7.9316e+00, -8.5473e+00, -8.3741e+00, -8.1892e+00, -8.0816e+00,\n",
      "         -7.6140e+00, -8.2132e+00]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([36,  1, 35,  0,  0], device='cuda:0')\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor([3, 2, 5, 4, 5, 4, 3, 5, 5, 2, 4, 4, 2, 4, 5, 2, 5, 4, 3, 2, 5, 4, 5, 5,\n",
      "        3, 4, 4, 3, 4, 3, 2, 4])\n",
      "torch.Size([32, 1024, 1, 7])\n",
      "here\n",
      "tensor([[-3.5252, -3.2004, -4.3636, -4.2188, -4.0921, -3.3103, -4.0257, -3.8186,\n",
      "         -4.3852, -3.1783, -3.3778, -3.2526, -3.6191, -4.2183, -3.6572, -3.2160,\n",
      "         -4.3385, -3.3511, -3.5248, -4.2181, -4.4047, -3.7611, -4.3313, -2.5276,\n",
      "         -3.5503, -3.5703, -3.3173, -3.2161, -4.2706, -4.1239, -4.3915, -3.5190,\n",
      "         -3.4137, -2.7730, -3.9664, -3.8546, -4.3191],\n",
      "        [-3.7687, -3.2091, -4.2419, -4.1744, -4.0399, -3.2818, -3.8809, -3.7394,\n",
      "         -4.2563, -3.2030, -3.3734, -3.2285, -3.5152, -4.1170, -3.6015, -3.1743,\n",
      "         -4.1839, -3.4236, -3.3941, -4.1789, -4.2757, -3.6752, -4.2487, -2.9003,\n",
      "         -3.5152, -3.5162, -3.2975, -3.2480, -4.1553, -3.9759, -4.3723, -3.4021,\n",
      "         -3.3365, -3.0300, -3.8305, -3.7627, -4.2936],\n",
      "        [-3.7136, -3.3208, -4.0485, -4.0348, -3.8710, -3.3276, -3.6922, -3.6335,\n",
      "         -4.0480, -3.3323, -3.4244, -3.2618, -3.4333, -3.9412, -3.5488, -3.2409,\n",
      "         -3.9454, -3.5259, -3.3102, -4.0509, -4.0736, -3.5124, -4.0879, -3.5027,\n",
      "         -3.4997, -3.5028, -3.3482, -3.3491, -3.9758, -3.7538, -4.2306, -3.3689,\n",
      "         -3.3512, -3.4611, -3.6466, -3.6369, -4.1584],\n",
      "        [-2.8853, -3.5052, -3.9412, -3.9419, -3.7614, -3.4296, -3.5944, -3.5934,\n",
      "         -3.9272, -3.4988, -3.5471, -3.3457, -3.4137, -3.8359, -3.5637, -3.3576,\n",
      "         -3.8148, -3.6554, -3.2837, -3.9175, -3.9666, -3.4166, -4.0218, -4.0165,\n",
      "         -3.5582, -3.5740, -3.4597, -3.4637, -3.8794, -3.6344, -4.0910, -3.4714,\n",
      "         -3.4699, -3.8333, -3.5817, -3.5606, -4.0805],\n",
      "        [-1.4310, -3.8168, -4.1071, -4.1020, -3.9224, -3.6960, -3.7629, -3.7526,\n",
      "         -4.1069, -3.7770, -3.8328, -3.6031, -3.6054, -3.9572, -3.8306, -3.6060,\n",
      "         -3.9963, -3.9071, -3.4463, -3.9557, -4.1086, -3.5896, -4.1645, -4.3794,\n",
      "         -3.8362, -3.8669, -3.7084, -3.6603, -3.9788, -3.8261, -4.1767, -3.8368,\n",
      "         -3.8203, -4.1530, -3.7845, -3.6883, -4.2587],\n",
      "        [-0.5089, -4.4887, -4.7210, -4.7234, -4.6036, -4.3739, -4.4211, -4.3599,\n",
      "         -4.7615, -4.4240, -4.4719, -4.3029, -4.2845, -4.5291, -4.5790, -4.2828,\n",
      "         -4.6349, -4.5502, -4.0799, -4.4271, -4.6675, -4.2797, -4.7249, -4.9175,\n",
      "         -4.5730, -4.6066, -4.3771, -4.2558, -4.5275, -4.5219, -4.7151, -4.6321,\n",
      "         -4.6284, -4.7103, -4.4457, -4.2738, -4.8518],\n",
      "        [-0.3411, -4.8076, -5.0085, -5.0045, -5.0583, -4.7162, -4.7902, -4.6855,\n",
      "         -5.0581, -4.7055, -4.7404, -4.6869, -4.6912, -4.7773, -5.0148, -4.6397,\n",
      "         -4.9358, -4.8716, -4.4628, -4.6243, -4.8546, -4.7186, -4.9993, -5.0302,\n",
      "         -4.9648, -5.0172, -4.7120, -4.5681, -4.8198, -4.8737, -4.9522, -5.0222,\n",
      "         -5.0965, -4.8314, -4.7625, -4.6013, -5.0950]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([15, 12, 10, 11,  8], device='cuda:0')\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor([5, 5, 4, 4, 4, 4, 5, 4, 2, 2, 5, 5, 3, 3, 3, 5, 4, 2, 4, 3, 5, 2, 4, 3,\n",
      "        5, 4, 4, 5, 4, 3, 5, 5])\n",
      "torch.Size([32, 1024, 1, 7])\n",
      "here\n",
      "tensor([[-3.6072, -3.9874, -3.3143, -3.2029, -3.2937, -4.0158, -3.5415, -3.7236,\n",
      "         -3.1824, -4.0079, -3.8463, -4.0737, -3.8007, -3.3008, -3.8259, -3.9307,\n",
      "         -3.1565, -3.8657, -3.7906, -3.3724, -3.2232, -3.5232, -3.3286, -4.7197,\n",
      "         -3.8773, -3.9193, -4.1453, -4.0834, -3.3255, -3.4244, -3.2188, -3.8405,\n",
      "         -3.8850, -4.5928, -3.3698, -3.4924, -3.2933],\n",
      "        [-3.4145, -3.7910, -3.5929, -3.3648, -3.3971, -3.8744, -3.5645, -3.6542,\n",
      "         -3.3969, -3.7353, -3.6418, -3.7468, -3.6736, -3.4935, -3.6918, -3.6318,\n",
      "         -3.3887, -3.6152, -3.6682, -3.5706, -3.4612, -3.4397, -3.5418, -4.3788,\n",
      "         -3.7435, -3.8067, -3.8898, -3.8329, -3.5093, -3.4272, -3.4565, -3.7499,\n",
      "         -3.7215, -4.2619, -3.3987, -3.4935, -3.4156],\n",
      "        [-2.7798, -3.3736, -4.2943, -4.0026, -3.8312, -3.5394, -3.8768, -3.7451,\n",
      "         -4.1260, -3.2334, -3.4414, -3.2543, -3.5355, -4.0837, -3.6469, -3.2011,\n",
      "         -4.1111, -3.3542, -3.5645, -4.1147, -4.1809, -3.5353, -4.1938, -3.1321,\n",
      "         -3.5638, -3.6460, -3.3956, -3.3276, -4.1391, -3.8044, -4.1928, -3.5822,\n",
      "         -3.5627, -3.1989, -3.8111, -3.7558, -3.9639],\n",
      "        [-1.7607, -3.5236, -5.2628, -4.8901, -4.6660, -3.6269, -4.5354, -4.1243,\n",
      "         -5.0484, -3.1614, -3.6832, -3.3206, -3.8949, -4.8770, -4.0681, -3.1625,\n",
      "         -5.0205, -3.6394, -3.8654, -4.7779, -5.0379, -4.0040, -5.0726, -2.0362,\n",
      "         -3.9175, -3.8352, -3.4227, -3.3105, -5.0052, -4.5619, -5.0259, -3.9332,\n",
      "         -3.8338, -2.3634, -4.5363, -4.3354, -4.7865],\n",
      "        [-0.7492, -4.1790, -6.1930, -5.7875, -5.5724, -4.1579, -5.3223, -4.7167,\n",
      "         -5.9729, -3.6300, -4.3416, -3.9287, -4.5689, -5.6889, -4.8481, -3.6800,\n",
      "         -5.8950, -4.3592, -4.4403, -5.4468, -5.8443, -4.6919, -5.9259, -2.0378,\n",
      "         -4.6726, -4.4444, -4.0004, -3.8147, -5.7998, -5.3491, -5.8263, -4.7054,\n",
      "         -4.5643, -2.5004, -5.3345, -5.0486, -5.6860],\n",
      "        [-0.1985, -5.2686, -7.0889, -6.7601, -6.5273, -5.1212, -6.2418, -5.6042,\n",
      "         -6.9320, -4.6774, -5.3953, -4.9992, -5.5206, -6.5533, -5.9341, -4.7145,\n",
      "         -6.8165, -5.4460, -5.2986, -6.2146, -6.7289, -5.6674, -6.7804, -3.2328,\n",
      "         -5.7839, -5.5140, -5.0794, -4.8035, -6.6080, -6.2683, -6.6672, -5.8496,\n",
      "         -5.7908, -3.6570, -6.2944, -5.9418, -6.6344],\n",
      "        [-0.0980, -5.8174, -7.1045, -6.9145, -6.8123, -5.6304, -6.4415, -6.0066,\n",
      "         -7.0373, -5.3377, -5.8638, -5.6496, -5.9098, -6.6410, -6.4128, -5.3761,\n",
      "         -6.8889, -5.9983, -5.6465, -6.2786, -6.7436, -6.0978, -6.8568, -4.4792,\n",
      "         -6.3003, -6.1368, -5.6684, -5.4278, -6.6522, -6.5090, -6.7455, -6.3632,\n",
      "         -6.4267, -4.7081, -6.5648, -6.1762, -6.8020]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([30, 21, 31,  9, 34], device='cuda:0')\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor([5, 3, 5, 3, 5, 3, 5, 2, 4, 3, 5, 4, 5, 5, 3, 4, 3, 3, 4, 3, 3, 5, 5, 4,\n",
      "        4, 4, 5, 5, 4, 5, 3, 3])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1024, 1, 7])\n",
      "here\n",
      "tensor([[-4.0994, -3.4269, -3.7470, -3.6593, -3.6224, -3.5381, -3.5913, -3.6126,\n",
      "         -3.6967, -3.5242, -3.4823, -3.5231, -3.4778, -3.6743, -3.5552, -3.4892,\n",
      "         -3.6419, -3.5827, -3.5449, -3.7714, -3.7032, -3.4679, -3.7037, -3.9326,\n",
      "         -3.5126, -3.6279, -3.5124, -3.5872, -3.6606, -3.5939, -3.8181, -3.4556,\n",
      "         -3.5000, -3.8532, -3.4584, -3.5358, -3.7816],\n",
      "        [-4.0721, -3.4224, -3.7888, -3.6917, -3.5790, -3.5336, -3.5560, -3.6165,\n",
      "         -3.7289, -3.5398, -3.4754, -3.4829, -3.4434, -3.7006, -3.5174, -3.4773,\n",
      "         -3.6299, -3.5874, -3.4748, -3.8044, -3.7505, -3.4056, -3.7545, -4.1143,\n",
      "         -3.4893, -3.6077, -3.5118, -3.5856, -3.6927, -3.5624, -3.8724, -3.4402,\n",
      "         -3.4863, -3.9818, -3.4360, -3.5201, -3.8152],\n",
      "        [-3.6796, -3.3981, -3.8804, -3.7768, -3.5978, -3.4956, -3.5523, -3.6134,\n",
      "         -3.8059, -3.5263, -3.4742, -3.4250, -3.3966, -3.7804, -3.4921, -3.4175,\n",
      "         -3.7016, -3.5819, -3.3780, -3.8639, -3.8703, -3.3466, -3.8811, -4.1829,\n",
      "         -3.4497, -3.5998, -3.4771, -3.5301, -3.7836, -3.5583, -3.9739, -3.4518,\n",
      "         -3.4651, -4.0187, -3.4794, -3.5328, -3.9169],\n",
      "        [-2.4626, -3.4488, -4.0626, -3.9585, -3.7118, -3.4997, -3.6283, -3.6520,\n",
      "         -3.9699, -3.5494, -3.5778, -3.4121, -3.3723, -3.9337, -3.5628, -3.3729,\n",
      "         -3.8946, -3.6305, -3.3038, -3.9433, -4.0911, -3.3392, -4.0891, -4.1933,\n",
      "         -3.4553, -3.6651, -3.4623, -3.4638, -3.9213, -3.6268, -4.1258, -3.6000,\n",
      "         -3.5254, -4.0145, -3.6181, -3.5941, -4.1393],\n",
      "        [-0.4316, -4.4792, -5.0617, -5.0181, -4.6927, -4.4369, -4.5885, -4.5237,\n",
      "         -5.0080, -4.4885, -4.5588, -4.3808, -4.3103, -4.8561, -4.6007, -4.3099,\n",
      "         -4.9520, -4.6119, -4.1408, -4.7090, -5.0917, -4.2287, -5.0013, -4.9895,\n",
      "         -4.5156, -4.7247, -4.4211, -4.2877, -4.7291, -4.6824, -4.9765, -4.8457,\n",
      "         -4.6262, -4.8784, -4.6690, -4.4630, -5.1619],\n",
      "        [-0.0403, -6.7902, -7.1024, -7.1859, -6.8702, -6.6572, -6.7250, -6.6346,\n",
      "         -7.1481, -6.7090, -6.6671, -6.7026, -6.5964, -6.8988, -6.8938, -6.6175,\n",
      "         -7.0593, -6.8572, -6.3020, -6.5777, -7.1055, -6.4529, -6.9695, -7.1207,\n",
      "         -6.9300, -7.0848, -6.7684, -6.4699, -6.6865, -6.9608, -6.9127, -7.2896,\n",
      "         -7.0365, -7.0394, -6.8657, -6.5038, -7.1907],\n",
      "        [-0.0314, -7.1278, -7.2235, -7.3245, -7.2538, -6.9822, -6.9682, -6.8740,\n",
      "         -7.2484, -6.9504, -6.8543, -7.0457, -6.9764, -6.9949, -7.2607, -6.9409,\n",
      "         -7.1700, -7.1390, -6.6460, -6.6545, -7.1112, -6.8797, -7.1396, -7.3522,\n",
      "         -7.2928, -7.4501, -7.0578, -6.7971, -6.8765, -7.1978, -7.0276, -7.5496,\n",
      "         -7.4485, -7.1870, -7.0283, -6.7148, -7.2911]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([21, 16, 14,  7,  0], device='cuda:0')\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor([4, 5, 2, 5, 3, 4, 3, 4, 5, 5, 2, 4, 2, 3, 3, 2, 3, 3, 4, 3, 3, 4, 4, 5,\n",
      "        5, 5, 4, 5, 3, 2, 4, 4])\n",
      "torch.Size([32, 1024, 1, 7])\n",
      "here\n",
      "tensor([[-3.8399, -3.6823, -3.5582, -3.4674, -3.4679, -3.7486, -3.5522, -3.6303,\n",
      "         -3.4631, -3.6932, -3.5709, -3.6811, -3.6220, -3.4846, -3.6006, -3.6196,\n",
      "         -3.4226, -3.6352, -3.6484, -3.5974, -3.5185, -3.4215, -3.5330, -4.2807,\n",
      "         -3.6654, -3.7547, -3.7718, -3.7835, -3.5214, -3.4826, -3.5661, -3.6118,\n",
      "         -3.6127, -4.1602, -3.4156, -3.4918, -3.5506],\n",
      "        [-3.5726, -3.3780, -3.9809, -3.8288, -3.6675, -3.5356, -3.6769, -3.6634,\n",
      "         -3.8862, -3.3767, -3.3702, -3.3144, -3.4957, -3.8331, -3.4731, -3.3266,\n",
      "         -3.8168, -3.4136, -3.4956, -3.9126, -3.9583, -3.4063, -3.9097, -3.7288,\n",
      "         -3.5095, -3.6506, -3.4740, -3.4692, -3.8782, -3.6061, -4.0402, -3.4745,\n",
      "         -3.4842, -3.6953, -3.6147, -3.5762, -3.8629],\n",
      "        [-2.7233, -3.2682, -4.6014, -4.3830, -4.0798, -3.4053, -3.9868, -3.8249,\n",
      "         -4.4498, -3.1316, -3.3473, -3.0841, -3.5410, -4.3145, -3.5803, -3.0781,\n",
      "         -4.4022, -3.4016, -3.5180, -4.3176, -4.5403, -3.5703, -4.4605, -2.8734,\n",
      "         -3.5362, -3.6348, -3.2708, -3.2155, -4.4340, -3.9542, -4.6040, -3.5111,\n",
      "         -3.5238, -3.0125, -4.0272, -3.8414, -4.3600],\n",
      "        [-1.2130, -3.6405, -5.4473, -5.1723, -4.8349, -3.6789, -4.5971, -4.1966,\n",
      "         -5.2125, -3.2871, -3.7533, -3.3263, -3.9716, -5.0023, -4.1113, -3.2495,\n",
      "         -5.1895, -3.8543, -3.9033, -4.8711, -5.2609, -4.0489, -5.2105, -2.3654,\n",
      "         -4.0002, -3.9762, -3.5202, -3.3994, -5.1658, -4.5847, -5.2666, -4.0458,\n",
      "         -3.9918, -2.7489, -4.6999, -4.3931, -5.1027],\n",
      "        [-0.2081, -5.0774, -6.8377, -6.6020, -6.2594, -4.9668, -5.9703, -5.4330,\n",
      "         -6.6575, -4.6114, -5.1758, -4.6948, -5.3074, -6.3263, -5.6118, -4.5900,\n",
      "         -6.5731, -5.2857, -5.1243, -6.0468, -6.6033, -5.3763, -6.5428, -3.4849,\n",
      "         -5.4632, -5.3737, -4.9392, -4.6795, -6.4279, -5.9547, -6.5158, -5.6232,\n",
      "         -5.5710, -3.9967, -6.1079, -5.6852, -6.4656],\n",
      "        [-0.0443, -6.5527, -7.9490, -7.8033, -7.5205, -6.3761, -7.1996, -6.7488,\n",
      "         -7.8759, -6.1278, -6.5958, -6.2023, -6.6407, -7.4767, -7.0738, -6.0968,\n",
      "         -7.7316, -6.7423, -6.3580, -7.1197, -7.7284, -6.7123, -7.6925, -5.2416,\n",
      "         -6.9347, -6.9004, -6.4671, -6.1343, -7.4974, -7.2311, -7.6010, -7.1630,\n",
      "         -7.1440, -5.6874, -7.3961, -6.9030, -7.6404],\n",
      "        [-0.0420, -6.6207, -7.5527, -7.4643, -7.4092, -6.4548, -6.9776, -6.7261,\n",
      "         -7.5313, -6.2773, -6.5647, -6.4190, -6.6411, -7.1514, -7.1062, -6.2739,\n",
      "         -7.3723, -6.7772, -6.3418, -6.8009, -7.2624, -6.7491, -7.3914, -5.7970,\n",
      "         -6.9877, -7.0451, -6.5658, -6.2831, -7.1448, -7.0636, -7.2413, -7.1751,\n",
      "         -7.2110, -6.0198, -7.1683, -6.7134, -7.3490]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([29, 34,  1,  7,  0], device='cuda:0')\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor([4, 3, 3, 5, 2, 5, 4, 3, 2, 4, 2, 5, 4, 3, 4, 3, 3, 4, 3, 5, 5, 4, 4, 3,\n",
      "        4, 4, 5, 4, 5, 4, 5, 3])\n",
      "torch.Size([32, 1024, 1, 7])\n",
      "here\n",
      "tensor([[-3.9297e+00, -3.9740e+00, -3.2155e+00, -3.2560e+00, -3.2167e+00,\n",
      "         -4.0289e+00, -3.4692e+00, -3.7809e+00, -3.1709e+00, -4.1006e+00,\n",
      "         -3.9635e+00, -4.0791e+00, -3.8012e+00, -3.2940e+00, -3.8228e+00,\n",
      "         -4.0523e+00, -3.0517e+00, -3.9851e+00, -3.7566e+00, -3.3600e+00,\n",
      "         -3.2451e+00, -3.4759e+00, -3.2823e+00, -4.9103e+00, -3.8072e+00,\n",
      "         -3.9112e+00, -4.1136e+00, -4.1985e+00, -3.2507e+00, -3.4633e+00,\n",
      "         -3.1655e+00, -3.8685e+00, -3.8444e+00, -4.8050e+00, -3.3030e+00,\n",
      "         -3.4788e+00, -3.3475e+00],\n",
      "        [-3.8790e+00, -3.9413e+00, -3.2859e+00, -3.3201e+00, -3.1443e+00,\n",
      "         -4.0570e+00, -3.3937e+00, -3.7753e+00, -3.2354e+00, -4.0700e+00,\n",
      "         -3.9250e+00, -4.0018e+00, -3.7792e+00, -3.3263e+00, -3.7648e+00,\n",
      "         -4.0139e+00, -3.0381e+00, -3.9746e+00, -3.6974e+00, -3.4022e+00,\n",
      "         -3.3188e+00, -3.4009e+00, -3.3326e+00, -5.0778e+00, -3.7849e+00,\n",
      "         -3.8721e+00, -4.0573e+00, -4.2160e+00, -3.2389e+00, -3.4512e+00,\n",
      "         -3.2270e+00, -3.8904e+00, -3.8290e+00, -4.9644e+00, -3.2950e+00,\n",
      "         -3.4345e+00, -3.3786e+00],\n",
      "        [-2.8605e+00, -4.0771e+00, -3.2908e+00, -3.4773e+00, -3.0716e+00,\n",
      "         -4.1939e+00, -3.2523e+00, -3.8295e+00, -3.2782e+00, -4.1502e+00,\n",
      "         -4.0146e+00, -4.0287e+00, -3.8636e+00, -3.3458e+00, -3.7441e+00,\n",
      "         -4.1039e+00, -3.0552e+00, -4.0724e+00, -3.6742e+00, -3.3762e+00,\n",
      "         -3.3811e+00, -3.3650e+00, -3.3658e+00, -5.4780e+00, -3.8708e+00,\n",
      "         -3.9678e+00, -4.1063e+00, -4.3156e+00, -3.1701e+00, -3.5593e+00,\n",
      "         -3.2542e+00, -4.1365e+00, -3.9623e+00, -5.3570e+00, -3.3503e+00,\n",
      "         -3.3611e+00, -3.4725e+00],\n",
      "        [-1.5308e-01, -6.2397e+00, -5.0749e+00, -5.5627e+00, -4.9810e+00,\n",
      "         -6.2678e+00, -5.0506e+00, -5.6331e+00, -5.2269e+00, -6.1096e+00,\n",
      "         -5.9192e+00, -6.0314e+00, -5.8925e+00, -5.1989e+00, -5.6920e+00,\n",
      "         -6.1391e+00, -5.0833e+00, -6.0493e+00, -5.5578e+00, -4.9523e+00,\n",
      "         -5.2079e+00, -5.2695e+00, -5.0572e+00, -7.4233e+00, -6.0087e+00,\n",
      "         -5.9515e+00, -6.0759e+00, -6.1126e+00, -4.7935e+00, -5.7565e+00,\n",
      "         -5.0458e+00, -6.4944e+00, -6.1189e+00, -7.4048e+00, -5.2976e+00,\n",
      "         -5.0461e+00, -5.4301e+00],\n",
      "        [-1.1454e-02, -8.7667e+00, -7.5680e+00, -8.1982e+00, -7.6345e+00,\n",
      "         -8.6932e+00, -7.6505e+00, -8.0211e+00, -7.8031e+00, -8.5763e+00,\n",
      "         -8.2393e+00, -8.5272e+00, -8.4265e+00, -7.7453e+00, -8.2784e+00,\n",
      "         -8.6438e+00, -7.7400e+00, -8.5551e+00, -8.0022e+00, -7.3406e+00,\n",
      "         -7.7052e+00, -7.7989e+00, -7.5427e+00, -9.6996e+00, -8.6538e+00,\n",
      "         -8.5597e+00, -8.5996e+00, -8.4772e+00, -7.3271e+00, -8.4135e+00,\n",
      "         -7.5650e+00, -9.0690e+00, -8.7038e+00, -9.6294e+00, -7.8547e+00,\n",
      "         -7.4748e+00, -8.0361e+00],\n",
      "        [-6.2037e-03, -9.3174e+00, -8.1965e+00, -8.8533e+00, -8.3635e+00,\n",
      "         -9.1765e+00, -8.3307e+00, -8.5476e+00, -8.4198e+00, -9.0948e+00,\n",
      "         -8.7160e+00, -9.0507e+00, -8.9869e+00, -8.3840e+00, -8.8968e+00,\n",
      "         -9.1471e+00, -8.4367e+00, -9.1221e+00, -8.5338e+00, -7.9614e+00,\n",
      "         -8.3147e+00, -8.4521e+00, -8.2243e+00, -1.0094e+01, -9.2400e+00,\n",
      "         -9.2171e+00, -9.1307e+00, -8.9755e+00, -8.0178e+00, -8.9936e+00,\n",
      "         -8.2080e+00, -9.5882e+00, -9.3094e+00, -9.9319e+00, -8.4700e+00,\n",
      "         -8.0763e+00, -8.6882e+00],\n",
      "        [-1.2557e-02, -8.4496e+00, -7.6164e+00, -8.1181e+00, -7.9124e+00,\n",
      "         -8.2923e+00, -7.7382e+00, -7.7881e+00, -7.7260e+00, -8.2054e+00,\n",
      "         -7.8800e+00, -8.1931e+00, -8.1882e+00, -7.6955e+00, -8.2340e+00,\n",
      "         -8.2422e+00, -7.8461e+00, -8.2903e+00, -7.7796e+00, -7.3434e+00,\n",
      "         -7.6232e+00, -7.8595e+00, -7.6585e+00, -8.9755e+00, -8.4366e+00,\n",
      "         -8.5220e+00, -8.2498e+00, -8.0740e+00, -7.4764e+00, -8.1526e+00,\n",
      "         -7.5939e+00, -8.7074e+00, -8.5552e+00, -8.7372e+00, -7.7684e+00,\n",
      "         -7.4518e+00, -8.0256e+00]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([ 8, 35, 14,  0,  0], device='cuda:0')\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor([3, 3, 3, 4, 2, 3, 4, 5, 2, 2, 4, 3, 3, 4, 4, 3, 5, 4, 4, 4, 5, 3, 2, 3,\n",
      "        2, 4, 3, 5, 4, 5, 4, 3])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1024, 1, 7])\n",
      "here\n",
      "tensor([[-3.7257, -3.4502, -3.7519, -3.7398, -3.6151, -3.5127, -3.5516, -3.6077,\n",
      "         -3.6681, -3.5311, -3.5315, -3.5490, -3.5170, -3.6651, -3.6338, -3.5419,\n",
      "         -3.5815, -3.6788, -3.5303, -3.7322, -3.6978, -3.4142, -3.8125, -3.8752,\n",
      "         -3.5086, -3.5996, -3.4430, -3.6191, -3.6553, -3.6497, -3.7952, -3.4957,\n",
      "         -3.5125, -3.8164, -3.5044, -3.5380, -3.7977],\n",
      "        [-3.5270, -3.7541, -3.4292, -3.5426, -3.3565, -3.7722, -3.3973, -3.6823,\n",
      "         -3.3774, -3.8387, -3.7907, -3.7943, -3.6485, -3.4055, -3.6716, -3.9092,\n",
      "         -3.2100, -3.9099, -3.5761, -3.5116, -3.3736, -3.3806, -3.5944, -4.6924,\n",
      "         -3.6867, -3.7197, -3.7399, -3.9885, -3.3972, -3.5623, -3.4889, -3.6529,\n",
      "         -3.7033, -4.5116, -3.3546, -3.4154, -3.5955],\n",
      "        [-2.6107, -4.3198, -3.1313, -3.3122, -3.1806, -4.3022, -3.3769, -3.9268,\n",
      "         -3.0852, -4.3973, -4.2519, -4.2656, -4.0335, -3.2092, -3.8891, -4.4396,\n",
      "         -2.9275, -4.2804, -3.8215, -3.3086, -3.0467, -3.5691, -3.3867, -5.6945,\n",
      "         -4.1093, -4.0917, -4.3121, -4.5417, -3.2096, -3.6068, -3.1217, -4.1118,\n",
      "         -4.1600, -5.4421, -3.3286, -3.3936, -3.3741],\n",
      "        [-0.8661, -5.2173, -3.4258, -3.7073, -3.6372, -5.2134, -3.8324, -4.5385,\n",
      "         -3.4146, -5.2228, -5.0082, -5.0778, -4.8697, -3.5568, -4.6045, -5.2096,\n",
      "         -3.3307, -5.0265, -4.4670, -3.5574, -3.2426, -4.2446, -3.6909, -6.8128,\n",
      "         -4.9486, -4.8896, -5.1252, -5.2762, -3.5610, -4.2500, -3.3705, -5.0470,\n",
      "         -5.0545, -6.5492, -3.7943, -3.7853, -3.6509],\n",
      "        [-0.1058, -7.0258, -5.1543, -5.5709, -5.4840, -7.0108, -5.5996, -6.2022,\n",
      "         -5.2462, -6.9228, -6.6478, -6.8336, -6.6975, -5.2653, -6.4564, -6.9292,\n",
      "         -5.2334, -6.7976, -6.2167, -5.1633, -4.9486, -6.0138, -5.3459, -8.5179,\n",
      "         -6.7541, -6.7088, -6.7851, -6.8907, -5.2567, -6.1547, -5.0903, -6.9664,\n",
      "         -6.9112, -8.2883, -5.5655, -5.4395, -5.3959],\n",
      "        [-0.0253, -8.2458, -6.6059, -7.1263, -6.9491, -8.1756, -7.0192, -7.3980,\n",
      "         -6.7500, -8.0986, -7.7288, -8.0356, -7.9252, -6.7433, -7.7743, -8.0893,\n",
      "         -6.8090, -8.0282, -7.4846, -6.5263, -6.4590, -7.3363, -6.7239, -9.4341,\n",
      "         -8.0341, -7.9977, -7.9321, -7.9858, -6.6422, -7.5588, -6.5485, -8.2884,\n",
      "         -8.1757, -9.2223, -6.9730, -6.7769, -6.9206],\n",
      "        [-0.0276, -7.8411, -6.6788, -7.1292, -7.0564, -7.7383, -6.9990, -7.1166,\n",
      "         -6.7538, -7.6936, -7.2845, -7.6552, -7.5990, -6.7755, -7.5870, -7.6541,\n",
      "         -6.9121, -7.6653, -7.2199, -6.5234, -6.5544, -7.2159, -6.7651, -8.6167,\n",
      "         -7.7685, -7.7936, -7.5516, -7.5184, -6.6755, -7.3558, -6.6271, -7.9783,\n",
      "         -7.8924, -8.3706, -6.8957, -6.7270, -7.0149]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([11,  3,  7,  6,  0], device='cuda:0')\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor([4, 2, 2, 5, 3, 3, 5, 2, 3, 4, 3, 3, 3, 3, 3, 3, 4, 3, 3, 5, 2, 4, 5, 3,\n",
      "        3, 4, 3, 5, 3, 3, 4, 4])\n",
      "torch.Size([32, 1024, 1, 7])\n",
      "here\n",
      "tensor([[-3.6784, -3.2693, -4.0913, -3.9745, -3.8975, -3.3772, -3.7948, -3.6467,\n",
      "         -4.0828, -3.2379, -3.3009, -3.2883, -3.5597, -3.9838, -3.5420, -3.2993,\n",
      "         -4.0376, -3.3719, -3.5114, -4.0161, -4.0879, -3.5329, -4.0856, -3.2534,\n",
      "         -3.4841, -3.6467, -3.3144, -3.3798, -4.0131, -3.8164, -4.2092, -3.3701,\n",
      "         -3.4232, -3.2761, -3.7489, -3.6680, -4.0314],\n",
      "        [-3.5167, -3.3119, -4.1488, -4.0160, -3.8872, -3.4015, -3.7878, -3.6324,\n",
      "         -4.1066, -3.2199, -3.3114, -3.2215, -3.5619, -4.0168, -3.5000, -3.2898,\n",
      "         -4.0431, -3.3671, -3.4845, -4.0541, -4.1391, -3.4730, -4.1459, -3.3351,\n",
      "         -3.4721, -3.6311, -3.3017, -3.3908, -4.0783, -3.7459, -4.2688, -3.3509,\n",
      "         -3.4234, -3.3230, -3.7555, -3.6821, -4.0636],\n",
      "        [-2.7148, -3.3271, -4.5605, -4.3612, -4.1638, -3.4004, -4.0360, -3.7201,\n",
      "         -4.4799, -3.0743, -3.3486, -3.0988, -3.6538, -4.3580, -3.5976, -3.1871,\n",
      "         -4.4122, -3.3905, -3.5778, -4.3199, -4.5345, -3.5838, -4.5205, -2.8016,\n",
      "         -3.5270, -3.6644, -3.2110, -3.2877, -4.4622, -3.9542, -4.6121, -3.4256,\n",
      "         -3.5209, -2.8878, -4.0481, -3.8968, -4.3687],\n",
      "        [-1.3516, -3.7013, -5.5194, -5.2161, -5.0031, -3.7104, -4.7433, -4.1422,\n",
      "         -5.3768, -3.1748, -3.7274, -3.3596, -4.1459, -5.1483, -4.1381, -3.3861,\n",
      "         -5.2666, -3.8278, -4.0594, -4.9536, -5.3463, -4.1096, -5.3637, -2.0614,\n",
      "         -4.0106, -4.0580, -3.4316, -3.4811, -5.2793, -4.7041, -5.3586, -3.9717,\n",
      "         -4.0275, -2.4016, -4.7776, -4.4856, -5.1277],\n",
      "        [-0.4015, -4.6700, -6.6012, -6.2757, -6.0873, -4.5737, -5.7362, -4.9972,\n",
      "         -6.4768, -3.9808, -4.6790, -4.2839, -5.0823, -6.1250, -5.1724, -4.2512,\n",
      "         -6.2920, -4.8174, -4.9365, -5.8037, -6.3031, -5.0529, -6.3753, -2.5713,\n",
      "         -5.0366, -5.0070, -4.3405, -4.2854, -6.2345, -5.6998, -6.2769, -5.0636,\n",
      "         -5.1260, -3.0377, -5.7644, -5.3596, -6.1345],\n",
      "        [-0.1084, -5.8264, -7.5276, -7.2559, -7.0738, -5.6535, -6.7049, -6.0042,\n",
      "         -7.4526, -5.1418, -5.8103, -5.4461, -6.0912, -7.0261, -6.3073, -5.3795,\n",
      "         -7.2360, -5.9554, -5.8865, -6.6369, -7.2051, -6.1172, -7.2869, -3.9079,\n",
      "         -6.2069, -6.1567, -5.5128, -5.3648, -7.0910, -6.6834, -7.1351, -6.2751,\n",
      "         -6.4066, -4.3464, -6.7717, -6.2954, -7.0755],\n",
      "        [-0.0692, -6.1743, -7.3710, -7.2005, -7.1608, -5.9960, -6.7231, -6.2651,\n",
      "         -7.3590, -5.6315, -6.1171, -5.9183, -6.2783, -6.9325, -6.6150, -5.8247,\n",
      "         -7.1296, -6.2982, -6.0501, -6.5493, -7.0200, -6.3838, -7.1991, -4.8887,\n",
      "         -6.5423, -6.5547, -5.9325, -5.7905, -6.9583, -6.7533, -7.0272, -6.6069,\n",
      "         -6.7897, -5.1531, -6.8467, -6.3692, -7.0410]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([27, 16,  8, 33,  0], device='cuda:0')\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor([4, 5, 2, 4, 3, 3, 5, 3, 3, 3, 3, 4, 5, 4, 3, 4, 3, 4, 5, 3, 2, 4, 3, 4,\n",
      "        3, 3, 2, 5, 5, 5, 4, 4])\n",
      "torch.Size([32, 1024, 1, 7])\n",
      "here\n",
      "tensor([[-3.3773e+00, -3.3312e+00, -3.9383e+00, -4.0062e+00, -3.7306e+00,\n",
      "         -3.4237e+00, -3.6318e+00, -3.6075e+00, -3.9052e+00, -3.4470e+00,\n",
      "         -3.4419e+00, -3.4613e+00, -3.5225e+00, -3.8838e+00, -3.5656e+00,\n",
      "         -3.4331e+00, -3.7594e+00, -3.5723e+00, -3.4869e+00, -3.7913e+00,\n",
      "         -4.0358e+00, -3.3658e+00, -4.0279e+00, -3.4847e+00, -3.4085e+00,\n",
      "         -3.5759e+00, -3.2707e+00, -3.5095e+00, -3.8166e+00, -3.8205e+00,\n",
      "         -4.0147e+00, -3.5240e+00, -3.3683e+00, -3.6343e+00, -3.6972e+00,\n",
      "         -3.5613e+00, -4.1025e+00],\n",
      "        [-2.1622e+00, -3.9256e+00, -3.4551e+00, -3.7860e+00, -3.4063e+00,\n",
      "         -4.0150e+00, -3.3813e+00, -3.7818e+00, -3.4658e+00, -3.9984e+00,\n",
      "         -3.9199e+00, -3.9179e+00, -3.8627e+00, -3.5128e+00, -3.6092e+00,\n",
      "         -4.0472e+00, -3.2305e+00, -4.0656e+00, -3.6163e+00, -3.3654e+00,\n",
      "         -3.5500e+00, -3.3543e+00, -3.6456e+00, -4.8652e+00, -3.8037e+00,\n",
      "         -3.9205e+00, -3.8003e+00, -4.1721e+00, -3.3477e+00, -3.7692e+00,\n",
      "         -3.4334e+00, -3.9301e+00, -3.7864e+00, -4.8979e+00, -3.5724e+00,\n",
      "         -3.3984e+00, -3.8658e+00],\n",
      "        [-7.1345e-02, -6.9331e+00, -5.7934e+00, -6.3165e+00, -5.9016e+00,\n",
      "         -6.9731e+00, -5.8316e+00, -6.3221e+00, -5.9756e+00, -6.8435e+00,\n",
      "         -6.6353e+00, -6.7776e+00, -6.6805e+00, -5.9072e+00, -6.2572e+00,\n",
      "         -6.9386e+00, -5.7227e+00, -6.8358e+00, -6.2946e+00, -5.4862e+00,\n",
      "         -5.8438e+00, -6.0265e+00, -5.8628e+00, -7.9598e+00, -6.7982e+00,\n",
      "         -6.7290e+00, -6.6537e+00, -6.8737e+00, -5.5842e+00, -6.5767e+00,\n",
      "         -5.6910e+00, -7.0331e+00, -6.7618e+00, -8.0098e+00, -6.1190e+00,\n",
      "         -5.7278e+00, -6.2750e+00],\n",
      "        [-6.6271e-03, -9.2531e+00, -8.1570e+00, -8.7789e+00, -8.3017e+00,\n",
      "         -9.2542e+00, -8.2077e+00, -8.5639e+00, -8.3943e+00, -9.1208e+00,\n",
      "         -8.7819e+00, -9.0754e+00, -8.9729e+00, -8.3208e+00, -8.6355e+00,\n",
      "         -9.2245e+00, -8.1906e+00, -9.1357e+00, -8.5435e+00, -7.7624e+00,\n",
      "         -8.2039e+00, -8.3465e+00, -8.1969e+00, -1.0187e+01, -9.2200e+00,\n",
      "         -9.1664e+00, -9.0217e+00, -9.0596e+00, -7.9375e+00, -9.0219e+00,\n",
      "         -8.0553e+00, -9.4845e+00, -9.1839e+00, -1.0166e+01, -8.4960e+00,\n",
      "         -7.9571e+00, -8.6526e+00],\n",
      "        [-3.4904e-03, -9.8291e+00, -8.8372e+00, -9.4905e+00, -8.9940e+00,\n",
      "         -9.7827e+00, -8.8802e+00, -9.1354e+00, -9.0617e+00, -9.6576e+00,\n",
      "         -9.2919e+00, -9.6235e+00, -9.5340e+00, -9.0142e+00, -9.2622e+00,\n",
      "         -9.7498e+00, -8.9304e+00, -9.7243e+00, -9.0805e+00, -8.4432e+00,\n",
      "         -8.8919e+00, -8.9730e+00, -8.9142e+00, -1.0665e+01, -9.8114e+00,\n",
      "         -9.8528e+00, -9.6105e+00, -9.5775e+00, -8.6343e+00, -9.6257e+00,\n",
      "         -8.7454e+00, -1.0083e+01, -9.8064e+00, -1.0577e+01, -9.1597e+00,\n",
      "         -8.5706e+00, -9.3263e+00],\n",
      "        [-3.5067e-03, -9.7380e+00, -8.9031e+00, -9.5162e+00, -9.0742e+00,\n",
      "         -9.6417e+00, -8.9195e+00, -9.0841e+00, -9.0876e+00, -9.5335e+00,\n",
      "         -9.1924e+00, -9.5074e+00, -9.4475e+00, -9.0499e+00, -9.2721e+00,\n",
      "         -9.6075e+00, -9.0271e+00, -9.6409e+00, -9.0051e+00, -8.5212e+00,\n",
      "         -8.9435e+00, -8.9957e+00, -8.9865e+00, -1.0417e+01, -9.7312e+00,\n",
      "         -9.8498e+00, -9.5223e+00, -9.4312e+00, -8.7036e+00, -9.5343e+00,\n",
      "         -8.8045e+00, -9.9931e+00, -9.7624e+00, -1.0281e+01, -9.1708e+00,\n",
      "         -8.5878e+00, -9.3446e+00],\n",
      "        [-8.9912e-03, -8.6558e+00, -8.0839e+00, -8.5284e+00, -8.3668e+00,\n",
      "         -8.5459e+00, -8.0810e+00, -8.1217e+00, -8.1585e+00, -8.4148e+00,\n",
      "         -8.1592e+00, -8.4363e+00, -8.4407e+00, -8.1090e+00, -8.4359e+00,\n",
      "         -8.4883e+00, -8.2049e+00, -8.5756e+00, -8.0372e+00, -7.6836e+00,\n",
      "         -8.0271e+00, -8.1828e+00, -8.1522e+00, -9.0970e+00, -8.7086e+00,\n",
      "         -8.8985e+00, -8.4401e+00, -8.3158e+00, -7.8992e+00, -8.4616e+00,\n",
      "         -7.9676e+00, -8.9211e+00, -8.7966e+00, -8.8975e+00, -8.2136e+00,\n",
      "         -7.7438e+00, -8.4288e+00]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([ 9, 34,  0,  0,  0], device='cuda:0')\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor([2, 4, 3, 5, 5, 4, 4, 3, 4, 5, 4, 5, 5, 3, 3, 3, 3, 5, 2, 5, 5, 2, 2, 3,\n",
      "        3, 5, 5, 2, 3, 4, 2, 3])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1024, 1, 7])\n",
      "here\n",
      "tensor([[-3.8432e+00, -3.3922e+00, -3.8004e+00, -3.7625e+00, -3.6489e+00,\n",
      "         -3.5113e+00, -3.5585e+00, -3.5742e+00, -3.7327e+00, -3.5331e+00,\n",
      "         -3.3778e+00, -3.4944e+00, -3.5094e+00, -3.7799e+00, -3.4665e+00,\n",
      "         -3.4523e+00, -3.6308e+00, -3.5653e+00, -3.5028e+00, -3.7431e+00,\n",
      "         -3.7819e+00, -3.3465e+00, -3.7719e+00, -4.0832e+00, -3.4710e+00,\n",
      "         -3.7052e+00, -3.4768e+00, -3.6335e+00, -3.7248e+00, -3.6295e+00,\n",
      "         -3.9019e+00, -3.4430e+00, -3.4484e+00, -4.0282e+00, -3.4854e+00,\n",
      "         -3.4680e+00, -3.9116e+00],\n",
      "        [-3.5121e+00, -3.2548e+00, -4.0242e+00, -3.9658e+00, -3.7433e+00,\n",
      "         -3.4205e+00, -3.5852e+00, -3.6058e+00, -3.9453e+00, -3.4528e+00,\n",
      "         -3.3218e+00, -3.3729e+00, -3.4234e+00, -3.9720e+00, -3.3956e+00,\n",
      "         -3.3553e+00, -3.7791e+00, -3.5189e+00, -3.3883e+00, -3.8481e+00,\n",
      "         -4.0332e+00, -3.3068e+00, -3.9733e+00, -4.0488e+00, -3.3680e+00,\n",
      "         -3.6839e+00, -3.3509e+00, -3.5482e+00, -3.9049e+00, -3.6757e+00,\n",
      "         -4.1420e+00, -3.4238e+00, -3.3898e+00, -3.9840e+00, -3.5889e+00,\n",
      "         -3.4845e+00, -4.1302e+00],\n",
      "        [-1.9334e+00, -3.3063e+00, -4.3469e+00, -4.3184e+00, -3.9917e+00,\n",
      "         -3.4751e+00, -3.6973e+00, -3.7432e+00, -4.2531e+00, -3.5104e+00,\n",
      "         -3.4871e+00, -3.4295e+00, -3.4054e+00, -4.2587e+00, -3.4636e+00,\n",
      "         -3.4076e+00, -4.0563e+00, -3.6762e+00, -3.4092e+00, -3.9506e+00,\n",
      "         -4.3726e+00, -3.3937e+00, -4.2487e+00, -4.0817e+00, -3.4046e+00,\n",
      "         -3.8353e+00, -3.3370e+00, -3.5457e+00, -4.1175e+00, -3.8732e+00,\n",
      "         -4.3679e+00, -3.6570e+00, -3.5135e+00, -4.0207e+00, -3.8493e+00,\n",
      "         -3.5574e+00, -4.5062e+00],\n",
      "        [-5.4727e-02, -6.3735e+00, -6.9219e+00, -7.0691e+00, -6.6491e+00,\n",
      "         -6.3825e+00, -6.2591e+00, -6.3888e+00, -6.9508e+00, -6.3794e+00,\n",
      "         -6.3703e+00, -6.3919e+00, -6.2367e+00, -6.8166e+00, -6.2035e+00,\n",
      "         -6.4271e+00, -6.7052e+00, -6.6688e+00, -6.0986e+00, -6.1913e+00,\n",
      "         -6.9245e+00, -6.0469e+00, -6.7482e+00, -6.9403e+00, -6.4426e+00,\n",
      "         -6.8316e+00, -6.3690e+00, -6.3387e+00, -6.4204e+00, -6.7757e+00,\n",
      "         -6.6244e+00, -6.8952e+00, -6.5406e+00, -7.0046e+00, -6.6589e+00,\n",
      "         -6.0511e+00, -7.0422e+00],\n",
      "        [-6.2418e-03, -8.8062e+00, -8.7373e+00, -9.0989e+00, -8.6572e+00,\n",
      "         -8.7394e+00, -8.2872e+00, -8.5166e+00, -8.9318e+00, -8.7069e+00,\n",
      "         -8.5252e+00, -8.7761e+00, -8.5942e+00, -8.7204e+00, -8.5044e+00,\n",
      "         -8.8126e+00, -8.6169e+00, -8.9399e+00, -8.2836e+00, -7.9906e+00,\n",
      "         -8.7505e+00, -8.2773e+00, -8.6689e+00, -9.3885e+00, -8.8471e+00,\n",
      "         -9.1643e+00, -8.8055e+00, -8.6169e+00, -8.2925e+00, -8.9853e+00,\n",
      "         -8.4668e+00, -9.2979e+00, -8.9293e+00, -9.3920e+00, -8.7231e+00,\n",
      "         -8.0434e+00, -8.9015e+00],\n",
      "        [-3.9139e-03, -9.4415e+00, -9.0044e+00, -9.4807e+00, -9.0744e+00,\n",
      "         -9.3575e+00, -8.7356e+00, -8.9723e+00, -9.2306e+00, -9.2706e+00,\n",
      "         -9.0192e+00, -9.3444e+00, -9.2092e+00, -9.0495e+00, -9.0871e+00,\n",
      "         -9.3904e+00, -8.9632e+00, -9.4677e+00, -8.8256e+00, -8.3650e+00,\n",
      "         -9.0126e+00, -8.8637e+00, -9.0318e+00, -1.0058e+01, -9.4334e+00,\n",
      "         -9.7118e+00, -9.3597e+00, -9.1855e+00, -8.6775e+00, -9.4536e+00,\n",
      "         -8.8184e+00, -9.8074e+00, -9.5494e+00, -9.9269e+00, -9.1005e+00,\n",
      "         -8.4923e+00, -9.2525e+00],\n",
      "        [-8.5664e-03, -8.6522e+00, -8.2128e+00, -8.6005e+00, -8.4585e+00,\n",
      "         -8.5621e+00, -8.0458e+00, -8.1738e+00, -8.3301e+00, -8.4235e+00,\n",
      "         -8.2038e+00, -8.5194e+00, -8.4493e+00, -8.1922e+00, -8.4332e+00,\n",
      "         -8.5361e+00, -8.2174e+00, -8.6339e+00, -8.0903e+00, -7.6539e+00,\n",
      "         -8.1325e+00, -8.2390e+00, -8.2680e+00, -9.1220e+00, -8.6614e+00,\n",
      "         -8.9505e+00, -8.4980e+00, -8.3427e+00, -7.9709e+00, -8.5655e+00,\n",
      "         -8.0443e+00, -8.9460e+00, -8.8293e+00, -8.8897e+00, -8.2547e+00,\n",
      "         -7.7893e+00, -8.4559e+00]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([ 4, 14, 25,  0,  0], device='cuda:0')\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor([3, 3, 4, 5, 4, 2, 4, 3, 3, 5, 3, 3, 5, 4, 4, 5, 2, 5, 5, 3, 3, 4, 4, 2,\n",
      "        2, 4, 4, 4, 3, 4, 5, 3])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-981c3ff7f921>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredict_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mcap2txt_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/final/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/final/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"TRAINING\"\"\"\n",
    "\n",
    "SAVE_PATH = './ckpt/'\n",
    "display_interval = 1\n",
    "save_interval=3\n",
    "max_epoch = 1\n",
    "\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    train_it = iter(my_train_loader)\n",
    "    it=0\n",
    "    while it<len(my_train_loader):\n",
    "        for param in net.parameters():\n",
    "            param.requires_grad=True\n",
    "        net.train()\n",
    "        chunk = train_it.next()\n",
    "        img, label = chunk\n",
    "        utils.loadData(image,img)\n",
    "        #print(image)\n",
    "        txt, lth = converter.encode(label)\n",
    "        utils.loadData(text,txt)\n",
    "        #print(text)\n",
    "        utils.loadData(length,lth)\n",
    "        #print(length)\n",
    "        cap2txt_optim.zero_grad()\n",
    "        predict = net(image)\n",
    "        predict_size = Variable(torch.LongTensor([predict.size(0)] * img.size(0)))\n",
    "        print('here')\n",
    "        print(predict[:,0,:])\n",
    "        print(text[0,:])\n",
    "        print(predict_size)\n",
    "        print(length)\n",
    "        loss = loss_func(predict,text,predict_size,length)/img.size(0)\n",
    "        loss.backward()\n",
    "        cap2txt_optim.step()\n",
    "        \n",
    "        loss_avg.add(loss)\n",
    "        it+=1\n",
    "        \n",
    "    if epoch%display_interval==0:\n",
    "        print('epoch : %d, Loss : %f' %(epoch,loss_avg.val()))\n",
    "        loss_avg.reset()\n",
    "    if epoch%save_interval==0:\n",
    "        print('saved')\n",
    "        #torch.save(net.state_dict(),SAVE_PATH=\"Cap2TxT_\"+str(epoch)+'pth')\n",
    "        t#orch.save(net.state_dict(),SAVE_PATH+'Cap2TxT_GoogLe_'+str(epoch)+'.pth')\n",
    "\n",
    "print('Finished Training')\n",
    "#torch.save(net.state_dict(),SAVE_PATH+'Cap2TxT'+'_final.pth')\n",
    "print('Saved Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e_uKOpe8IGJk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---char correct---\n",
      "77.55714285714286 %\n",
      "---word correct---\n",
      "62.6 %\n"
     ]
    }
   ],
   "source": [
    "'''Test'''\n",
    "\n",
    "def get_char_count(arg1):\n",
    "    c0 = ALL_CHAR_SET[np.argmax(arg1[0:ALL_CHAR_SET_LEN])]\n",
    "    c1 = ALL_CHAR_SET[np.argmax(arg1[ALL_CHAR_SET_LEN:ALL_CHAR_SET_LEN*2])]\n",
    "    c2 = ALL_CHAR_SET[np.argmax(arg1[ALL_CHAR_SET_LEN*2:ALL_CHAR_SET_LEN*3])]\n",
    "    c3 = ALL_CHAR_SET[np.argmax(arg1[ALL_CHAR_SET_LEN*3:ALL_CHAR_SET_LEN*4])]\n",
    "    c4 = ALL_CHAR_SET[np.argmax(arg1[ALL_CHAR_SET_LEN*4:ALL_CHAR_SET_LEN*5])]\n",
    "    c5 = ALL_CHAR_SET[np.argmax(arg1[ALL_CHAR_SET_LEN*5:ALL_CHAR_SET_LEN*6])]\n",
    "    c6 = ALL_CHAR_SET[np.argmax(arg1[ALL_CHAR_SET_LEN*6:ALL_CHAR_SET_LEN*7])]\n",
    "    return c0, c1, c2,c3, c4, c5, c6 \n",
    "\n",
    "def oh_encoding(a):\n",
    "    label_oh = []\n",
    "    for i in range(7):\n",
    "        if i<len(a):\n",
    "            label_oh+=encode(a[i])\n",
    "        else:\n",
    "            label_oh+=encode('NONE')\n",
    "    return label_oh\n",
    "\n",
    "\n",
    "\n",
    "#model_path = '../final/ckpt/netCRNN_80_266.pth'\n",
    "\n",
    "#best at Cap2TxT\n",
    "model_path ='./ckpt/Cap2TxT_180.pth'\n",
    "\n",
    "#best at Cap2TxT_GoogLeNet\n",
    "\n",
    "\n",
    "image_path = './Data/test/'\n",
    "\n",
    "pred_list =[]\n",
    "\n",
    "class_num = len(alphabet) + 1\n",
    "\n",
    "model = Cap2TxT(hidden_size,class_num)\n",
    "#model = Cap2TxT_2(64,1,class_num,hidden_size)\n",
    "\n",
    "model = model.cuda()\n",
    "\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "converter = utils.strLabelConverter(alphabet)\n",
    "transformer=dataset.resizeNormalize((160,64))\n",
    "for i in range(1000):\n",
    "    image = Image.open(image_path+str(i)+'.png').convert('L')\n",
    "    image = transformer(image)\n",
    "    if torch.cuda.is_available():\n",
    "        image = image.cuda()\n",
    "    image = image.view(1, *image.size())\n",
    "    image = Variable(image)\n",
    "    preds = model(image)\n",
    "\n",
    "    _, preds = preds.max(2)\n",
    "    preds = preds.transpose(1, 0).contiguous().view(-1)\n",
    "    preds_size = Variable(torch.LongTensor([preds.size(0)]))\n",
    "    pred = converter.decode(preds.data, preds_size.data, raw=False)\n",
    "    pred_list.append(pred)\n",
    "\n",
    "answer='./Data/test.txt'\n",
    "answer_list=list()\n",
    "with open (answer) as f:\n",
    "    for line in f:\n",
    "        answer_list.append(line.rstrip('\\n'))\n",
    "\n",
    "char_correct = 0\n",
    "word_correct = 0\n",
    "total = 0\n",
    "\n",
    "for i in range(1000): # size of test set is 1000\n",
    "    char_count=0\n",
    "    c0,c1,c2,c3,c4,c5,c6 = get_char_count(oh_encoding(pred_list[i]))\n",
    "    d0,d1,d2,d3,d4,d5,d6 = get_char_count(oh_encoding(answer_list[i]))\n",
    "    c = '%s%s%s%s%s%s%s' % (c0, c1, c2, c3, c4, c5, c6)\n",
    "    d = '%s%s%s%s%s%s%s' % (d0, d1, d2, d3, d4, d5, d6)\n",
    "    char_count += (c0==d0)+(c1==d1)+(c2==d2)+(c3==d3)+(c4==d4)+(c5==d5)+(c6==d6)\n",
    "    char_correct += char_count\n",
    "    if(bool(str(answer_list[i]) in str(c))):\n",
    "        word_correct+=1\n",
    "    total += 1\n",
    "\n",
    "print('---char correct---')\n",
    "print(100*(char_correct/(total*7)), end=' ')\n",
    "print('%')\n",
    "print('---word correct---')\n",
    "print(100*word_correct/total, end=' ')\n",
    "print('%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "current...\n",
      "0:36.72857142857143\n",
      "best...\n",
      "0:36.72857142857143\n",
      "------------------------------------\n",
      "current...\n",
      "3:38.2\n",
      "best...\n",
      "3:38.2\n",
      "------------------------------------\n",
      "current...\n",
      "6:36.32857142857143\n",
      "best...\n",
      "3:38.2\n",
      "------------------------------------\n",
      "current...\n",
      "9:101.45714285714286\n",
      "best...\n",
      "9:101.45714285714286\n",
      "------------------------------------\n",
      "current...\n",
      "12:111.01428571428572\n",
      "best...\n",
      "12:111.01428571428572\n",
      "------------------------------------\n",
      "current...\n",
      "15:121.2\n",
      "best...\n",
      "15:121.2\n",
      "------------------------------------\n",
      "current...\n",
      "18:86.57142857142857\n",
      "best...\n",
      "15:121.2\n",
      "------------------------------------\n",
      "current...\n",
      "21:147.22857142857143\n",
      "best...\n",
      "21:147.22857142857143\n",
      "------------------------------------\n",
      "current...\n",
      "24:148.51428571428573\n",
      "best...\n",
      "24:148.51428571428573\n",
      "------------------------------------\n",
      "current...\n",
      "27:138.38571428571427\n",
      "best...\n",
      "24:148.51428571428573\n",
      "------------------------------------\n",
      "current...\n",
      "30:49.800000000000004\n",
      "best...\n",
      "24:148.51428571428573\n",
      "------------------------------------\n",
      "current...\n",
      "33:145.94285714285715\n",
      "best...\n",
      "24:148.51428571428573\n",
      "------------------------------------\n",
      "current...\n",
      "36:144.04285714285714\n",
      "best...\n",
      "24:148.51428571428573\n",
      "------------------------------------\n",
      "current...\n",
      "39:148.78571428571428\n",
      "best...\n",
      "39:148.78571428571428\n",
      "------------------------------------\n",
      "current...\n",
      "42:138.25714285714287\n",
      "best...\n",
      "39:148.78571428571428\n",
      "------------------------------------\n",
      "current...\n",
      "45:146.42857142857144\n",
      "best...\n",
      "39:148.78571428571428\n",
      "------------------------------------\n",
      "current...\n",
      "48:140.45714285714286\n",
      "best...\n",
      "39:148.78571428571428\n",
      "------------------------------------\n",
      "current...\n",
      "51:156.71428571428572\n",
      "best...\n",
      "51:156.71428571428572\n",
      "------------------------------------\n",
      "current...\n",
      "54:146.22857142857143\n",
      "best...\n",
      "51:156.71428571428572\n",
      "------------------------------------\n",
      "current...\n",
      "57:147.77142857142857\n",
      "best...\n",
      "51:156.71428571428572\n",
      "------------------------------------\n",
      "current...\n",
      "60:155.54285714285714\n",
      "best...\n",
      "51:156.71428571428572\n",
      "------------------------------------\n",
      "current...\n",
      "63:152.39999999999998\n",
      "best...\n",
      "51:156.71428571428572\n",
      "------------------------------------\n",
      "current...\n",
      "66:159.8857142857143\n",
      "best...\n",
      "66:159.8857142857143\n",
      "------------------------------------\n",
      "current...\n",
      "69:159.5142857142857\n",
      "best...\n",
      "66:159.8857142857143\n",
      "------------------------------------\n",
      "current...\n",
      "72:157.8857142857143\n",
      "best...\n",
      "66:159.8857142857143\n",
      "------------------------------------\n",
      "current...\n",
      "75:119.07142857142857\n",
      "best...\n",
      "66:159.8857142857143\n",
      "------------------------------------\n",
      "current...\n",
      "78:156.6857142857143\n",
      "best...\n",
      "66:159.8857142857143\n",
      "------------------------------------\n",
      "current...\n",
      "81:159.0857142857143\n",
      "best...\n",
      "66:159.8857142857143\n",
      "------------------------------------\n",
      "current...\n",
      "84:161.24285714285713\n",
      "best...\n",
      "84:161.24285714285713\n",
      "------------------------------------\n",
      "current...\n",
      "87:159.15714285714284\n",
      "best...\n",
      "84:161.24285714285713\n",
      "------------------------------------\n",
      "current...\n",
      "90:158.95714285714286\n",
      "best...\n",
      "84:161.24285714285713\n",
      "------------------------------------\n",
      "current...\n",
      "93:152.27142857142857\n",
      "best...\n",
      "84:161.24285714285713\n",
      "------------------------------------\n",
      "current...\n",
      "96:158.82857142857142\n",
      "best...\n",
      "84:161.24285714285713\n",
      "------------------------------------\n",
      "current...\n",
      "99:157.42857142857144\n",
      "best...\n",
      "84:161.24285714285713\n",
      "------------------------------------\n",
      "current...\n",
      "102:160.4\n",
      "best...\n",
      "84:161.24285714285713\n",
      "------------------------------------\n",
      "current...\n",
      "105:158.48571428571427\n",
      "best...\n",
      "84:161.24285714285713\n",
      "------------------------------------\n",
      "current...\n",
      "108:136.88571428571427\n",
      "best...\n",
      "84:161.24285714285713\n",
      "------------------------------------\n",
      "current...\n",
      "111:155.54285714285714\n",
      "best...\n",
      "84:161.24285714285713\n",
      "------------------------------------\n",
      "current...\n",
      "114:101.55714285714285\n",
      "best...\n",
      "84:161.24285714285713\n",
      "------------------------------------\n",
      "current...\n",
      "117:157.27142857142857\n",
      "best...\n",
      "84:161.24285714285713\n",
      "------------------------------------\n",
      "current...\n",
      "120:161.34285714285716\n",
      "best...\n",
      "120:161.34285714285716\n",
      "------------------------------------\n",
      "current...\n",
      "123:153.42857142857144\n",
      "best...\n",
      "120:161.34285714285716\n",
      "------------------------------------\n",
      "current...\n",
      "126:158.4714285714286\n",
      "best...\n",
      "120:161.34285714285716\n",
      "------------------------------------\n",
      "current...\n",
      "129:158.28571428571428\n",
      "best...\n",
      "120:161.34285714285716\n",
      "------------------------------------\n",
      "current...\n",
      "132:158.89999999999998\n",
      "best...\n",
      "120:161.34285714285716\n",
      "------------------------------------\n",
      "current...\n",
      "135:160.21428571428572\n",
      "best...\n",
      "120:161.34285714285716\n",
      "------------------------------------\n",
      "current...\n",
      "138:156.52857142857144\n",
      "best...\n",
      "120:161.34285714285716\n",
      "------------------------------------\n",
      "current...\n",
      "141:159.64285714285714\n",
      "best...\n",
      "120:161.34285714285716\n",
      "------------------------------------\n",
      "current...\n",
      "144:159.2\n",
      "best...\n",
      "120:161.34285714285716\n",
      "------------------------------------\n",
      "current...\n",
      "147:161.82857142857142\n",
      "best...\n",
      "147:161.82857142857142\n",
      "------------------------------------\n",
      "current...\n",
      "150:162.75714285714287\n",
      "best...\n",
      "150:162.75714285714287\n",
      "------------------------------------\n",
      "current...\n",
      "153:162.8142857142857\n",
      "best...\n",
      "153:162.8142857142857\n",
      "------------------------------------\n",
      "current...\n",
      "156:160.8\n",
      "best...\n",
      "153:162.8142857142857\n",
      "------------------------------------\n",
      "current...\n",
      "159:155.37142857142857\n",
      "best...\n",
      "153:162.8142857142857\n",
      "------------------------------------\n",
      "current...\n",
      "162:158.12857142857143\n",
      "best...\n",
      "153:162.8142857142857\n",
      "------------------------------------\n",
      "current...\n",
      "165:162.14285714285717\n",
      "best...\n",
      "153:162.8142857142857\n",
      "------------------------------------\n",
      "current...\n",
      "168:162.35714285714283\n",
      "best...\n",
      "153:162.8142857142857\n",
      "------------------------------------\n",
      "current...\n",
      "171:161.60000000000002\n",
      "best...\n",
      "153:162.8142857142857\n",
      "------------------------------------\n",
      "current...\n",
      "174:158.4142857142857\n",
      "best...\n",
      "153:162.8142857142857\n",
      "------------------------------------\n",
      "current...\n",
      "177:161.84285714285716\n",
      "best...\n",
      "153:162.8142857142857\n",
      "------------------------------------\n",
      "current...\n",
      "180:162.17142857142858\n",
      "best...\n",
      "153:162.8142857142857\n",
      "------------------------------------\n",
      "current...\n",
      "183:160.98571428571427\n",
      "best...\n",
      "153:162.8142857142857\n",
      "------------------------------------\n",
      "current...\n",
      "186:163.25714285714287\n",
      "best...\n",
      "186:163.25714285714287\n",
      "------------------------------------\n",
      "current...\n",
      "189:158.55714285714285\n",
      "best...\n",
      "186:163.25714285714287\n",
      "------------------------------------\n",
      "current...\n",
      "192:161.14285714285714\n",
      "best...\n",
      "186:163.25714285714287\n",
      "------------------------------------\n",
      "current...\n",
      "195:161.28571428571428\n",
      "best...\n",
      "186:163.25714285714287\n",
      "------------------------------------\n",
      "current...\n",
      "198:150.64285714285714\n",
      "best...\n",
      "186:163.25714285714287\n",
      "------------------------------------\n",
      "current...\n",
      "201:162.87142857142857\n",
      "best...\n",
      "186:163.25714285714287\n",
      "------------------------------------\n",
      "current...\n",
      "204:160.15714285714284\n",
      "best...\n",
      "186:163.25714285714287\n",
      "------------------------------------\n",
      "current...\n",
      "207:162.4\n",
      "best...\n",
      "186:163.25714285714287\n",
      "------------------------------------\n",
      "current...\n",
      "210:161.8\n",
      "best...\n",
      "186:163.25714285714287\n",
      "------------------------------------\n",
      "current...\n",
      "213:162.4714285714286\n",
      "best...\n",
      "186:163.25714285714287\n",
      "------------------------------------\n",
      "current...\n",
      "216:162.89999999999998\n",
      "best...\n",
      "186:163.25714285714287\n",
      "------------------------------------\n",
      "current...\n",
      "219:161.07142857142858\n",
      "best...\n",
      "186:163.25714285714287\n",
      "------------------------------------\n",
      "current...\n",
      "222:157.92857142857144\n",
      "best...\n",
      "186:163.25714285714287\n",
      "------------------------------------\n",
      "current...\n",
      "225:160.9142857142857\n",
      "best...\n",
      "186:163.25714285714287\n",
      "------------------------------------\n",
      "current...\n",
      "228:160.0\n",
      "best...\n",
      "186:163.25714285714287\n",
      "------------------------------------\n",
      "current...\n",
      "231:166.10000000000002\n",
      "best...\n",
      "231:166.10000000000002\n",
      "------------------------------------\n",
      "current...\n",
      "234:164.4142857142857\n",
      "best...\n",
      "231:166.10000000000002\n",
      "------------------------------------\n",
      "current...\n",
      "237:161.0\n",
      "best...\n",
      "231:166.10000000000002\n",
      "------------------------------------\n",
      "current...\n",
      "240:162.94285714285715\n",
      "best...\n",
      "231:166.10000000000002\n",
      "------------------------------------\n",
      "current...\n",
      "243:163.44285714285715\n",
      "best...\n",
      "231:166.10000000000002\n",
      "------------------------------------\n",
      "current...\n",
      "246:162.1142857142857\n",
      "best...\n",
      "231:166.10000000000002\n",
      "------------------------------------\n",
      "current...\n",
      "249:163.85714285714286\n",
      "best...\n",
      "231:166.10000000000002\n"
     ]
    }
   ],
   "source": [
    "'''Find the best weight from my experiment results.'''\n",
    "\n",
    "def get_char_count(arg1):\n",
    "    c0 = ALL_CHAR_SET[np.argmax(arg1[0:ALL_CHAR_SET_LEN])]\n",
    "    c1 = ALL_CHAR_SET[np.argmax(arg1[ALL_CHAR_SET_LEN:ALL_CHAR_SET_LEN*2])]\n",
    "    c2 = ALL_CHAR_SET[np.argmax(arg1[ALL_CHAR_SET_LEN*2:ALL_CHAR_SET_LEN*3])]\n",
    "    c3 = ALL_CHAR_SET[np.argmax(arg1[ALL_CHAR_SET_LEN*3:ALL_CHAR_SET_LEN*4])]\n",
    "    c4 = ALL_CHAR_SET[np.argmax(arg1[ALL_CHAR_SET_LEN*4:ALL_CHAR_SET_LEN*5])]\n",
    "    c5 = ALL_CHAR_SET[np.argmax(arg1[ALL_CHAR_SET_LEN*5:ALL_CHAR_SET_LEN*6])]\n",
    "    c6 = ALL_CHAR_SET[np.argmax(arg1[ALL_CHAR_SET_LEN*6:ALL_CHAR_SET_LEN*7])]\n",
    "    return c0, c1, c2,c3, c4, c5, c6 \n",
    "\n",
    "def oh_encoding(a):\n",
    "    label_oh = []\n",
    "    for i in range(7):\n",
    "        if i<len(a):\n",
    "            label_oh+=encode(a[i])\n",
    "        else:\n",
    "            label_oh+=encode('NONE')\n",
    "    return label_oh\n",
    "\n",
    "\n",
    "\n",
    "#model_path = '../final/ckpt/netCRNN_80_266.pth'\n",
    "\n",
    "#best at Cap2TxT\n",
    "#model_path ='./ckpt/Cap2TxT_180.pth'\n",
    "\n",
    "#best at Cap2TxT_GoogLeNet\n",
    "#model_path = './ckpt/Cap2TxT_final.pth'\n",
    "#model_path = './ckpt/Cap2TxT_GoogLe_150.pth'\n",
    "image_path = './Data/test/'\n",
    "class_num = len(alphabet) + 1\n",
    "converter = utils.strLabelConverter(alphabet)\n",
    "transformer=dataset.resizeNormalize((160,64))\n",
    "\n",
    "answer='./Data/test.txt'\n",
    "answer_list=list()\n",
    "with open (answer) as f:\n",
    "    for line in f:\n",
    "        answer_list.append(line.rstrip('\\n'))\n",
    "\n",
    "#Let's find best fitted model among 0~250 epoch that performs best at test set!\n",
    "\n",
    "max_accuracy = 0\n",
    "best_epoch = 0\n",
    "epoch=0\n",
    "while epoch <250 :\n",
    "    model_path = './ckpt/Cap2TxT_GoogLe_'+str(epoch)+'.pth'\n",
    "    pred_list =[]\n",
    "    model = Cap2TxT(hidden_size,class_num)\n",
    "    #model = Cap2TxT_2(64,1,class_num,hidden_size)\n",
    "    model = model.cuda()\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    \n",
    "    for i in range(1000):\n",
    "        image = Image.open(image_path+str(i)+'.png').convert('L')\n",
    "        image = transformer(image)\n",
    "        if torch.cuda.is_available():\n",
    "            image = image.cuda()\n",
    "        image = image.view(1, *image.size())\n",
    "        image = Variable(image)\n",
    "        preds = model(image)\n",
    "\n",
    "        _, preds = preds.max(2)\n",
    "        preds = preds.transpose(1, 0).contiguous().view(-1)\n",
    "        preds_size = Variable(torch.LongTensor([preds.size(0)]))\n",
    "        pred = converter.decode(preds.data, preds_size.data, raw=False)\n",
    "        pred_list.append(pred)\n",
    "\n",
    "    char_correct = 0\n",
    "    word_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i in range(1000): # size of test set is 1000\n",
    "        char_count=0\n",
    "        c0,c1,c2,c3,c4,c5,c6 = get_char_count(oh_encoding(pred_list[i]))\n",
    "        d0,d1,d2,d3,d4,d5,d6 = get_char_count(oh_encoding(answer_list[i]))\n",
    "        c = '%s%s%s%s%s%s%s' % (c0, c1, c2, c3, c4, c5, c6)\n",
    "        d = '%s%s%s%s%s%s%s' % (d0, d1, d2, d3, d4, d5, d6)\n",
    "        char_count += (c0==d0)+(c1==d1)+(c2==d2)+(c3==d3)+(c4==d4)+(c5==d5)+(c6==d6)\n",
    "        char_correct += char_count\n",
    "        if(bool(str(answer_list[i]) in str(c))):\n",
    "            word_correct+=1\n",
    "        total += 1\n",
    "    if max_accuracy < (100*(char_correct/(total*7))+100*word_correct/total):\n",
    "        max_accuracy = (100*(char_correct/(total*7))+100*word_correct/total)\n",
    "        best_epoch = epoch\n",
    "    print(\"------------------------------------\")\n",
    "    print(\"current...\")\n",
    "    print(epoch, end=':')\n",
    "    print(100*(char_correct/(total*7))+100*word_correct/total)\n",
    "    print(\"best...\")\n",
    "    print(best_epoch, end=\":\")\n",
    "    print(max_accuracy)\n",
    "    epoch += 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231\n"
     ]
    }
   ],
   "source": [
    "best_epoch=231\n",
    "print(best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---char correct---\n",
      "88.2 %\n",
      "---word correct---\n",
      "77.9 %\n"
     ]
    }
   ],
   "source": [
    "'''Test'''\n",
    "\n",
    "def get_char_count(arg1):\n",
    "    c0 = ALL_CHAR_SET[np.argmax(arg1[0:ALL_CHAR_SET_LEN])]\n",
    "    c1 = ALL_CHAR_SET[np.argmax(arg1[ALL_CHAR_SET_LEN:ALL_CHAR_SET_LEN*2])]\n",
    "    c2 = ALL_CHAR_SET[np.argmax(arg1[ALL_CHAR_SET_LEN*2:ALL_CHAR_SET_LEN*3])]\n",
    "    c3 = ALL_CHAR_SET[np.argmax(arg1[ALL_CHAR_SET_LEN*3:ALL_CHAR_SET_LEN*4])]\n",
    "    c4 = ALL_CHAR_SET[np.argmax(arg1[ALL_CHAR_SET_LEN*4:ALL_CHAR_SET_LEN*5])]\n",
    "    c5 = ALL_CHAR_SET[np.argmax(arg1[ALL_CHAR_SET_LEN*5:ALL_CHAR_SET_LEN*6])]\n",
    "    c6 = ALL_CHAR_SET[np.argmax(arg1[ALL_CHAR_SET_LEN*6:ALL_CHAR_SET_LEN*7])]\n",
    "    return c0, c1, c2,c3, c4, c5, c6 \n",
    "\n",
    "def oh_encoding(a):\n",
    "    label_oh = []\n",
    "    for i in range(7):\n",
    "        if i<len(a):\n",
    "            label_oh+=encode(a[i])\n",
    "        else:\n",
    "            label_oh+=encode('NONE')\n",
    "    return label_oh\n",
    "\n",
    "#best weight of Cap2TxT Network.\n",
    "model_path ='./ckpt/Cap2TxT_GoogLe_'+str(best_epoch)+'.pth'\n",
    "image_path = './Data/test/'\n",
    "\n",
    "pred_list =[]\n",
    "\n",
    "class_num = len(alphabet) + 1\n",
    "\n",
    "model = Cap2TxT(hidden_size,class_num)\n",
    "model = model.cuda()\n",
    "\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "converter = utils.strLabelConverter(alphabet)\n",
    "transformer=dataset.resizeNormalize((160,64))\n",
    "for i in range(1000):\n",
    "    image = Image.open(image_path+str(i)+'.png').convert('L')\n",
    "    image = transformer(image)\n",
    "    if torch.cuda.is_available():\n",
    "        image = image.cuda()\n",
    "    image = image.view(1, *image.size())\n",
    "    image = Variable(image)\n",
    "    preds = model(image)\n",
    "\n",
    "    _, preds = preds.max(2)\n",
    "    preds = preds.transpose(1, 0).contiguous().view(-1)\n",
    "    preds_size = Variable(torch.LongTensor([preds.size(0)]))\n",
    "    pred = converter.decode(preds.data, preds_size.data, raw=False)\n",
    "    pred_list.append(pred)\n",
    "\n",
    "answer='./Data/test.txt'\n",
    "answer_list=list()\n",
    "with open (answer) as f:\n",
    "    for line in f:\n",
    "        answer_list.append(line.rstrip('\\n'))\n",
    "\n",
    "char_correct = 0\n",
    "word_correct = 0\n",
    "total = 0\n",
    "\n",
    "for i in range(1000): # size of test set is 1000\n",
    "    char_count=0\n",
    "    c0,c1,c2,c3,c4,c5,c6 = get_char_count(oh_encoding(pred_list[i]))\n",
    "    d0,d1,d2,d3,d4,d5,d6 = get_char_count(oh_encoding(answer_list[i]))\n",
    "    c = '%s%s%s%s%s%s%s' % (c0, c1, c2, c3, c4, c5, c6)\n",
    "    d = '%s%s%s%s%s%s%s' % (d0, d1, d2, d3, d4, d5, d6)\n",
    "    char_count += (c0==d0)+(c1==d1)+(c2==d2)+(c3==d3)+(c4==d4)+(c5==d5)+(c6==d6)\n",
    "    char_correct += char_count\n",
    "    if(bool(str(answer_list[i]) in str(c))):\n",
    "        word_correct+=1\n",
    "    total += 1\n",
    "\n",
    "print('---char correct---')\n",
    "print(100*(char_correct/(total*7)), end=' ')\n",
    "print('%')\n",
    "print('---word correct---')\n",
    "print(100*word_correct/total, end=' ')\n",
    "print('%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 160])\n",
      "cnn out\n",
      "torch.Size([1, 1024, 1, 7])\n",
      "torch.Size([1, 1024, 1, 7])\n",
      "torch.Size([7, 1, 1024])\n",
      "torch.Size([7, 1, 37])\n",
      "torch.Size([7, 1, 37])\n",
      "b9x\n",
      "b9x\n"
     ]
    }
   ],
   "source": [
    "'''debug'''\n",
    "\n",
    "def get_char_count(arg1):\n",
    "    c0 = ALL_CHAR_SET[np.argmax(arg1[0:ALL_CHAR_SET_LEN])]\n",
    "    c1 = ALL_CHAR_SET[np.argmax(arg1[ALL_CHAR_SET_LEN:ALL_CHAR_SET_LEN*2])]\n",
    "    c2 = ALL_CHAR_SET[np.argmax(arg1[ALL_CHAR_SET_LEN*2:ALL_CHAR_SET_LEN*3])]\n",
    "    c3 = ALL_CHAR_SET[np.argmax(arg1[ALL_CHAR_SET_LEN*3:ALL_CHAR_SET_LEN*4])]\n",
    "    c4 = ALL_CHAR_SET[np.argmax(arg1[ALL_CHAR_SET_LEN*4:ALL_CHAR_SET_LEN*5])]\n",
    "    c5 = ALL_CHAR_SET[np.argmax(arg1[ALL_CHAR_SET_LEN*5:ALL_CHAR_SET_LEN*6])]\n",
    "    c6 = ALL_CHAR_SET[np.argmax(arg1[ALL_CHAR_SET_LEN*6:ALL_CHAR_SET_LEN*7])]\n",
    "    return c0, c1, c2,c3, c4, c5, c6 \n",
    "\n",
    "def oh_encoding(a):\n",
    "    label_oh = []\n",
    "    for i in range(7):\n",
    "        if i<len(a):\n",
    "            label_oh+=encode(a[i])\n",
    "        else:\n",
    "            label_oh+=encode('NONE')\n",
    "    return label_oh\n",
    "\n",
    "#best weight of Cap2TxT Network.\n",
    "model_path ='./ckpt/Cap2TxT_GoogLe_'+str(best_epoch)+'.pth'\n",
    "image_path = './Data/test/'\n",
    "\n",
    "pred_list =[]\n",
    "\n",
    "class_num = len(alphabet) + 1\n",
    "\n",
    "model = Cap2TxT(hidden_size,class_num)\n",
    "model = model.cuda()\n",
    "\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "converter = utils.strLabelConverter(alphabet)\n",
    "transformer=dataset.resizeNormalize((160,64))\n",
    "for i in range(1):\n",
    "    image = Image.open(image_path+str(i)+'.png').convert('L')\n",
    "    image = transformer(image)\n",
    "    if torch.cuda.is_available():\n",
    "        image = image.cuda()\n",
    "    image = image.view(1, *image.size())\n",
    "    image = Variable(image)\n",
    "    print(image.shape)\n",
    "    preds = model(image)\n",
    "    print(preds.shape)\n",
    "    _, preds = preds.max(2)\n",
    "    print(pred)\n",
    "    preds = preds.transpose(1, 0).contiguous().view(-1)\n",
    "    preds_size = Variable(torch.LongTensor([preds.size(0)]))\n",
    "    pred = converter.decode(preds.data, preds_size.data, raw=False)\n",
    "    print(pred)\n",
    "    pred_list.append(pred)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "main.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
